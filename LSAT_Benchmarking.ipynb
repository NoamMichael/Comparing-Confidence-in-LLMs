{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPiKF5ByqBIctLTyuBn5pK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoamMichael/Comparing-Confidence-in-LLMs/blob/main/LSAT_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WuTZXLzGAGkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a9620b-0669-482a-962a-dba244e40121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.52.2-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.52.2-py3-none-any.whl (286 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/286.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.3/286.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.52.2\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# This Notebook will test all models on the formatted LSAT-AR dataset\n",
        "# I have no issue running multiple API clients simultaneously. However, running\n",
        "# local models is pretty memory intensive so I can only run one at a time.\n",
        "%pip install anthropic\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (AutoTokenizer,\n",
        "                        AutoModelForCausalLM,\n",
        "                        BitsAndBytesConfig,\n",
        "                        pipeline)\n",
        "import warnings\n",
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "class OpenModel: ## This class is built around Hugging Face methods\n",
        "  def __init__(self, name, key, MaxTokens = 150):\n",
        "    self.name = name\n",
        "    self.key = key\n",
        "    self.MaxTokens = MaxTokens\n",
        "    print(f\"Downloading Tokenizer for {self.name}\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name,token = self.key) ## Import Tokenizer\n",
        "    print(f\"Downloading Model Weights for {self.name}\")\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.name, token = self.key, device_map=\"auto\") ## Import Model\n",
        "\n",
        "    ## Make text generation pipeline\n",
        "    self.pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = self.model,\n",
        "    tokenizer = self.tokenizer,\n",
        "    do_sample = False,\n",
        "    max_new_tokens = self.MaxTokens,\n",
        "    eos_token_id = self.tokenizer.eos_token_id,\n",
        "    pad_token_id = self.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  def generate(self, prompt):\n",
        "    return self.pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "  def GetTokens(self, prompt: str):\n",
        "    ## Get Answer:\n",
        "    batch = self.tokenizer(prompt, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    ## Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "\n",
        "    ##Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    ## Convert token indices to tokens\n",
        "    top_k_tokens = [self.tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    ## Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "\n",
        "    ## Create a Pandas Series with tokens as index and probabilities as values\n",
        "    logit_series = pd.Series(top_k_probs, index=top_k_tokens)\n",
        "\n",
        "    ## Sort the series by values in descending order\n",
        "    logit_series = logit_series.sort_values(ascending=False)\n",
        "    logit_series.index.name = \"Token\"\n",
        "    logit_series.name = \"Probability\"\n",
        "    return logit_series\n",
        "\n",
        "class ClosedModel(ABC):\n",
        "  @abstractmethod\n",
        "  def generate(self, prompt: str, system:str = \"\")-> str:\n",
        "        \"\"\"\n",
        "        Abstract method to generate a response from the language model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "  @abstractmethod\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def client(self):\n",
        "    pass\n",
        "\n",
        "class GPTmodel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the OpenAI client with the API key\n",
        "    self.client = openai.OpenAI(api_key=self.key)\n",
        "\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Use the new client-based API call\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    # Access the content from the new response object structure\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "class AnthropicModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "  def client(self):\n",
        "    # Initialize the Anthropic client with the API key\n",
        "    self.client = anthropic.Anthropic(api_key=self.key)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # The messages list should only contain user and assistant roles\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Use the Anthropic client to create a message\n",
        "    # Pass the system message as a top-level 'system' parameter\n",
        "    message = self.client.messages.create(\n",
        "        model=self.name,\n",
        "        max_tokens=100, # You can adjust this or make it an instance variable\n",
        "        messages=messages,\n",
        "        system=system if system else None # Pass system as a separate parameter, or None if empty\n",
        "    )\n",
        "    # Access the content from the response object\n",
        "    return message.content[0].text\n",
        "\n",
        "\n",
        "class GeminiModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the google.generativeai client with the API key\n",
        "\n",
        "    genai.configure(api_key=self.key)\n",
        "    self.model = genai.GenerativeModel(model_name=self.name)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Build the content list, including the system message if provided\n",
        "    contents = [{\"role\": \"user\", \"parts\": [prompt]}]\n",
        "    if system:\n",
        "        contents = [{\"role\": \"user\", \"parts\": [system]}] + contents\n",
        "\n",
        "    # Use the Gemini model to generate content\n",
        "    response = self.model.generate_content(contents)\n",
        "\n",
        "    # Access the content from the response object\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "ZqNsBM1oBHmp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Llama\n",
        "hf_llama_token = userdata.get('hf_llama_token')\n",
        "test_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
        "test_key = hf_llama_token\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "\n",
        "test_model = OpenModel(name = test_name, key = test_key)"
      ],
      "metadata": {
        "id": "sA1Rq7e6HlTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_model.generate(test_prompt))\n",
        "test_model.GetTokens(test_prompt)"
      ],
      "metadata": {
        "id": "H-bj6Bn9HsSF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for GPT\n",
        "\n",
        "gpt_4_key = userdata.get('gpt_api_key')\n",
        "test_name = 'gpt-4'\n",
        "test_key = gpt_4_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\"\n",
        "\n",
        "my_gpt = GPTmodel(name = test_name, api_key = test_key)\n",
        "my_gpt.client()\n",
        "my_gpt.generate(test_prompt, test_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "1r9p6BsQO5_Q",
        "outputId": "db5bf329-eee3-4102-d913-3a38bc70e578"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. Beksiński's works are characterized by their haunting, dystopian feel, often featuring desolate landscapes and tormented figures. Despite the grim themes, he insisted his work was not to be read literally and that he was not a pessimist. Beksiński was born on February 24, 1929, and tragically murdered in\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Claude\n",
        "\n",
        "claude_key = userdata.get('claude_api_key')\n",
        "test_name = 'claude-3-haiku-20240307'\n",
        "test_key = claude_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\"\n",
        "\n",
        "my_claude = AnthropicModel(name = test_name, api_key = test_key)\n",
        "my_claude.client()\n",
        "my_claude.generate(test_prompt, test_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WrdIcyVUff4I",
        "outputId": "6e42ad11-8abf-441a-a6dc-76777cff72c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor who was known for his distinctive surrealist and dystopian style. Here are some key facts about him:\\n\\n- Born in 1929 in Sanok, Poland, Beksiński initially studied to be an architect before turning to art.\\n\\n- His paintings often depicted post-apocalyptic, nightmarish landscapes and figures. His style was highly detailed and technical, with a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Gemini\n",
        "\n",
        "gemini_api_key = userdata.get('gemini_api_key') # Assuming you stored your key in Userdata\n",
        "test_name = \"gemini-2.0-flash\" # Or another Gemini model name like 'gemini-1.5-flash'\n",
        "test_key = gemini_api_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\" # Optional system message\n",
        "\n",
        "my_gemini = GeminiModel(name = test_name, api_key = test_key)\n",
        "my_gemini.client()\n",
        "my_gemini.generate(test_prompt, test_system)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dG59Di8PtRH4",
        "outputId": "023d6b33-7d38-47b7-9768-ba4b044efed2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor specializing in dystopian surrealism. He is known for his distinctive and hauntingly beautiful, yet often disturbing, imagery. His art explores themes of death, decay, anxiety, and the human condition.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Pseudo Code\n",
        "## Import dataset\n",
        "\n",
        "## Initialize all closed models\n",
        "\n",
        "##--Initialize all GPT models\n",
        "##----GPT-4o\n",
        "##----GPT-o3\n",
        "##--Initialize all Claude models\n",
        "##----Claude-3.7 Sonnet\n",
        "##----Claude-4 Sonnet\n",
        "##--Initialize Gemini\n",
        "##----Gemini-2.0 Flash\n",
        "##----Gemini-1.5 Flash\n",
        "##----Gemini-2.5 Pro\n",
        "\n",
        "'''\n",
        "for question in dataset:\n",
        "  for model in ClosedModels:\n",
        "    model.generate(question) #Since we are iterating over ClosedModels we can call the abstract method .generate()\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkvW0SL0xkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initializing my closed models\n",
        "\n",
        "my_closed_models = {\n",
        "    'GPT': {\n",
        "        'api_key_name': 'gpt_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gpt-4',\n",
        "            'gpt-3.5-turbo'\n",
        "        ]\n",
        "    },\n",
        "    'Claude': {\n",
        "        'api_key_name': 'claude_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            #'claude-3-sonnet-20240229',\n",
        "            'claude-3-haiku-20240307'\n",
        "        ]\n",
        "    },\n",
        "    'Gemini': {\n",
        "        'api_key_name': 'gemini_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gemini-1.5-flash',\n",
        "            #'gemini-1.5-pro'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "print('Initializing Closed Models:')\n",
        "closed_models = []\n",
        "for model_type in my_closed_models:\n",
        "    print(f'{model_type}:')\n",
        "    api_key_name = my_closed_models[model_type]['api_key_name']\n",
        "    api_key = userdata.get(api_key_name)\n",
        "    print(f'  API Key Name: {my_closed_models[model_type][\"api_key_name\"]}')\n",
        "    for model_name in my_closed_models[model_type]['models']:\n",
        "      # Instantiate the correct subclass based on model_type\n",
        "      if model_type == 'GPT':\n",
        "          my_model = GPTmodel(name = model_name, api_key = api_key)\n",
        "      elif model_type == 'Claude':\n",
        "          my_model = AnthropicModel(name = model_name, api_key = api_key)\n",
        "      elif model_type == 'Gemini':\n",
        "          my_model = GeminiModel(name = model_name, api_key = api_key)\n",
        "      else:\n",
        "          # Handle unexpected model types if necessary\n",
        "          print(f\"Warning: Unknown model type {model_type}. Skipping.\")\n",
        "          continue # Skip to the next model name if type is unknow\n",
        "      my_model.client()\n",
        "      closed_models.append(my_model)\n",
        "      print(f'    {model_name}')\n",
        "\n",
        "\n",
        "print(f'Models Initialized: {len(closed_models)}')\n",
        "print(f'Model locations:\\n{closed_models}')\n",
        "\n",
        "print('-'*42)\n",
        "print('Testing all closed models:')\n",
        "print(f'Test prompt: {test_prompt}')\n",
        "print(f'Test system: {test_system}')\n",
        "\n",
        "for model in closed_models:\n",
        "  print(f'\\nTesting model: {model.name}')\n",
        "  print(model.generate(test_prompt, test_system))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "Xl01iH3a1Vl4",
        "outputId": "39618ebb-44f9-474c-e833-adbacf4293aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Closed Models:\n",
            "GPT:\n",
            "  API Key Name: gpt_api_key\n",
            "    gpt-4\n",
            "    gpt-3.5-turbo\n",
            "Claude:\n",
            "  API Key Name: claude_api_key\n",
            "    claude-3-haiku-20240307\n",
            "Gemini:\n",
            "  API Key Name: gemini_api_key\n",
            "    gemini-1.5-flash\n",
            "Models Initialized: 4\n",
            "Model locations:\n",
            "[<__main__.GPTmodel object at 0x786e62e22410>, <__main__.GPTmodel object at 0x786e624e2b50>, <__main__.AnthropicModel object at 0x786e624e3c10>, <__main__.GeminiModel object at 0x786e624e3250>]\n",
            "------------------------------------------\n",
            "Testing all closed models:\n",
            "Test prompt: Zdzisław Beksiński was\n",
            "Test system: You are a helpful assistant.\n",
            "\n",
            "Testing model: gpt-4\n",
            "a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. Beksiński's works are characterized by their haunting, dystopian feel, often featuring desolate landscapes and grotesque, distorted figures. Despite the grim themes, he insisted his work was not to be interpreted too literally, and that he was more interested in the form and color than the specific details. Beksiński was born on February \n",
            "\n",
            "Testing model: gpt-3.5-turbo\n",
            "Zdzisław Beksiński was a renowned Polish artist known for his surreal and dystopian paintings. He was known for his dark and haunting imagery, often depicting nightmarish landscapes and figures. Beksiński's work has gained a cult following and continues to be influential in the realm of dark art. Tragically, he was murdered in his Warsaw apartment in 2005.\n",
            "\n",
            "Testing model: claude-3-haiku-20240307\n",
            "Zdzisław Beksiński was a Polish painter, photographer and sculptor who was known for his distinctive dark, dystopian and surreal style of art. Some key facts about Zdzisław Beksiński:\n",
            "\n",
            "- He was born in 1929 in Sanok, Poland and died in 2005 in Warsaw, Poland.\n",
            "\n",
            "- His painting style is often described as \"fantastic realism\" or \"dark surrealism\", depicting apocal\n",
            "\n",
            "Testing model: gemini-1.5-flash\n",
            "Zdzisław Beksiński was a Polish painter, sculptor, and photographer.  He is best known for his dystopian and surrealist paintings, characterized by their bleak, apocalyptic imagery and often featuring decaying figures, desolate landscapes, and bizarre, symbolic forms. His work is often described as disturbing, haunting, and intensely emotional.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import LSAT dataset\n",
        "\n",
        "file_path = '/content/LSAT_formatted.csv'\n",
        "\n",
        "dataset = pd.read_csv(file_path)\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "1DpX0W9yIitP",
        "outputId": "1fab49a3-0008-4d06-e6b2-088429d91aa9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Question Correct Answer  \\\n",
              "0  Exactly six trade representatives negotiate a ...              B   \n",
              "1  Exactly six trade representatives negotiate a ...              A   \n",
              "2  Exactly six trade representatives negotiate a ...              B   \n",
              "3  Exactly six trade representatives negotiate a ...              E   \n",
              "4  Exactly six trade representatives negotiate a ...              E   \n",
              "\n",
              "  Question Number                                      Option A  \\\n",
              "0  199106_2-G_1_1  Klosnik, Poirier, Neri, Manley, Osata, Londi   \n",
              "1  199106_2-G_1_2                             Klosnik and Osata   \n",
              "2  199106_2-G_1_3                                Londi and Neri   \n",
              "3  199106_2-G_1_4                              Londi and Manley   \n",
              "4  199106_2-G_1_5                                       Klosnik   \n",
              "\n",
              "                                       Option B  \\\n",
              "0  Klosnik, Londi, Manley, Poirier, Neri, Osata   \n",
              "1                                Londi and Neri   \n",
              "2                               Londi and Osata   \n",
              "3                             Londi and Poirier   \n",
              "4                                 Klosnik, Neri   \n",
              "\n",
              "                                       Option C  \\\n",
              "0  Klosnik, Londi, Manley, Osata, Poirier, Neri   \n",
              "1                               Londi and Osata   \n",
              "2                                Neri and Osata   \n",
              "3                                Neri and Osata   \n",
              "4                                 Neri, Poirier   \n",
              "\n",
              "                                       Option D  \\\n",
              "0  Klosnik, Osata, Poirier, Neri, Londi, Manley   \n",
              "1                               Manley and Neri   \n",
              "2                              Neri and Poirier   \n",
              "3                              Neri and Poirier   \n",
              "4                       Klosnik, Osata, Poirier   \n",
              "\n",
              "                                       Option E  \n",
              "0  Klosnik, Neri, Londi, Osata, Manley, Poirier  \n",
              "1                            Manley and Poirier  \n",
              "2                             Osata and Poirier  \n",
              "3                             Poirier and Osata  \n",
              "4                 Klosnik, Neri, Osata, Poirier  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cfd8729-92e4-4a21-84fd-beedf3dda6ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Correct Answer</th>\n",
              "      <th>Question Number</th>\n",
              "      <th>Option A</th>\n",
              "      <th>Option B</th>\n",
              "      <th>Option C</th>\n",
              "      <th>Option D</th>\n",
              "      <th>Option E</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Exactly six trade representatives negotiate a ...</td>\n",
              "      <td>B</td>\n",
              "      <td>199106_2-G_1_1</td>\n",
              "      <td>Klosnik, Poirier, Neri, Manley, Osata, Londi</td>\n",
              "      <td>Klosnik, Londi, Manley, Poirier, Neri, Osata</td>\n",
              "      <td>Klosnik, Londi, Manley, Osata, Poirier, Neri</td>\n",
              "      <td>Klosnik, Osata, Poirier, Neri, Londi, Manley</td>\n",
              "      <td>Klosnik, Neri, Londi, Osata, Manley, Poirier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Exactly six trade representatives negotiate a ...</td>\n",
              "      <td>A</td>\n",
              "      <td>199106_2-G_1_2</td>\n",
              "      <td>Klosnik and Osata</td>\n",
              "      <td>Londi and Neri</td>\n",
              "      <td>Londi and Osata</td>\n",
              "      <td>Manley and Neri</td>\n",
              "      <td>Manley and Poirier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Exactly six trade representatives negotiate a ...</td>\n",
              "      <td>B</td>\n",
              "      <td>199106_2-G_1_3</td>\n",
              "      <td>Londi and Neri</td>\n",
              "      <td>Londi and Osata</td>\n",
              "      <td>Neri and Osata</td>\n",
              "      <td>Neri and Poirier</td>\n",
              "      <td>Osata and Poirier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exactly six trade representatives negotiate a ...</td>\n",
              "      <td>E</td>\n",
              "      <td>199106_2-G_1_4</td>\n",
              "      <td>Londi and Manley</td>\n",
              "      <td>Londi and Poirier</td>\n",
              "      <td>Neri and Osata</td>\n",
              "      <td>Neri and Poirier</td>\n",
              "      <td>Poirier and Osata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Exactly six trade representatives negotiate a ...</td>\n",
              "      <td>E</td>\n",
              "      <td>199106_2-G_1_5</td>\n",
              "      <td>Klosnik</td>\n",
              "      <td>Klosnik, Neri</td>\n",
              "      <td>Neri, Poirier</td>\n",
              "      <td>Klosnik, Osata, Poirier</td>\n",
              "      <td>Klosnik, Neri, Osata, Poirier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cfd8729-92e4-4a21-84fd-beedf3dda6ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5cfd8729-92e4-4a21-84fd-beedf3dda6ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5cfd8729-92e4-4a21-84fd-beedf3dda6ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c33c1b0c-054c-48d3-8ffe-a285cfb74de4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c33c1b0c-054c-48d3-8ffe-a285cfb74de4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c33c1b0c-054c-48d3-8ffe-a285cfb74de4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 1630,\n  \"fields\": [\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1580,\n        \"samples\": [\n          \"Detectives investigating a citywide increase in burglaries questioned exactly seven suspects\\u2014S, T, V, W, X, Y, and Z\\u2014each on a different one of seven consecutive days. Each suspect was questioned exactly once. Any suspect who confessed did so while being questioned. The investigation conformed to the following: T was questioned on day three. The suspect questioned on day four did not confess. S was questioned after W was questioned. Both X and V were questioned after Z was questioned. No suspects confessed after W was questioned. Exactly two suspects confessed after T was questioned. Which one of the following suspects must have been questioned before T was questioned?\",\n          \"Bird-watchers explore a forest to see which of the following six kinds of birds\\u2014grosbeak, harrier, jay, martin, shrike, wren\\u2014it contains. The findings are consistent with the following conditions: If harriers are in the forest, then grosbeaks are not. If jays, martins, or both are in the forest, then so are harriers. If wrens are in the forest, then so are grosbeaks. If jays are not in the forest, then shrikes are. Suppose the condition is added that if shrikes are in the forest, then harriers are not. If all other conditions remain in effect, then which one of the following could be true?\",\n          \"A courier delivers exactly eight parcels\\u2014G, H, J, K, L, M, N, and O. No two parcels are delivered at the same time, nor is any parcel delivered more than once. The following conditions must apply: L is delivered later than H. K is delivered earlier than O. H is delivered earlier than M. O is delivered later than G. M is delivered earlier than G. Both N and J are delivered earlier than M. Which one of the following must be true?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"A\",\n          \"D\",\n          \"E\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question Number\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1630,\n        \"samples\": [\n          \"199809_1-G_4_24\",\n          \"199402_2-G_2_12\",\n          \"200709_2-G_1_5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Option A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1347,\n        \"samples\": [\n          \"Nation X: oranges, rice; Nation Y: oranges, tea; Nation Z: soybeans, wheat\",\n          \"There is exactly one yellow toy included in the display.\",\n          \"1 is a split-level house\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Option B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1386,\n        \"samples\": [\n          \"1, 3, 5, and 6\",\n          \"2, 4\",\n          \"Every subcommittee has either Hsia or Irving as a member.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Option C\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1387,\n        \"samples\": [\n          \"Lido is inspected on Tuesday morning.\",\n          \"4, 5\",\n          \"Irving serves on a subcommittee with Pinsky.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Option D\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1402,\n        \"samples\": [\n          \"Zinser is the only performer who signs with Star Agency.\",\n          \"Nordique, Lofton, Plattesville, Jackson, Oceana\",\n          \"One of the men orders swordfish.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Option E\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1357,\n        \"samples\": [\n          \"Two of the people order tilefish.\",\n          \"Sethna is on the second-place team.\",\n          \"If the house in Townsend is not shown fifth, then it must be shown immediately before the house in Riverton.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## System Prompts\n",
        "\n",
        "debug = True ## Test condition for print statements / less runs\n",
        "set_seed = 42 ## Random Seed\n",
        "\n",
        "sys_prompt1 = '''\n",
        "Given the following question, analyze the options, and provide a concise reasoning for your selected answer. Your reasoning should not exceed 100 words. After your explanation, clearly state your answer by choosing one of the options listed (A, B, C, D, or E).\n",
        "\n",
        "Question: ${Question}\n",
        "Options:\n",
        "A) ${Option A}\n",
        "B) ${Option B}\n",
        "C) ${Option C}\n",
        "D) ${Option D}\n",
        "E) ${Option E}\n",
        "\n",
        "Please provide your reasoning first, limited to 100 words, and then conclusively state only your selected answer using the corresponding letter (A, B, C, D, or E).\n",
        "Reasoning: <Your concise reasoning here. Max 100 words>\n",
        "'''\n",
        "sys_prompt2 = '''\n",
        "Based on the reasoning above, Provide the correct answer and the likelihood that each option is correct from 0.0 to 1.0 in a JSON format. The four probabilities should sum to 1.0. For example:\n",
        "\n",
        "{\n",
        "'Answer': <Your answer choice here, as a single letter and nothing else.>\n",
        "'A': <Probability choice A is correct. As a float from 0.0 to 1.0>,\n",
        "'B': <Probability choice B is correct. As a float from 0.0 to 1.0>,\n",
        "'C': <Probability choice C is correct. As a float from 0.0 to 1.0>,\n",
        "'D': <Probability choice D is correct. As a float from 0.0 to 1.0>,\n",
        "'E': <Probability choice E is correct. As a float from 0.0 to 1.0>\n",
        "}\n",
        "'''\n",
        "\n",
        "## Edit system Prompts in order to match size of dataset\n",
        "columns = dataset.columns\n",
        "num_options = columns.str.contains('Option').astype(int).sum()\n",
        "\n",
        "sys_prompt_temp1 = sys_prompt1\n",
        "sys_prompt_temp2 = sys_prompt2\n",
        "## Reformat system prompt in order to fit number of options in benchmark\n",
        "if num_options < 5: ## ABCD\n",
        "  sys_prompt_temp1 = (sys_prompt1\n",
        "                .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                .replace('E) ${Option E}', '') ## Drop option E\n",
        "      )\n",
        "  sys_prompt_temp2 = (sys_prompt2\n",
        "                .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                .replace('E) ${Option E}', '') ## Drop option E\n",
        "      )\n",
        "  if num_options < 4: ## ABC\n",
        "    sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                  .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                  .replace('D) ${Option D}', '') ## Drop option D\n",
        "        )\n",
        "    sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                .replace('D) ${Option D}', '') ## Drop option D\n",
        "      )\n",
        "\n",
        "    if num_options < 3: ## AB\n",
        "      sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                    .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                    .replace('C) ${Option C}', '') ## Drop option C\n",
        "          )\n",
        "      sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                  .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                  .replace('C) ${Option C}', '') ## Drop option C\n",
        "        )\n",
        "\n",
        "sys_prompt1 = sys_prompt_temp1\n",
        "sys_prompt2 = sys_prompt_temp2"
      ],
      "metadata": {
        "id": "9EVsC1FaKS0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%\n",
        "## Minor edits needed to generalize to other datasets\n",
        "## %%%%\n",
        "\n",
        "def format_df(df):\n",
        "  ## Takes in a dataframe in the form:\n",
        "  ## | Question Number | Question Text | Option A | Option B | ... | Correct Answer Letter |\n",
        "  ## |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "  ##\n",
        "  ## Returns a dataframe in the form:\n",
        "  ## | Question Number | Full Prompt 1 | Full Prompt 2 |\n",
        "  ## |     (Int)       |    (Str)      |    (Str)      |\n",
        "\n",
        "  columns = df.columns\n",
        "  num_options = columns.str.contains('Option').astype(int).sum()\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Check if DF is formatted properly\n",
        "  error_text = f'''Make sure dataframe is in following format:\n",
        "  | Question Number | Question Text | Option A | Option B | ... | Correct Answer Letter |\n",
        "  |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "\n",
        "  The current format of Dataframe is: {columns}\n",
        "  '''\n",
        "  ['Question Number', 'Question Text', 'Correct Answer Letter']\n",
        "  if num_options < 2:\n",
        "    raise Exception(error_text)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  sys_prompt_temp1 = sys_prompt1\n",
        "  sys_prompt_temp2 = sys_prompt2\n",
        "  ## Reformat system prompt in order to fit number of options in benchmark\n",
        "  if num_options < 5: ## ABCD\n",
        "    sys_prompt_temp1 = (sys_prompt1\n",
        "                  .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                  .replace('E) ${Option E}', '') ## Drop option E\n",
        "        )\n",
        "    sys_prompt_temp2 = (sys_prompt2\n",
        "                  .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                  .replace('E) ${Option E}', '') ## Drop option E\n",
        "        )\n",
        "    if num_options < 4: ## ABC\n",
        "      sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                    .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                    .replace('D) ${Option D}', '') ## Drop option D\n",
        "          )\n",
        "      sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                  .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                  .replace('D) ${Option D}', '') ## Drop option D\n",
        "        )\n",
        "\n",
        "      if num_options < 3: ## AB\n",
        "        sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                      .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                      .replace('C) ${Option C}', '') ## Drop option C\n",
        "            )\n",
        "        sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                    .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                    .replace('C) ${Option C}', '') ## Drop option C\n",
        "          )\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Initialize Output dataframe:\n",
        "  header = ['Question Num', 'Full Prompt 1', 'Full Prompt 2']\n",
        "  output_df = pd.DataFrame(columns = header)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "\n",
        "  ## Format questions for benchmark\n",
        "  letters = ['A', 'B', 'C', 'D', 'E']\n",
        "  options = ['Option A', 'Option B', 'Option C', 'Option D', 'Option E']\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    question = df['Question Text'][i]\n",
        "    option_text = df[options[:num_options]].iloc[i].to_list()\n",
        "\n",
        "    ## Prompt for specific question\n",
        "    new_prompt = sys_prompt_temp1.replace('${Question}', question)\n",
        "    for j in range(num_options): ## This for loop allows for dynamic question amounts\n",
        "        new_prompt = new_prompt.replace(f'${{Option {letters[j]}}}', str(option_text[j]))\n",
        "\n",
        "\n",
        "    ## Add formatted prompts.\n",
        "    ## Note that this is formatted to llama so changes may be needed down the line.\n",
        "    prompts1 = (new_prompt.split('<Your concise reasoning here. Max 100 words>')[0]) ## Specific prompt for question\n",
        "\n",
        "    prompts2 = (sys_prompt_temp2) ## Generic prompt for question confidence\n",
        "    output_df.loc[i] = [df['Question Number'].iloc[i], prompts1, prompts2]\n",
        "\n",
        "  return output_df\n"
      ],
      "metadata": {
        "id": "pPVxES4WVeru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%%\n",
        "## Need to clarify what goes in what with the rest of the group.\n",
        "\n",
        "## I think I can just use the model.generate method with some edits\n",
        "## %%%%%\n",
        "\n",
        "def get_reasoning(row, prompts):\n",
        "  input = prompts['Full Prompt 1'][row]\n",
        "  reasoning = generation_pipeline(input)\n",
        "  return reasoning\n",
        "\n",
        "def get_answer_confidence(row, prompts, reasoning):\n",
        "  sys_prompt2 = \"Based on the reasoning above, Provide the answer and the likelihood that each option is correct from 0.0 to 1.0 in a JSON format. The four probabilities should sum to 1.0. For example:\\nQuestion: ${Question}\\nOptions:\\nA) ${Option A}\\nB) ${Option B}\\nC) ${Option C}\\nD) ${Option D}\\nE) ${Option E}\\n\\nPlease provide your reasoning first, limited to 100 words, and then conclusively state only your selected answer using the corresponding letter (A, B, C, D, or E).\\nReasoning: ${Reasoning}\\nAnswer: ${Answer choice}\\n\\nProvide the likelihood that each answer is correct:\\n\\n{\\n'A': <Probability choice A is correct. As a float from 0.0 to 1.0>,\\n'B': <Probability choice B is correct. As a float from 0.0 to 1.0>,\\n'C': <Probability choice C is correct. As a float from 0.0 to 1.0>,\\n'D': <Probability choice D is correct. As a float from 0.0 to 1.0>,\\n'E': <Probability choice E is correct. As a float from 0.0 to 1.0>\\n}\"\n",
        "  begin_JSON= \"\\n{\\n'Answer': \"\n",
        "\n",
        "  input = prompts['Full Prompt 1'][row] + reasoning + begin_JSON\n",
        "  input = input.strip('Answer:')\n",
        "  print('______________________')\n",
        "  print(f'Input:\\n{input}')\n",
        "  print('______________________')\n",
        "\n",
        "  if is_api:\n",
        "    answer = generation_pipeline(input)\n",
        "  else:\n",
        "    ## Get Answer:\n",
        "    batch = tokenizer(input, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "    #print(probs)\n",
        "\n",
        "    # Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    # Convert token indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    # Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "    arr = list(zip(top_k_tokens, top_k_probs))          #Creates an array of tokens and their prob.\n",
        "    logit_df = pd.Series(arr, columns= [\"Token\", \"Prob\"] ) #converts array -> dataframe\n",
        "    answer = logit_df[\"Token\"][0].strip()\n",
        "    logit_confidence = json.dumps(logit_df.set_index('Token').to_dict(),indent=4, sort_keys=True)\n",
        "    ## Get Stated Confidence:\n",
        "    new_input = input + answer + prompts['Full Prompt 2'][row]\n",
        "    stated_confidence = generation_pipeline(new_input)\n",
        "\n",
        "  return answer, stated_confidence, logit_confidence\n",
        "\n",
        "#get_answer_confidence(i, benchmark_prompts, raw_output1, open_model = is_api)"
      ],
      "metadata": {
        "id": "2dgiR5hNW_rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## %%%%\n",
        "## Edit this to work for all models\n",
        "## %%%%\n",
        "\n",
        "def test_model_benchmark(benchmark_prompts, ## The benchmark dataframe in the form:  | Question Num (Start at 0) | Full Prompt |\n",
        "                         custom_length = -1, ## If we want to change how much of the benchmark we run on\n",
        "                         output_loc = ''): ## Where we want to save the output dataframe\n",
        "\n",
        "  ## Output Format:\n",
        "  ## | Question Num | Question | Reasoning | Answer |   Stated Probs   |  Logit Probs  |\n",
        "  ## |    (Int)     |   (Str)  |   (Str)   | (Char) | (String as JSON) | (Dict as Str) |\n",
        "  ##                                                                     Only for Open\n",
        "\n",
        "  random.seed(set_seed)\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Set the amount of runs we will do custom_length == -1 is the default\n",
        "  if custom_length == -1:\n",
        "    length = len(benchmark_prompts)\n",
        "  else:\n",
        "    length = custom_length\n",
        "  print(f'Testing benchmark on {length} rows')\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Initilize the output Dataframe:\n",
        "\n",
        "  if is_api:\n",
        "    ## For a closed model like GPT\n",
        "    header = ['Question Num', 'Question Text', 'Raw Output Prompt 1', 'Raw Output Prompt 2']\n",
        "  else:\n",
        "    ## For an open / local model like Llama\n",
        "    header = ['Question Num', 'Question Text', 'Reasoning', 'Answer', 'Stated Confidence', 'Logit Confidence']\n",
        "\n",
        "  output_df = pd.DataFrame(columns = header)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Test the model:\n",
        "  for i in range(length):\n",
        "    ## Get Reasoning\n",
        "    reasoning = get_reasoning(i, benchmark_prompts)\n",
        "    print(1)\n",
        "    if debug: print(reasoning)\n",
        "\n",
        "    ## Get Answer/ Confidence:\n",
        "    if is_api: ## For a closed model like GPT\n",
        "      answer, stated_confidence = get_answer_confidence(i, benchmark_prompts, reasoning)\n",
        "\n",
        "      if debug: print(f'Answer: \\n{answer}') ## Print the results if testing\n",
        "      output_df.loc[i] = [i, benchmark_prompts['Full Prompt 1'][i], reasoning, answer, stated_confidence] ## Save to output df\n",
        "    else: ## For an open / local model like Llama\n",
        "\n",
        "      ## Get the raw confidence and logit probabilities for answer.\n",
        "      ## Note that for models like llama, the answer letter will be obtained by\n",
        "      ## post proccessing the logit confidence down the line.\n",
        "      answer, stated_confidence, logit_confidence = get_answer_confidence(i, benchmark_prompts, reasoning)\n",
        "\n",
        "      output_df.loc[i] = [i, benchmark_prompts['Full Prompt 1'][i], reasoning, answer, stated_confidence, logit_confidence] ## Save to output df\n",
        "      if debug: print(f'Answer: \\n{answer}\\nLogit Confidence: \\n{logit_confidence}') ## Print the results if testing\n",
        "    ## Save output_df to csv to save progress.\n",
        "    output_df.to_csv(output_loc)\n",
        "  return output_df"
      ],
      "metadata": {
        "id": "6WqAkRYfXePO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Example Implementation:\n",
        "\n",
        "\n",
        "## Import dataset\n",
        "\n",
        "# Assuming you have your API keys stored in userdata\n",
        "gpt_4_key = userdata.get('gpt_api_key')\n",
        "claude_key = userdata.get('claude_api_key')\n",
        "gemini_api_key = userdata.get('gemini_api_key')\n",
        "\n",
        "## Initialize all closed models you want to test\n",
        "\n",
        "# Create a list to hold your model instances\n",
        "ClosedModels = []\n",
        "\n",
        "##--Initialize all GPT models\n",
        "##----GPT-4o\n",
        "ClosedModels.append(GPTmodel(name='gpt-4o', api_key=gpt_4_key))\n",
        "##----GPT-o3 (Assuming you meant gpt-3.5-turbo or similar)\n",
        "ClosedModels.append(GPTmodel(name='gpt-3.5-turbo', api_key=gpt_4_key)) # Or the correct GPT-3 model name\n",
        "\n",
        "##--Initialize all Claude models\n",
        "##----Claude-3 Sonnet (Corrected name, typically claude-3-sonnet-20240229)\n",
        "ClosedModels.append(AnthropicModel(name='claude-3-sonnet-20240229', api_key=claude_key))\n",
        "##----Claude-3 Haiku (You already tested this one)\n",
        "ClosedModels.append(AnthropicModel(name='claude-3-haiku-20240307', api_key=claude_key))\n",
        "# Claude 4 is not a standard model name, perhaps you meant Claude 3.5 Sonnet?\n",
        "# ClosedModels.append(AnthropicModel(name='claude-3-5-sonnet-20240620', api_key=claude_key)) # If Claude 3.5 Sonnet is what you meant\n",
        "\n",
        "##--Initialize Gemini\n",
        "##----Gemini-2.0 Flash (Assuming gemini-1.5-flash as a common name)\n",
        "ClosedModels.append(GeminiModel(name='gemini-1.5-flash', api_key=gemini_api_key))\n",
        "##----Gemini-2.0 Pro (Assuming gemini-1.5-pro as a common name)\n",
        "ClosedModels.append(GeminiModel(name='gemini-1.5-pro', api_key=gemini_api_key))\n",
        "# Gemini 2.5 Pro is not a standard model name, perhaps you meant Gemini 1.5 Pro?\n",
        "\n",
        "# You can add more models to the list as needed.\n",
        "\n",
        "# --- Example Dataset (Replace with your actual dataset loading) ---\n",
        "# Assuming your dataset is a list of strings representing questions/prompts\n",
        "dataset = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Explain the concept of recursion in programming.\",\n",
        "    \"Write a short story about a cat.\",\n",
        "    \"Summarize the plot of The Great Gatsby.\"\n",
        "]\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "\n",
        "for question in dataset:\n",
        "  print(f\"\\nTesting prompt: {question}\")\n",
        "  for model in ClosedModels:\n",
        "    try:\n",
        "        print(f\"--- Calling model: {model.name} ---\")\n",
        "        # Note: Your ClosedModel base class doesn't define 'generate'.\n",
        "        # You likely want to use the 'generate' method defined in your subclasses.\n",
        "        # Make sure the 'generate' method in your subclasses takes the expected\n",
        "        # parameters (prompt, and potentially system).\n",
        "        # Let's assume your closed models' 'generate' method takes prompt and system.\n",
        "        # If a system message is needed, you'll need to provide one here or modify\n",
        "        # your model classes/loop structure. For simplicity, let's assume\n",
        "        # the prompts can be handled without a separate system message for now,\n",
        "        # or you can add one if needed.\n",
        "        response = model.generate(prompt=question, system=\"\") # Pass the question from the dataset\n",
        "        print(f\"Response from {model.name}: {response[:200]}...\") # Print first 200 chars of response\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling model {model.name}: {e}\")\n"
      ],
      "metadata": {
        "id": "9TzDyWzd1OP3"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}