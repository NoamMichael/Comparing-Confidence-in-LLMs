{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMG5oAoBr5sFybisisqCwjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoamMichael/Comparing-Confidence-in-LLMs/blob/main/LSAT_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuTZXLzGAGkZ"
      },
      "outputs": [],
      "source": [
        "# This Notebook will test all models on the formatted LSAT-AR dataset\n",
        "# I have no issue running multiple API clients simultaneously. However, running\n",
        "# local models is pretty memory intensive so I can only run one at a time.\n",
        "%pip install anthropic\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (AutoTokenizer,\n",
        "                        AutoModelForCausalLM,\n",
        "                        BitsAndBytesConfig,\n",
        "                        pipeline)\n",
        "import warnings\n",
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "class OpenModel: ## This class is built around Hugging Face methods\n",
        "  def __init__(self, name, key, MaxTokens = 150):\n",
        "    self.name = name\n",
        "    self.key = key\n",
        "    self.MaxTokens = MaxTokens\n",
        "    print(f\"Downloading Tokenizer for {self.name}\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name,token = self.key) ## Import Tokenizer\n",
        "    print(f\"Downloading Model Weights for {self.name}\")\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.name, token = self.key, device_map=\"auto\") ## Import Model\n",
        "\n",
        "    ## Make text generation pipeline\n",
        "    self.pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = self.model,\n",
        "    tokenizer = self.tokenizer,\n",
        "    do_sample = False,\n",
        "    max_new_tokens = self.MaxTokens,\n",
        "    eos_token_id = self.tokenizer.eos_token_id,\n",
        "    pad_token_id = self.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  def generate(self, prompt):\n",
        "    return self.pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "  def GetTokens(self, prompt: str):\n",
        "    ## Get Answer:\n",
        "    batch = self.tokenizer(prompt, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    ## Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "\n",
        "    ##Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    ## Convert token indices to tokens\n",
        "    top_k_tokens = [self.tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    ## Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "\n",
        "    ## Create a Pandas Series with tokens as index and probabilities as values\n",
        "    logit_series = pd.Series(top_k_probs, index=top_k_tokens)\n",
        "\n",
        "    ## Sort the series by values in descending order\n",
        "    logit_series = logit_series.sort_values(ascending=False)\n",
        "    logit_series.index.name = \"Token\"\n",
        "    logit_series.name = \"Probability\"\n",
        "    return logit_series\n",
        "\n",
        "class ClosedModel(ABC):\n",
        "  @abstractmethod\n",
        "  def generate(self, prompt: str, system:str = \"\")-> str:\n",
        "        \"\"\"\n",
        "        Abstract method to generate a response from the language model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "  @abstractmethod\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def client(self):\n",
        "    pass\n",
        "\n",
        "class GPTmodel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the OpenAI client with the API key\n",
        "    self.client = openai.OpenAI(api_key=self.key)\n",
        "\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Use the new client-based API call\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    # Access the content from the new response object structure\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "class AnthropicModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "  def client(self):\n",
        "    # Initialize the Anthropic client with the API key\n",
        "    self.client = anthropic.Anthropic(api_key=self.key)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # The messages list should only contain user and assistant roles\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Use the Anthropic client to create a message\n",
        "    # Pass the system message as a top-level 'system' parameter\n",
        "    message = self.client.messages.create(\n",
        "        model=self.name,\n",
        "        max_tokens=100, # You can adjust this or make it an instance variable\n",
        "        messages=messages,\n",
        "        system=system if system else None # Pass system as a separate parameter, or None if empty\n",
        "    )\n",
        "    # Access the content from the response object\n",
        "    return message.content[0].text\n",
        "\n",
        "\n",
        "class GeminiModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the google.generativeai client with the API key\n",
        "\n",
        "    genai.configure(api_key=self.key)\n",
        "    self.model = genai.GenerativeModel(model_name=self.name)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Build the content list, including the system message if provided\n",
        "    contents = [{\"role\": \"user\", \"parts\": [prompt]}]\n",
        "    if system:\n",
        "        contents = [{\"role\": \"user\", \"parts\": [system]}] + contents\n",
        "\n",
        "    # Use the Gemini model to generate content\n",
        "    response = self.model.generate_content(contents)\n",
        "\n",
        "    # Access the content from the response object\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "ZqNsBM1oBHmp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Llama\n",
        "hf_llama_token = userdata.get('hf_llama_token')\n",
        "test_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
        "test_key = hf_llama_token\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "\n",
        "test_model = OpenModel(name = test_name, key = test_key)"
      ],
      "metadata": {
        "id": "sA1Rq7e6HlTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_model.generate(test_prompt))\n",
        "test_model.GetTokens(test_prompt)"
      ],
      "metadata": {
        "id": "H-bj6Bn9HsSF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for GPT\n",
        "\n",
        "gpt_4_key = userdata.get('gpt_api_key')\n",
        "test_name = 'gpt-4'\n",
        "test_key = gpt_4_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\"\n",
        "\n",
        "my_gpt = GPTmodel(name = test_name, api_key = test_key)\n",
        "my_gpt.client()\n",
        "my_gpt.generate(test_prompt, test_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "1r9p6BsQO5_Q",
        "outputId": "7b91d9f5-4f32-4aee-ad13-3c10ad4a7a7c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. Beksiński's works are characterized by their haunting, dystopian feel, often featuring desolate landscapes and tormented figures. Despite the grim themes, he insisted his work was not to be read literally and that he was not a pessimist. Beksiński was born on February 24, 1929, and tragically murdered in\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Claude\n",
        "\n",
        "claude_key = userdata.get('claude_api_key')\n",
        "test_name = 'claude-3-haiku-20240307'\n",
        "test_key = claude_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "\n",
        "my_claude = AnthropicModel(name = test_name, api_key = test_key)\n",
        "my_claude.client()\n",
        "my_claude.generate(test_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "WrdIcyVUff4I",
        "outputId": "76adbfeb-a208-4009-d6a8-009e7be359c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor known for his dark, dystopian, and surreal artworks. Here are some key facts about Zdzisław Beksiński:\\n\\n1. Born in 1929 in Sanok, Poland, Beksiński studied architecture and civil engineering before turning to art full-time in the late 1960s.\\n\\n2. His paintings are characterized by a unique style'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Gemini\n",
        "\n",
        "gemini_api_key = userdata.get('gemini_api_key') # Assuming you stored your key in Userdata\n",
        "test_name = \"gemini-2.0-flash\" # Or another Gemini model name like 'gemini-1.5-flash'\n",
        "test_key = gemini_api_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\" # Optional system message\n",
        "\n",
        "my_gemini = GeminiModel(name = test_name, api_key = test_key)\n",
        "my_gemini.client()\n",
        "my_gemini.generate(test_prompt, test_system)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "dG59Di8PtRH4",
        "outputId": "94c94658-08e0-458d-f513-24e1868a6671"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor, known for his dystopian surrealist art. His works often depicted nightmarish environments with decaying figures, desolate landscapes, and unsettling imagery. He famously refused to title his pieces, preferring to leave them open to interpretation by the viewer.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Pseudo Code\n",
        "## Import dataset\n",
        "\n",
        "## Initialize all closed models\n",
        "\n",
        "##--Initialize all GPT models\n",
        "##----GPT-4o\n",
        "##----GPT-o3\n",
        "##--Initialize all Claude models\n",
        "##----Claude-3.7 Sonnet\n",
        "##----Claude-4 Sonnet\n",
        "##--Initialize Gemini\n",
        "##----Gemini-2.0 Flash\n",
        "##----Gemini-1.5 Flash\n",
        "##----Gemini-2.5 Pro\n",
        "\n",
        "'''\n",
        "for question in dataset:\n",
        "  for model in ClosedModels:\n",
        "    model.generate(question) #Since we are iterating over ClosedModels we can call the abstract method .generate()\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkvW0SL0xkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initializing my closed models\n",
        "\n",
        "my_closed_models = {\n",
        "    'GPT': {\n",
        "        'api_key_name': 'gpt_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gpt-4',\n",
        "            'gpt-3.5-turbo'\n",
        "        ]\n",
        "    },\n",
        "    'Claude': {\n",
        "        'api_key_name': 'claude_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            #'claude-3-sonnet-20240229',\n",
        "            'claude-3-haiku-20240307'\n",
        "        ]\n",
        "    },\n",
        "    'Gemini': {\n",
        "        'api_key_name': 'gemini_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gemini-1.5-flash',\n",
        "            #'gemini-1.5-pro'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "print('Initializing Closed Models:')\n",
        "closed_models = []\n",
        "for model_type in my_closed_models:\n",
        "    print(f'{model_type}:')\n",
        "    api_key_name = my_closed_models[model_type]['api_key_name']\n",
        "    api_key = userdata.get(api_key_name)\n",
        "    print(f'  API Key Name: {my_closed_models[model_type][\"api_key_name\"]}')\n",
        "    for model_name in my_closed_models[model_type]['models']:\n",
        "      # Instantiate the correct subclass based on model_type\n",
        "      if model_type == 'GPT':\n",
        "          my_model = GPTmodel(name = model_name, api_key = api_key)\n",
        "      elif model_type == 'Claude':\n",
        "          my_model = AnthropicModel(name = model_name, api_key = api_key)\n",
        "      elif model_type == 'Gemini':\n",
        "          my_model = GeminiModel(name = model_name, api_key = api_key)\n",
        "      else:\n",
        "          # Handle unexpected model types if necessary\n",
        "          print(f\"Warning: Unknown model type {model_type}. Skipping.\")\n",
        "          continue # Skip to the next model name if type is unknow\n",
        "      my_model.client()\n",
        "      closed_models.append(my_model)\n",
        "      print(f'    {model_name}')\n",
        "\n",
        "\n",
        "print(f'Models Initialized: {len(models)}')\n",
        "print(f'Model locations:\\n{models}')\n",
        "\n",
        "print('-'*42)\n",
        "print('Testing all closed models:')\n",
        "print(f'Test prompt: {test_prompt}')\n",
        "print(f'Test system: {test_system}')\n",
        "\n",
        "for model in closed_models:\n",
        "  print(f'\\nTesting model: {model.name}')\n",
        "  print(model.generate(test_prompt, test_system))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "Xl01iH3a1Vl4",
        "outputId": "9044ba96-14d9-47a5-9c8f-0b3016f0e8e0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Closed Models:\n",
            "GPT:\n",
            "  API Key Name: gpt_api_key\n",
            "    gpt-4\n",
            "    gpt-3.5-turbo\n",
            "Claude:\n",
            "  API Key Name: claude_api_key\n",
            "    claude-3-haiku-20240307\n",
            "Gemini:\n",
            "  API Key Name: gemini_api_key\n",
            "    gemini-1.5-flash\n",
            "Models Initialized: 6\n",
            "Model locations:\n",
            "[<__main__.GPTmodel object at 0x78ab348a2b50>, <__main__.GPTmodel object at 0x78ab33dce250>, <__main__.AnthropicModel object at 0x78ab34564690>, <__main__.AnthropicModel object at 0x78ab33e43c50>, <__main__.GeminiModel object at 0x78ab33fa7390>, <__main__.GeminiModel object at 0x78ab33fb0990>]\n",
            "------------------------------------------\n",
            "Testing all closed models:\n",
            "Test prompt: Zdzisław Beksiński was\n",
            "Test system: You are a helpful assistant.\n",
            "\n",
            "Testing model: gpt-4\n",
            "a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. His works are often disturbing and filled with themes of death, decay, and darkness. Despite the grim themes, Beksiński claimed his works were often misunderstood and that he was the happiest man he knew. He was born on February 24, 1929, and tragically murdered in 2005.\n",
            "\n",
            "Testing model: gpt-3.5-turbo\n",
            "Zdzisław Beksiński was a renowned Polish artist known for his surreal and dystopian paintings. He was primarily known for his dark and haunting artworks that often depicted nightmarish and otherworldly scenes. Beksiński's work has gained international recognition and has left a lasting impact on the world of art.\n",
            "\n",
            "Testing model: claude-3-haiku-20240307\n",
            "Zdzisław Beksiński was a renowned Polish painter, photographer, and sculptor who was known for his unique and unsettling surrealist art style. Here are some key facts about Zdzisław Beksiński:\n",
            "\n",
            "1. Born in 1929 in Sanok, Poland, Beksiński initially studied to be an architect before turning his focus to art.\n",
            "\n",
            "2. His painting style was often described as \"fantastic realism\" or\n",
            "\n",
            "Testing model: gemini-1.5-flash\n",
            "Zdzisław Beksiński was a Polish painter, sculptor, and photographer.  He is best known for his dystopian and surrealist paintings, characterized by their bleak, nightmarish imagery, often featuring decaying figures, desolate landscapes, and a strong sense of alienation and despair.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Example Implementation:\n",
        "\n",
        "\n",
        "## Import dataset\n",
        "\n",
        "# Assuming you have your API keys stored in userdata\n",
        "gpt_4_key = userdata.get('gpt_api_key')\n",
        "claude_key = userdata.get('claude_api_key')\n",
        "gemini_api_key = userdata.get('gemini_api_key')\n",
        "\n",
        "## Initialize all closed models you want to test\n",
        "\n",
        "# Create a list to hold your model instances\n",
        "ClosedModels = []\n",
        "\n",
        "##--Initialize all GPT models\n",
        "##----GPT-4o\n",
        "ClosedModels.append(GPTmodel(name='gpt-4o', api_key=gpt_4_key))\n",
        "##----GPT-o3 (Assuming you meant gpt-3.5-turbo or similar)\n",
        "ClosedModels.append(GPTmodel(name='gpt-3.5-turbo', api_key=gpt_4_key)) # Or the correct GPT-3 model name\n",
        "\n",
        "##--Initialize all Claude models\n",
        "##----Claude-3 Sonnet (Corrected name, typically claude-3-sonnet-20240229)\n",
        "ClosedModels.append(AnthropicModel(name='claude-3-sonnet-20240229', api_key=claude_key))\n",
        "##----Claude-3 Haiku (You already tested this one)\n",
        "ClosedModels.append(AnthropicModel(name='claude-3-haiku-20240307', api_key=claude_key))\n",
        "# Claude 4 is not a standard model name, perhaps you meant Claude 3.5 Sonnet?\n",
        "# ClosedModels.append(AnthropicModel(name='claude-3-5-sonnet-20240620', api_key=claude_key)) # If Claude 3.5 Sonnet is what you meant\n",
        "\n",
        "##--Initialize Gemini\n",
        "##----Gemini-2.0 Flash (Assuming gemini-1.5-flash as a common name)\n",
        "ClosedModels.append(GeminiModel(name='gemini-1.5-flash', api_key=gemini_api_key))\n",
        "##----Gemini-2.0 Pro (Assuming gemini-1.5-pro as a common name)\n",
        "ClosedModels.append(GeminiModel(name='gemini-1.5-pro', api_key=gemini_api_key))\n",
        "# Gemini 2.5 Pro is not a standard model name, perhaps you meant Gemini 1.5 Pro?\n",
        "\n",
        "# You can add more models to the list as needed.\n",
        "\n",
        "# --- Example Dataset (Replace with your actual dataset loading) ---\n",
        "# Assuming your dataset is a list of strings representing questions/prompts\n",
        "dataset = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Explain the concept of recursion in programming.\",\n",
        "    \"Write a short story about a cat.\",\n",
        "    \"Summarize the plot of The Great Gatsby.\"\n",
        "]\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "\n",
        "for question in dataset:\n",
        "  print(f\"\\nTesting prompt: {question}\")\n",
        "  for model in ClosedModels:\n",
        "    try:\n",
        "        print(f\"--- Calling model: {model.name} ---\")\n",
        "        # Note: Your ClosedModel base class doesn't define 'generate'.\n",
        "        # You likely want to use the 'generate' method defined in your subclasses.\n",
        "        # Make sure the 'generate' method in your subclasses takes the expected\n",
        "        # parameters (prompt, and potentially system).\n",
        "        # Let's assume your closed models' 'generate' method takes prompt and system.\n",
        "        # If a system message is needed, you'll need to provide one here or modify\n",
        "        # your model classes/loop structure. For simplicity, let's assume\n",
        "        # the prompts can be handled without a separate system message for now,\n",
        "        # or you can add one if needed.\n",
        "        response = model.generate(prompt=question, system=\"\") # Pass the question from the dataset\n",
        "        print(f\"Response from {model.name}: {response[:200]}...\") # Print first 200 chars of response\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling model {model.name}: {e}\")\n"
      ],
      "metadata": {
        "id": "9TzDyWzd1OP3"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}