{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNJ61jEpOLQfymoBhpSGAhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0af187c51354e3eac87e434369e45c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc43fac4aebf4220a2ea3be03d5dc84f",
              "IPY_MODEL_e7f65408412e4fb48022f01d83e84b70",
              "IPY_MODEL_272dace31cfc477e9a9f7214846e2a9a"
            ],
            "layout": "IPY_MODEL_78888c2d9cd14d7db12cd565bf03921b"
          }
        },
        "bc43fac4aebf4220a2ea3be03d5dc84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d63fb3c83ba4bb39cb19b5a8c45153d",
            "placeholder": "​",
            "style": "IPY_MODEL_1870a9e059404606b2ca55d9f8d85b86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e7f65408412e4fb48022f01d83e84b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aee1d04237849d9884f5e4a111cc064",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d711397bfaf4f75bf93e709f1898c86",
            "value": 4
          }
        },
        "272dace31cfc477e9a9f7214846e2a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d88ff77636844b697f7a83379016f36",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f8d9a3befd418b924fa33be25f0d02",
            "value": " 4/4 [00:04&lt;00:00,  1.01s/it]"
          }
        },
        "78888c2d9cd14d7db12cd565bf03921b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d63fb3c83ba4bb39cb19b5a8c45153d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1870a9e059404606b2ca55d9f8d85b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aee1d04237849d9884f5e4a111cc064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d711397bfaf4f75bf93e709f1898c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d88ff77636844b697f7a83379016f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f8d9a3befd418b924fa33be25f0d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoamMichael/Comparing-Confidence-in-LLMs/blob/main/LSAT_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuTZXLzGAGkZ"
      },
      "outputs": [],
      "source": [
        "# This Notebook will test all models on the formatted LSAT-AR dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (AutoTokenizer,\n",
        "                        AutoModelForCausalLM,\n",
        "                        BitsAndBytesConfig,\n",
        "                        pipeline)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata\n",
        "hf_llama_token = userdata.get('hf_llama_token')\n",
        "\n",
        "class OpenModel: ## This class is built around Hugging Face methods\n",
        "  def __init__(self, name, key, MaxTokens = 150):\n",
        "    self.name = name\n",
        "    self.key = key\n",
        "    self.MaxTokens = MaxTokens\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name,token = self.key) ## Import Tokenizer\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.name, token = self.key, device_map=\"auto\") ## Import Model\n",
        "\n",
        "    ## Make text generation pipeline\n",
        "    self.pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = self.model,\n",
        "    tokenizer = self.tokenizer,\n",
        "    do_sample = False,\n",
        "    max_new_tokens = self.MaxTokens,\n",
        "    eos_token_id = self.tokenizer.eos_token_id,\n",
        "    pad_token_id = self.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  def generate(self, prompt):\n",
        "    return self.pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "  def GetTokens(self, prompt: str):\n",
        "    ## Get Answer:\n",
        "    batch = self.tokenizer(prompt, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "    #print(probs)\n",
        "    #_____________________________________________________\n",
        "\n",
        "    # Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    # Convert token indices to tokens\n",
        "    # Use self.tokenizer\n",
        "    top_k_tokens = [self.tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    # Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "\n",
        "    # Create a Pandas Series with tokens as index and probabilities as values\n",
        "    logit_series = pd.Series(top_k_probs, index=top_k_tokens)\n",
        "\n",
        "    # Sort the series by values in descending order\n",
        "    logit_series = logit_series.sort_values(ascending=False)\n",
        "    logit_series.index.name = \"Token\"\n",
        "    logit_series.name = \"Probability\"\n",
        "    return logit_series"
      ],
      "metadata": {
        "id": "ZqNsBM1oBHmp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
        "test_key = hf_llama_token\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "\n",
        "test_model = OpenModel(name = test_name, key = test_key)"
      ],
      "metadata": {
        "id": "sA1Rq7e6HlTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f0af187c51354e3eac87e434369e45c7",
            "bc43fac4aebf4220a2ea3be03d5dc84f",
            "e7f65408412e4fb48022f01d83e84b70",
            "272dace31cfc477e9a9f7214846e2a9a",
            "78888c2d9cd14d7db12cd565bf03921b",
            "5d63fb3c83ba4bb39cb19b5a8c45153d",
            "1870a9e059404606b2ca55d9f8d85b86",
            "7aee1d04237849d9884f5e4a111cc064",
            "6d711397bfaf4f75bf93e709f1898c86",
            "3d88ff77636844b697f7a83379016f36",
            "f7f8d9a3befd418b924fa33be25f0d02"
          ]
        },
        "outputId": "f7866cfd-13b9-4e59-d752-1ebe4c8f9a13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0af187c51354e3eac87e434369e45c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_model.generate(test_prompt))\n",
        "test_model.GetTokens(test_prompt)"
      ],
      "metadata": {
        "id": "H-bj6Bn9HsSF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "e0b0d0ac-2d22-4dd1-96c7-ad207a5185be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zdzisław Beksiński was a Polish painter, photographer, and sculptor. He was born on February 24, 1929, in Sanok, Poland. Beksiński's work is characterized by its dark and surreal themes, often exploring the human condition, mortality, and the relationship between technology and nature. He was a prolific artist, creating over 20,000 works of art during his lifetime, including paintings, photographs, and sculptures.\n",
            "Beksiński's artistic style is often described as a blend of surrealism, expressionism, and fantasy. His paintings often feature dreamlike landscapes, eerie cityscapes, and abstracted forms, while his photographs capture the beauty and decay of the natural world. Beksiński's work has been exhibited\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Token\n",
              "a            0.752481\n",
              "born         0.197850\n",
              "one          0.018028\n",
              "an           0.008461\n",
              "the          0.005345\n",
              "               ...   \n",
              "conceived    0.000021\n",
              "appointed    0.000021\n",
              "trained      0.000021\n",
              "our          0.000021\n",
              "artist       0.000021\n",
              "Name: Probability, Length: 100, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Token</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.752481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>born</th>\n",
              "      <td>0.197850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>one</th>\n",
              "      <td>0.018028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>an</th>\n",
              "      <td>0.008461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.005345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conceived</th>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>appointed</th>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trained</th>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>our</th>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artist</th>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "source": [
        "## GPT pseudocode\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Step 1: Define a base class for your LLM wrappers\n",
        "class LLMWrapper(ABC):\n",
        "    @abstractmethod\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Abstract method to generate a response from the language model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# Step 2: Create subclasses for each LLM\n",
        "class APIModelWrapper(LLMWrapper):\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        # Initialize your API client here\n",
        "\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        # Implement the logic for making API calls to your model\n",
        "        # Use self.api_key for authentication if needed\n",
        "        print(f\"Calling API model with prompt: {prompt}\")\n",
        "        return \"Response from API model\" # Replace with actual API call\n",
        "\n",
        "class HuggingFaceModelWrapper(LLMWrapper):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        # Load your Hugging Face model and tokenizer here\n",
        "        # You might need to install the 'transformers' and 'torch' libraries\n",
        "        # pip install transformers torch\n",
        "        print(f\"Loading Hugging Face model: {model_name}\")\n",
        "        # Example: from transformers import pipeline\n",
        "        # self.generator = pipeline('text-generation', model=model_name)\n",
        "\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        # Implement the logic for generating responses using the Hugging Face model\n",
        "        print(f\"Calling Hugging Face model with prompt: {prompt}\")\n",
        "        # Example: result = self.generator(prompt)\n",
        "        return \"Response from Hugging Face model\" # Replace with actual generation\n",
        "\n",
        "# Example of adding another model type\n",
        "class AnotherModelWrapper(LLMWrapper):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        # Initialize your other model here\n",
        "\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        # Implement the logic for generating responses with this model\n",
        "        print(f\"Calling another model with prompt: {prompt}\")\n",
        "        return \"Response from another model\" # Replace with actual generation\n",
        "\n",
        "# Step 3: Instantiate your models\n",
        "models_to_test = [\n",
        "    APIModelWrapper(api_key=\"your_api_key\"),\n",
        "    HuggingFaceModelWrapper(model_name=\"gpt2\"), # Replace with your desired HF model\n",
        "    AnotherModelWrapper(config={\"setting\": \"value\"})\n",
        "]\n",
        "\n",
        "# Step 4: Iterate and test\n",
        "# Replace with your actual dataset loading logic\n",
        "dataset = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\n",
        "\n",
        "for prompt in dataset:\n",
        "    print(f\"\\nTesting prompt: {prompt}\")\n",
        "    for model in models_to_test:\n",
        "        response = model.generate_response(prompt)\n",
        "        print(f\"Model: {type(model).__name__}, Response: {response}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Lh9bJZJVE2Sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}