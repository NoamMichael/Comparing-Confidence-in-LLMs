{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "D80L4tLeW2n3"
      ],
      "authorship_tag": "ABX9TyOQTi3ngE7cbzTn0BsWJIOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoamMichael/Comparing-Confidence-in-LLMs/blob/main/LSAT_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WuTZXLzGAGkZ"
      },
      "outputs": [],
      "source": [
        "# This Notebook will test all models on the formatted LSAT-AR dataset\n",
        "# I have no issue running multiple API clients simultaneously. However, running\n",
        "# local models is pretty memory intensive so I can only run one at a time.\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "#                 TO DO LIST:\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "# (X) Implement GetRAC() for Anthropic and Gemini Models\n",
        "# (X) Fix formatdf() function (Daniels implementation)\n",
        "# (X) Fix how systemprompts work (maybe make a metaclass var?)\n",
        "#       --Honestly works fine now as global var. I dont see a need to change it.\n",
        "# (X) Make an init_models() function:\n",
        "#     def init_functions(models_dict):\n",
        "#         \"Do some stuff\"\n",
        "#         return models_list\n",
        "# (X) Clean up notebook\n",
        "\n",
        "\n",
        "## Stopped at 133 / 1620\n",
        "## ---Find out how to get beter Gini Coef. in confidence distribution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "%pip install openai\n",
        "%pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_36QvDiPdjy",
        "outputId": "4f4f9305-cd78-4c6d-c772-8608b4b1cb40"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.53.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (AutoTokenizer,\n",
        "                        AutoModelForCausalLM,\n",
        "                        BitsAndBytesConfig,\n",
        "                        pipeline)\n",
        "import warnings\n",
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm.notebook import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata\n",
        "\n",
        "max_tokens = 150\n",
        "\n",
        "class OpenModel: ## This class is built around Hugging Face methods\n",
        "  def __init__(self, name, key, MaxTokens = 150):\n",
        "    self.name = name\n",
        "    self.key = key\n",
        "    self.MaxTokens = MaxTokens\n",
        "    print(f\"Downloading Tokenizer for {self.name}\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name,token = self.key) ## Import Tokenizer\n",
        "    print(f\"Downloading Model Weights for {self.name}\")\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.name, token = self.key, device_map=\"auto\") ## Import Model\n",
        "\n",
        "    ## Make text generation pipeline\n",
        "    self.pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = self.model,\n",
        "    tokenizer = self.tokenizer,\n",
        "    do_sample = False,\n",
        "    max_new_tokens = self.MaxTokens,\n",
        "    eos_token_id = self.tokenizer.eos_token_id,\n",
        "    pad_token_id = self.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  def generate(self, prompt):\n",
        "    return self.pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "  def GetTokens(self, prompt: str):\n",
        "    ## Get Answer:\n",
        "    batch = self.tokenizer(prompt, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    ## Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "\n",
        "    ##Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    ## Convert token indices to tokens\n",
        "    top_k_tokens = [self.tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    ## Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "\n",
        "    ## Create a Pandas Series with tokens as index and probabilities as values\n",
        "    logit_series = pd.Series(top_k_probs, index=top_k_tokens)\n",
        "\n",
        "    ## Sort the series by values in descending order\n",
        "    logit_series = logit_series.sort_values(ascending=False)\n",
        "    logit_series.index.name = \"Token\"\n",
        "    logit_series.name = \"Probability\"\n",
        "    return logit_series\n",
        "\n",
        "class ClosedModel(ABC):\n",
        "  @abstractmethod\n",
        "  def generate(self, prompt: str, system:str = \"\")-> str:\n",
        "        \"\"\"\n",
        "        Abstract method to generate a response from the language model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "  @abstractmethod\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Reasoning', 'Error'])\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def client(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def GetRAC(self, prompt: str, system1:str = \"\", system2: str = \"\")-> tuple[str, str]: ## Get Reasoning Answer Confidence\n",
        "    pass\n",
        "\n",
        "class GPTmodel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Reasoning', 'Error'])\n",
        "  def client(self):\n",
        "    # Initialize the OpenAI client with the API key\n",
        "    self.client = openai.OpenAI(api_key=self.key)\n",
        "\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Use the new client-based API call\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens= max_tokens\n",
        "    )\n",
        "    # Access the content from the new response object structure\n",
        "    return response.choices[0].message.content\n",
        "  def GetRAC(self, prompt: str)-> tuple[str, str]: ## Get Reasoning Answer Confidence\n",
        "    ## For context here's the two system prompts:\n",
        "    ## System prompt 1:\n",
        "    '''\n",
        "    Given the following question, analyze the options, and provide a concise reasoning for your selected answer. Your reasoning should not exceed 100 words. After your explanation, clearly state your answer by choosing one of the options listed (A, B, C, D, or E).\n",
        "\n",
        "    Question: ${Question}\n",
        "    Options:\n",
        "    A) ${Option A}\n",
        "    B) ${Option B}\n",
        "    C) ${Option C}\n",
        "    D) ${Option D}\n",
        "    E) ${Option E}\n",
        "\n",
        "    Please provide your reasoning first, limited to 100 words, and then conclusively state only your selected answer using the corresponding letter (A, B, C, D, or E).\n",
        "    Reasoning: <Your concise reasoning here. Max 100 words>\n",
        "    '''\n",
        "\n",
        "    ## System prompt 2:\n",
        "    '''\n",
        "    Based on the reasoning above, Provide the correct answer and the likelihood that each option is correct from 0.0 to 1.0 in a JSON format. The four probabilities should sum to 1.0. For example:\n",
        "\n",
        "    {\n",
        "    'Answer': <Your answer choice here, as a single letter and nothing else.>\n",
        "    'A': <Probability choice A is correct. As a float from 0.0 to 1.0>,\n",
        "    'B': <Probability choice B is correct. As a float from 0.0 to 1.0>,\n",
        "    'C': <Probability choice C is correct. As a float from 0.0 to 1.0>,\n",
        "    'D': <Probability choice D is correct. As a float from 0.0 to 1.0>,\n",
        "    'E': <Probability choice E is correct. As a float from 0.0 to 1.0>\n",
        "    }\n",
        "    '''\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1, sys_prompt2\n",
        "    ## Get the reasoning\n",
        "    reasoning = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    answer_confidence = self.generate(prompt + reasoning + sys_prompt2, sys_prompt2)\n",
        "\n",
        "    return reasoning, answer_confidence\n",
        "\n",
        "class AnthropicModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Reasoning', 'Error'])\n",
        "  def client(self):\n",
        "    # Initialize the Anthropic client with the API key\n",
        "    self.client = anthropic.Anthropic(api_key=self.key)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # The messages list should only contain user and assistant roles\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Use the Anthropic client to create a message\n",
        "    # Pass the system message as a top-level 'system' parameter\n",
        "    message = self.client.messages.create(\n",
        "        model=self.name,\n",
        "        max_tokens= max_tokens, # You can adjust this or make it an instance variable\n",
        "        messages=messages,\n",
        "        system=system if system else None # Pass system as a separate parameter, or None if empty\n",
        "    )\n",
        "    # Access the content from the response object\n",
        "    return message.content[0].text\n",
        "\n",
        "  def GetRAC(self, prompt: str)-> tuple[str, str]: ## Get Reasoning Answer Confidence\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1, sys_prompt2\n",
        "    ## Get the reasoning\n",
        "    reasoning = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    answer_confidence = self.generate(prompt + reasoning + sys_prompt2, sys_prompt2)\n",
        "\n",
        "    return reasoning, answer_confidence\n",
        "class GeminiModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Reasoning', 'Error'])\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the google.generativeai client with the API key\n",
        "\n",
        "    genai.configure(api_key=self.key)\n",
        "    self.model = genai.GenerativeModel(model_name=self.name)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Build the content list, including the system message if provided\n",
        "    contents = [{\"role\": \"user\", \"parts\": [prompt]}]\n",
        "    if system:\n",
        "        contents = [{\"role\": \"user\", \"parts\": [system]}] + contents\n",
        "\n",
        "    # Use the Gemini model to generate content\n",
        "    response = self.model.generate_content(contents)\n",
        "\n",
        "    # Access the content from the response object\n",
        "    return response.text\n",
        "\n",
        "  def GetRAC(self, prompt: str)-> tuple[str, str]: ## Get Reasoning Answer Confidence\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1, sys_prompt2\n",
        "    ## Get the reasoning\n",
        "    reasoning = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    answer_confidence = self.generate(prompt + reasoning + sys_prompt2, sys_prompt2)\n",
        "\n",
        "    return reasoning, answer_confidence\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqNsBM1oBHmp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Functions:"
      ],
      "metadata": {
        "id": "bJhTlj9pXqdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Functions:\n",
        "def init_models(models_dict,\n",
        "                test_prompt = \"Zdzisław Beksiński was\",\n",
        "                test_system = \"You are a helpful assistant.\"):\n",
        "  print('Initializing Closed Models:')\n",
        "  closed_models = []\n",
        "  for model_type in my_closed_models:\n",
        "      print(f'{model_type}:')\n",
        "      api_key_name = my_closed_models[model_type]['api_key_name']\n",
        "      api_key = userdata.get(api_key_name)\n",
        "      print(f'  API Key Name: {my_closed_models[model_type][\"api_key_name\"]}')\n",
        "      for model_name in my_closed_models[model_type]['models']:\n",
        "        # Instantiate the correct subclass based on model_type\n",
        "        if model_type == 'GPT':\n",
        "            my_model = GPTmodel(name = model_name, api_key = api_key)\n",
        "        elif model_type == 'Claude':\n",
        "            my_model = AnthropicModel(name = model_name, api_key = api_key)\n",
        "        elif model_type == 'Gemini':\n",
        "            my_model = GeminiModel(name = model_name, api_key = api_key)\n",
        "        else:\n",
        "            # Handle unexpected model types if necessary\n",
        "            print(f\"Warning: Unknown model type {model_type}. Skipping.\")\n",
        "            continue # Skip to the next model name if type is unknow\n",
        "        my_model.client()\n",
        "        closed_models.append(my_model)\n",
        "        print(f'    {model_name}')\n",
        "\n",
        "\n",
        "  print(f'Models Initialized: {len(closed_models)}')\n",
        "  print(f'Model locations:\\n{closed_models}')\n",
        "\n",
        "  print('-'*42)\n",
        "  print('Testing all closed models:')\n",
        "  print(f'Test prompt: {test_prompt}')\n",
        "  print(f'Test system: {test_system}')\n",
        "\n",
        "  for model in closed_models:\n",
        "    print(f'\\nTesting model: {model.name}')\n",
        "    print(model.generate(test_prompt, test_system))\n",
        "  return closed_models\n",
        "\n",
        "def make_system_prompt(df, sys_prompt1 = '', sys_prompt2 = ''):\n",
        "  if sys_prompt1 == '' and sys_prompt2 == '':\n",
        "\n",
        "    sys_prompt1 = '''\n",
        "    Given the following question, analyze the options, and provide a concise reasoning for your selected answer. Your reasoning should not exceed 100 words. After your explanation, clearly state your answer by choosing one of the options listed (A, B, C, D, or E).\n",
        "\n",
        "    Question: ${Question}\n",
        "    Options:\n",
        "    A) ${Option A}\n",
        "    B) ${Option B}\n",
        "    C) ${Option C}\n",
        "    D) ${Option D}\n",
        "    E) ${Option E}\n",
        "\n",
        "    Please provide your reasoning first, limited to 100 words, and then conclusively state only your selected answer using the corresponding letter (A, B, C, D, or E).\n",
        "    Reasoning: <Your concise reasoning here. Max 100 words>\n",
        "    '''\n",
        "    sys_prompt2 = '''\n",
        "    Based on the reasoning above, Provide the correct answer and the likelihood that each option is correct from 0.0 to 1.0 in a JSON format. The four probabilities should sum to 1.0. For example:\n",
        "\n",
        "    {\n",
        "    'Answer': <Your answer choice here, as a single letter and nothing else.>\n",
        "    'A': <Probability choice A is correct. As a float from 0.0 to 1.0>,\n",
        "    'B': <Probability choice B is correct. As a float from 0.0 to 1.0>,\n",
        "    'C': <Probability choice C is correct. As a float from 0.0 to 1.0>,\n",
        "    'D': <Probability choice D is correct. As a float from 0.0 to 1.0>,\n",
        "    'E': <Probability choice E is correct. As a float from 0.0 to 1.0>\n",
        "    }\n",
        "\n",
        "    Do not provide any additional reasoning.\n",
        "    '''\n",
        "  ## Edit system Prompts in order to match size of dataset\n",
        "  columns = df.columns\n",
        "  num_options = columns.str.contains('Option').astype(int).sum()\n",
        "\n",
        "  sys_prompt_temp1 = sys_prompt1\n",
        "  sys_prompt_temp2 = sys_prompt2\n",
        "  ## Reformat system prompt in order to fit number of options in benchmark\n",
        "  if num_options < 5: ## ABCD\n",
        "    sys_prompt_temp1 = (sys_prompt1\n",
        "                  .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                  .replace('E) ${Option E}', '') ## Drop option E\n",
        "        )\n",
        "    sys_prompt_temp2 = (sys_prompt2\n",
        "                  .replace('(A, B, C, D, or E)', '(A, B, C, or D)') ## Change the available options\n",
        "                  .replace('E) ${Option E}', '') ## Drop option E\n",
        "        )\n",
        "    if num_options < 4: ## ABC\n",
        "      sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                    .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                    .replace('D) ${Option D}', '') ## Drop option D\n",
        "          )\n",
        "      sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                  .replace('(A, B, C, or D)', '(A, B, or C)') ## Change the available options\n",
        "                  .replace('D) ${Option D}', '') ## Drop option D\n",
        "        )\n",
        "\n",
        "      if num_options < 3: ## AB\n",
        "        sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                      .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                      .replace('C) ${Option C}', '') ## Drop option C\n",
        "            )\n",
        "        sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                    .replace('(A, B, or C)', '(A or B)') ## Change the available options\n",
        "                    .replace('C) ${Option C}', '') ## Drop option C\n",
        "          )\n",
        "\n",
        "  return sys_prompt_temp1, sys_prompt_temp2\n",
        "\n",
        "def format_df(df):\n",
        "\n",
        "  ## %%%%%%%%%%%%%%\n",
        "  ## I need to fix how formating is done for some Q's. As daniel pointed out some\n",
        "  ## questions only have 4 options, not 5.\n",
        "  ## %%%%%%%%%%%%%%\n",
        "\n",
        "  ## Takes in a dataframe in the form:\n",
        "  ## | Question Number | Question | Option A | Option B | ... | Correct Answer Letter |\n",
        "  ## |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "  ##\n",
        "  ## Returns a dataframe in the form:\n",
        "  ## | Question Number | Full Prompt 1 | Full Prompt 2 |\n",
        "  ## |     (Int)       |    (Str)      |    (Str)      |\n",
        "\n",
        "  columns = df.columns\n",
        "  num_options = columns.str.contains('Option').astype(int).sum()\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Check if DF is formatted properly\n",
        "  error_text = f'''Make sure dataframe is in following format:\n",
        "  | Question Number | Question | Option A | Option B | ... | Correct Answer Letter |\n",
        "  |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "\n",
        "  The current format of Dataframe is: {columns}\n",
        "  '''\n",
        "  ['Question Number', 'Question', 'Correct Answer Letter']\n",
        "  if num_options < 2:\n",
        "    raise Exception(error_text)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Initialize Output dataframe:\n",
        "  header = ['Question Num', 'Full Prompt 1', 'Full Prompt 2']\n",
        "  output_df = pd.DataFrame(columns = header)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "\n",
        "  ## Format questions for benchmark\n",
        "  letters = ['A', 'B', 'C', 'D', 'E']\n",
        "  options = ['Option A', 'Option B', 'Option C', 'Option D', 'Option E']\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    question = df['Question'][i]\n",
        "\n",
        "    sys_prompt_temp1 = sys_prompt1\n",
        "    sys_prompt_temp2 = sys_prompt2\n",
        "    ## Reformat system prompt in order to fit number of options in benchmark\n",
        "    if type(df['Option E'][i]) == float: ## ABCD\n",
        "      sys_prompt_temp1 = (sys_prompt1\n",
        "                    .replace('(A, B, C, D, or E)', '(A, B, C, or D)')\n",
        "                    .replace('E) ${Option E}', '')\n",
        "          )\n",
        "      sys_prompt_temp2 = (sys_prompt2\n",
        "                    .replace('(A, B, C, D, or E)', '(A, B, C, or D)')\n",
        "                    .replace('E) ${Option E}', '')\n",
        "          )\n",
        "      if type(df['Option D'][i]) == float: ## ABC\n",
        "        sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                      .replace('(A, B, C, or D)', '(A, B, or C)')\n",
        "                      .replace('D) ${Option D}', '')\n",
        "            )\n",
        "        sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                    .replace('(A, B, C, or D)', '(A, B, or C)')\n",
        "                    .replace('D) ${Option D}', '')\n",
        "          )\n",
        "\n",
        "        if type(df['Option C'][i]) == float: ## AB\n",
        "          sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                        .replace('(A, B, or C)', '(A or B)')\n",
        "                        .replace('C) ${Option C}', '')\n",
        "              )\n",
        "          sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                      .replace('(A, B, or C)', '(A or B)')\n",
        "                      .replace('C) ${Option C}', '')\n",
        "            )\n",
        "\n",
        "    option_text = df[options[:num_options]].iloc[i].to_list()\n",
        "    ## Prompt for specific question\n",
        "    new_prompt = sys_prompt_temp1.replace('${Question}', question)\n",
        "    for j in range(num_options): ## This for loop allows for dynamic question amounts\n",
        "        new_prompt = new_prompt.replace(f'${{Option {letters[j]}}}', str(option_text[j]))\n",
        "\n",
        "\n",
        "    ## Add formatted prompts.\n",
        "    ## Note that this is formatted to llama so changes may be needed down the line.\n",
        "    prompts1 = (new_prompt.split('<Your concise reasoning here. Max 100 words>')[0]) ## Specific prompt for question\n",
        "\n",
        "    prompts2 = (sys_prompt_temp2) ## Generic prompt for question confidence\n",
        "    output_df.loc[i] = [df['Question Number'].iloc[i], prompts1, prompts2]\n",
        "\n",
        "  return output_df\n",
        "\n",
        "def test_models_sequential_by_question(df, models, debug=False):\n",
        "    \"\"\"\n",
        "    Tests a list of models on a given dataset sequentially,\n",
        "    iterating through questions and then models for each question.\n",
        "    Includes a debug mode to process only the first 10 questions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset containing questions and prompts.\n",
        "        models (list): A list of initialized model objects.\n",
        "        debug (bool): If True, only process the first 10 questions.\n",
        "    \"\"\"\n",
        "    print(\"Clearing previous results for each model...\")\n",
        "    for model in models:\n",
        "        model.results = pd.DataFrame(columns=['Question ID', 'Question', 'Answer', 'Reasoning', 'Error'])\n",
        "        print(f\"  Cleared results for {model.name}\")\n",
        "    print(\"Starting sequential testing (by question)...\")\n",
        "\n",
        "    # Determine the number of questions to process\n",
        "    num_questions_to_process = 10 if debug else len(df)\n",
        "\n",
        "    # Iterate over questions first\n",
        "    for index, row in tqdm(df.head(num_questions_to_process).iterrows(), total=num_questions_to_process, desc=\"Processing Questions\"):\n",
        "        question_num = row['Question Num']\n",
        "        prompt = row['Full Prompt 1']\n",
        "        '''\n",
        "\n",
        "        I dont love this implementation But honestly,\n",
        "        unless my logic is flawed with some edge case, I think this should work\n",
        "        and I dont want to rewrite the functions for each subclass to take in the\n",
        "        dataframe in order to work.\n",
        "        '''\n",
        "        global sys_prompt2\n",
        "        sys_prompt2 = row['Full Prompt 2']\n",
        "\n",
        "        print(f\"\\nProcessing Question {question_num}\")\n",
        "\n",
        "        # Iterate over models for the current question\n",
        "        for model in models:\n",
        "\n",
        "            try:\n",
        "                print(f\"  Testing with model: {model.name}\")\n",
        "                # Call GetRAC and add the result to the model's self.results\n",
        "                reasoning, answer_confidence = model.GetRAC(prompt=prompt)\n",
        "\n",
        "                # Add the results to the model's self.results DataFrame\n",
        "                new_row = pd.DataFrame([{\n",
        "                    'Question ID': question_num,\n",
        "                    'Question': prompt,\n",
        "                    'Answer': answer_confidence,\n",
        "                    'Reasoning': reasoning,\n",
        "                    'Error': False\n",
        "                }])\n",
        "                model.results = model.results._append(new_row, ignore_index=True)\n",
        "                filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "                model.results.to_csv(filename, index=False)\n",
        "            except Exception as e:\n",
        "                print(f\"  Error testing {model.name} on Question {question_num}: {e}\")\n",
        "                # Optionally add an error entry to the results\n",
        "                error_row = pd.DataFrame([{\n",
        "                    'Question ID': question_num,\n",
        "                    'Question': prompt,\n",
        "                    'Answer': f\"Error: {e}\",\n",
        "                    'Reasoning': f\"Error: {e}\",\n",
        "                    'Error': True\n",
        "                }])\n",
        "                model.results = model.results._append(error_row, ignore_index=True)\n",
        "                filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "                model.results.to_csv(filename, index=False)\n",
        "    print(\"\\nSequential testing complete.\")\n",
        "\n",
        "    # After processing all questions, save the results for each model\n",
        "    for model in models:\n",
        "        filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "        model.results.to_csv(filename, index=False)\n",
        "        print(f\"Results for {model.name} saved to '{filename}'\")"
      ],
      "metadata": {
        "id": "lQehmt1zXh75"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Playground:"
      ],
      "metadata": {
        "id": "D80L4tLeW2n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Llama\n",
        "hf_llama_token = userdata.get('hf_llama_token')\n",
        "test_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
        "test_key = hf_llama_token\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "\n",
        "test_model = OpenModel(name = test_name, key = test_key)"
      ],
      "metadata": {
        "id": "sA1Rq7e6HlTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_model.generate(test_prompt))\n",
        "test_model.GetTokens(test_prompt)"
      ],
      "metadata": {
        "id": "H-bj6Bn9HsSF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for GPT\n",
        "\n",
        "gpt_4_key = userdata.get('gpt_api_key')\n",
        "test_name = 'gpt-4'\n",
        "test_key = gpt_4_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\"\n",
        "\n",
        "my_gpt = GPTmodel(name = test_name, api_key = test_key)\n",
        "my_gpt.client()\n",
        "my_gpt.generate(test_prompt, test_system)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1r9p6BsQO5_Q",
        "outputId": "3d8a8a9b-0909-4014-d5cd-448219575f44"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. Beksiński's works are characterized by their haunting, dreamlike nature, often featuring desolate landscapes and grotesque, distorted figures. Despite the grim themes, he insisted his work was not to be interpreted in a literal sense, and that he was more interested in the emotions and reactions they provoked. He was born on February 24, \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Claude\n",
        "\n",
        "claude_key = userdata.get('claude_api_key')\n",
        "test_name = 'claude-3-haiku-20240307'\n",
        "test_key = claude_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\"\n",
        "\n",
        "my_claude = AnthropicModel(name = test_name, api_key = test_key)\n",
        "my_claude.client()\n",
        "my_claude.generate(test_prompt, test_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WrdIcyVUff4I",
        "outputId": "6e42ad11-8abf-441a-a6dc-76777cff72c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor who was known for his distinctive surrealist and dystopian style. Here are some key facts about him:\\n\\n- Born in 1929 in Sanok, Poland, Beksiński initially studied to be an architect before turning to art.\\n\\n- His paintings often depicted post-apocalyptic, nightmarish landscapes and figures. His style was highly detailed and technical, with a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Playground for Gemini\n",
        "\n",
        "gemini_api_key = userdata.get('gemini_api_key') # Assuming you stored your key in Userdata\n",
        "test_name = \"gemini-2.0-flash\" # Or another Gemini model name like 'gemini-1.5-flash'\n",
        "test_key = gemini_api_key\n",
        "test_prompt = \"Zdzisław Beksiński was\"\n",
        "test_system = \"You are a helpful assistant.\" # Optional system message\n",
        "\n",
        "my_gemini = GeminiModel(name = test_name, api_key = test_key)\n",
        "my_gemini.client()\n",
        "my_gemini.generate(test_prompt, test_system)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dG59Di8PtRH4",
        "outputId": "023d6b33-7d38-47b7-9768-ba4b044efed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zdzisław Beksiński was a Polish painter, photographer, and sculptor specializing in dystopian surrealism. He is known for his distinctive and hauntingly beautiful, yet often disturbing, imagery. His art explores themes of death, decay, anxiety, and the human condition.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Benchmarking:"
      ],
      "metadata": {
        "id": "BUPi26fXW6aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Dataset\n",
        "print('-' *42)\n",
        "file_path = '/content/LSAT_formatted.csv'\n",
        "print(f'Importing Dataset: {file_path}')\n",
        "dataset = pd.read_csv(file_path)\n",
        "dataset.head()\n",
        "\n",
        "## Edit System Prompts\n",
        "print('-' *42)\n",
        "print('Editing System Prompts:')\n",
        "\n",
        "sys_prompt1 = '''\n",
        "Given the following question, analyze the options, and provide a concise reasoning for your selected answer. Your reasoning should not exceed 100 words. After your explanation, clearly state your answer by choosing one of the options listed (A, B, C, D, or E).\n",
        "\n",
        "Question: ${Question}\n",
        "Options:\n",
        "A) ${Option A}\n",
        "B) ${Option B}\n",
        "C) ${Option C}\n",
        "D) ${Option D}\n",
        "E) ${Option E}\n",
        "\n",
        "Please provide your reasoning first, limited to 100 words, and then conclusively state only your selected answer using the corresponding letter (A, B, C, D, or E).\n",
        "Reasoning: <Your concise reasoning here. Max 100 words>\n",
        "'''\n",
        "sys_prompt2 = '''\n",
        "Based on the reasoning above, Provide the correct answer and the likelihood that each option is correct from 0.0 to 1.0 in a JSON format. The four probabilities should sum to 1.0. For example:\n",
        "\n",
        "{\n",
        "'Answer': <Your answer choice here, as a single letter and nothing else.>\n",
        "'A': <Probability choice A is correct. As a float from 0.0 to 1.0>,\n",
        "'B': <Probability choice B is correct. As a float from 0.0 to 1.0>,\n",
        "'C': <Probability choice C is correct. As a float from 0.0 to 1.0>,\n",
        "'D': <Probability choice D is correct. As a float from 0.0 to 1.0>,\n",
        "'E': <Probability choice E is correct. As a float from 0.0 to 1.0>\n",
        "}\n",
        "\n",
        "Do not provide any additional reasoning.\n",
        "'''\n",
        "\n",
        "print('System Prompts:')\n",
        "print(f'  {sys_prompt1}')\n",
        "print(f'  {sys_prompt2}')\n",
        "\n",
        "\n",
        "\n",
        "## Format DF\n",
        "print('-' *42)\n",
        "print('Formatting Dataset:')\n",
        "new_dataset = format_df(dataset)\n",
        "print(' Successfully Formatted Dataset')\n",
        "print('New Dataset:')\n",
        "display(new_dataset.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Initialize Models\n",
        "print('-' *42)\n",
        "print('Initializing Models:')\n",
        "my_closed_models = {\n",
        "    'GPT': {\n",
        "        'api_key_name': 'gpt_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gpt-4',\n",
        "            'gpt-3.5-turbo'\n",
        "            ''\n",
        "        ]\n",
        "    },\n",
        "    'Claude': {\n",
        "        'api_key_name': 'claude_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'claude-3-7-sonnet-20250219',\n",
        "            #'claude-3-haiku-20240307'\n",
        "        ]\n",
        "    },\n",
        "    'Gemini': {\n",
        "        'api_key_name': 'gemini_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gemini-1.5-flash',\n",
        "            'gemini-2.5-pro-preview-06-05'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "closed_models = init_models(my_closed_models)\n",
        "print(' Successfully Initialied Models')\n",
        "## Test Models on LSAT\n",
        "print('-' *42)\n",
        "print('Testing Models:')\n",
        "test_models_sequential_by_question(new_dataset, closed_models, debug=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t8CFGxFDSznx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}