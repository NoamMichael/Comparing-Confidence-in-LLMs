{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2436248",
   "metadata": {},
   "source": [
    "# Comparing Calibration Data Across All Models / Question Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8736a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9704e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f7b78",
   "metadata": {},
   "source": [
    "## Combine All Results Into Composite CSV\n",
    "\n",
    "First we will read from the Parsed Results folder and make a dictionary that abstracts the structure of the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009288cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Claude', 'Deepseek', 'Gemini', 'GPT', 'Llama'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the folder and create a dictionary to model the structure of the files\n",
    "\n",
    "def folder_tree_dict(root, *, include_files=True, follow_symlinks=False, ignore_hidden=True):\n",
    "    root = Path(root)\n",
    "\n",
    "    def build(p: Path):\n",
    "        out = {}\n",
    "        for entry in sorted(p.iterdir(), key=lambda x: (x.is_file(), x.name.lower())):\n",
    "            if ignore_hidden and entry.name.startswith(\".\"):\n",
    "                continue\n",
    "            try:\n",
    "                if entry.is_dir() and (follow_symlinks or not entry.is_symlink()):\n",
    "                    out[entry.name] = build(entry)\n",
    "                else:\n",
    "                    if include_files:\n",
    "                        out[entry.name] = None  # or {\"size\": entry.stat().st_size}\n",
    "            except PermissionError:\n",
    "                out[entry.name] = \"<permission-denied>\"\n",
    "        return out\n",
    "\n",
    "    return {root.name: build(root)}\n",
    "\n",
    "\n",
    "folder_path = r\"Parsed Results\"\n",
    "\n",
    "folder_abstraction_dict = folder_tree_dict(folder_path)[folder_path]\n",
    "\n",
    "folder_abstraction_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f850c",
   "metadata": {},
   "source": [
    "### Combined CSV\n",
    "We want to make a well-formed CSV for future analysis. This CSV will have the following fields for columns:\n",
    "\n",
    "```text\n",
    "Question Set (str) ---------------- Required: The display name of the question set\n",
    "Question ID (str) ----------------- Required: The Question ID\n",
    "Model (str) ----------------------- Required: The model that provided the response (e.g. Llama-3.1-8B-Instruct)\n",
    "Model Type (str) ------------------ Required: The family of models which  the model (e.g. Llama)\n",
    "Coerce (Bool) --------------------- Required: Whether the parser was able to understand the response\n",
    "\n",
    "Question (str) -------------------- Required: The question posed to the model\n",
    "Correct Answer (str) -------------- Optional: Depends on Question Set (LifeEval is different than others)\n",
    "Content (str) --------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Reasoning (str) ------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Answer (str) ---------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Score (float) --------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "\n",
    "Stated Confidence Answer (float) -- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence A (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence B (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence C (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence D (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence E (float)-------- Optional: Depends on Question Set (NA if not available)\n",
    "\n",
    "Token Probability Answer (float) -- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability A (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability B (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability C (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability D (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability E (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968714d",
   "metadata": {},
   "source": [
    "---\n",
    "### Process All CSVs Into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac22891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Results/Claude\n",
      "    Parsed Results/Claude/claude-3-7-sonnet-20250219\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/Claude/claude-3-haiku-20240307\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/Claude/claude-sonnet-4-20250514\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "Parsed Results/Deepseek\n",
      "    Parsed Results/Deepseek/deepseek-r1\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/Deepseek/deepseek-v3\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "Parsed Results/Gemini\n",
      "    Parsed Results/Gemini/gemini-2.5-flash\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/Gemini/gemini-2.5-pro\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "Parsed Results/GPT\n",
      "    Parsed Results/GPT/gpt-4o\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/GPT/o3-2025-04-16\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "Parsed Results/Llama\n",
      "    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n",
      "    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct\n",
      "        boolq_valid    \n",
      "        halu_eval_qa    \n",
      "        life_eval    \n",
      "        lsat_ar_test    \n",
      "        sat_en    \n",
      "        sciq_test    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Stated Confidence A</th>\n",
       "      <th>Stated Confidence B</th>\n",
       "      <th>Stated Confidence C</th>\n",
       "      <th>Stated Confidence D</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House tax and property tax are often used inte...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"House tax and property ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reasoning Answer  \\\n",
       "0  The question is asking whether the production ...  False   \n",
       "1  House tax and property tax are often used inte...   True   \n",
       "2  This question is asking about phantom limb pai...   True   \n",
       "3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "\n",
       "  Stated Confidence Answer  Coerce  \\\n",
       "0                      0.6    True   \n",
       "1                     0.85    True   \n",
       "2                     0.95    True   \n",
       "3                     0.98    True   \n",
       "4                     0.98    True   \n",
       "\n",
       "                                             Content Question ID  \\\n",
       "0  {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "1  {\\n    \"Reasoning\": \"House tax and property ta...           1   \n",
       "2  {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "3  {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "4  {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "\n",
       "                        Model Model Type Question Set  Stated Confidence A  \\\n",
       "0  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "1  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "2  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "3  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "4  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "\n",
       "   Stated Confidence B  Stated Confidence C  Stated Confidence D  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   Stated Confidence E  Token Probability True  Token Probability False  \\\n",
       "0                  NaN                     NaN                      NaN   \n",
       "1                  NaN                     NaN                      NaN   \n",
       "2                  NaN                     NaN                      NaN   \n",
       "3                  NaN                     NaN                      NaN   \n",
       "4                  NaN                     NaN                      NaN   \n",
       "\n",
       "   Token Probability Answer  Token Probability A  Token Probability B  \\\n",
       "0                       NaN                  NaN                  NaN   \n",
       "1                       NaN                  NaN                  NaN   \n",
       "2                       NaN                  NaN                  NaN   \n",
       "3                       NaN                  NaN                  NaN   \n",
       "4                       NaN                  NaN                  NaN   \n",
       "\n",
       "   Token Probability C  Token Probability D  Token Probability E  \n",
       "0                  NaN                  NaN                  NaN  \n",
       "1                  NaN                  NaN                  NaN  \n",
       "2                  NaN                  NaN                  NaN  \n",
       "3                  NaN                  NaN                  NaN  \n",
       "4                  NaN                  NaN                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "clean_df = pd.DataFrame()\n",
    "\n",
    "qset_rename = {\n",
    " 'boolq_valid': \"BoolQ\",\n",
    " 'halu_eval_qa': \"HaluEval\",\n",
    " 'life_eval': \"LifeEval\",\n",
    " 'lsat_ar_test': \"LSAT-AR\",\n",
    " 'sat_en': \"SAT-EN\",\n",
    " 'sciq_test':\"SciQ\"\n",
    "}\n",
    "\n",
    "\n",
    "mcq_qsets = ['LSAT-AR', 'SAT-EN', 'SciQ']\n",
    "\n",
    "\n",
    "for model_type, models in folder_abstraction_dict.items():\n",
    "    model_type_path = folder_path + f\"/{model_type}\"\n",
    "    print(model_type_path)\n",
    "    for model_name, qsets in models.items():\n",
    "        model_path = model_type_path + f\"/{model_name}\"\n",
    "        print(f\"    {model_path}\")\n",
    "        for qset_file_name in qsets:\n",
    "            splitter = f\"_{model_name}\"\n",
    "            qset_name = qset_file_name.split(splitter)[0]\n",
    "            qset_path = model_path + f\"/{qset_file_name}\"\n",
    "            \n",
    "            #--------- Write a function to spit out a dataframe w/ model_name, qset_name, true_answer and concat it\n",
    "\n",
    "            source_df = pd.read_csv(qset_path)\n",
    "\n",
    "\n",
    "            print(f\"        {qset_name}    \")\n",
    "            source_df[\"Model\"] = model_name\n",
    "            source_df[\"Model Type\"] = model_type\n",
    "\n",
    "            qset_display = qset_rename[qset_name]\n",
    "            source_df[\"Question Set\"] = qset_display\n",
    "            source_df[\"Question ID\"] = source_df[\"Question ID\"].astype(str)\n",
    "\n",
    "            combined_df = pd.concat([combined_df, source_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df.drop([\"Unnamed: 0\", \"Question ID.1\"], axis = 1, inplace = True)\n",
    "\n",
    "## Still need to add correct answer and score\n",
    "\n",
    "col_rename_map ={\n",
    "# Metadata\n",
    "'Question Set': \"Question Set\",\n",
    "'Question ID': \"Question ID\",\n",
    "'Model': \"Model\",\n",
    "'Model Type': \"Model Type\",\n",
    "'coerce': \"Coerce\",\n",
    "\n",
    "# Model Response\n",
    "\n",
    "'content': \"Content\",\n",
    "'Reasoning': \"Reasoning\",\n",
    "'Answer': \"Answer\",\n",
    "\n",
    "# Stated Confidence\n",
    "'Confidence': \"Stated Confidence Answer\",\n",
    "\"A\": \"Stated Confidence A\",\n",
    "\"B\": \"Stated Confidence B\",\n",
    "'C': \"Stated Confidence C\",\n",
    "'D': \"Stated Confidence D\",\n",
    "'E': \"Stated Confidence E\",\n",
    "\n",
    "# Token Probability\n",
    "'True_prob': \"Token Probability True\",\n",
    "'False_prob': \"Token Probability False\",\n",
    "'Answer_prob': \"Token Probability Answer\",\n",
    "'A_prob': \"Token Probability A\",\n",
    "'B_prob': \"Token Probability B\",\n",
    "'C_prob': \"Token Probability C\",\n",
    "'D_prob': \"Token Probability D\",\n",
    "'E_prob': \"Token Probability E\"\n",
    "}\n",
    "\n",
    "combined_df = combined_df.rename(columns = col_rename_map)\n",
    "\n",
    "qset_rename = {\n",
    " 'boolq_valid': \"BoolQ\",\n",
    " 'halu_eval_qa': \"HaluEval\",\n",
    " 'life_eval': \"LifeEval\",\n",
    " 'lsat_ar_test': \"LSAT-AR\",\n",
    " 'sat_en': \"SAT-EN\",\n",
    " 'sciq_test':\"SciQ\"\n",
    "}\n",
    "\n",
    "#combined_df[\"Question Set\"] = combined_df[\"Question Set\"].map(qset_rename)\n",
    "\n",
    "with pd.option_context('display.max_columns', None,\n",
    "                       #'display.width', None,\n",
    "                       #'display.max_colwidth', None\n",
    "                       ):\n",
    "    display(combined_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb61b54",
   "metadata": {},
   "source": [
    "### Clean DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666f9edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Score</th>\n",
       "      <th>...</th>\n",
       "      <th>Stated Confidence Answer (MCQ)</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "      <th>combined_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4_BoolQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reasoning Answer  \\\n",
       "0  The question is asking whether the production ...  False   \n",
       "2  This question is asking about phantom limb pai...   True   \n",
       "3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "\n",
       "  Stated Confidence Answer  Coerce  \\\n",
       "0                      0.6    True   \n",
       "2                     0.95    True   \n",
       "3                     0.98    True   \n",
       "4                     0.98    True   \n",
       "\n",
       "                                             Content Question ID  \\\n",
       "0  {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "2  {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "3  {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "4  {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "\n",
       "                        Model Model Type Question Set  Score  ...  \\\n",
       "0  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "2  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "3  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "4  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "\n",
       "   Stated Confidence Answer (MCQ)  Token Probability True  \\\n",
       "0                             NaN                     NaN   \n",
       "2                             NaN                     NaN   \n",
       "3                             NaN                     NaN   \n",
       "4                             NaN                     NaN   \n",
       "\n",
       "   Token Probability False  Token Probability Answer  Token Probability A  \\\n",
       "0                      NaN                       NaN                  NaN   \n",
       "2                      NaN                       NaN                  NaN   \n",
       "3                      NaN                       NaN                  NaN   \n",
       "4                      NaN                       NaN                  NaN   \n",
       "\n",
       "   Token Probability B  Token Probability C  Token Probability D  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   Token Probability E  combined_name  \n",
       "0                  NaN        0_BoolQ  \n",
       "2                  NaN        2_BoolQ  \n",
       "3                  NaN        3_BoolQ  \n",
       "4                  NaN        4_BoolQ  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_clean = combined_df.copy()\n",
    "combined_clean['combined_name'] = combined_clean['Question ID'] + \"_\" + combined_clean[\"Question Set\"]\n",
    "\n",
    "s = combined_clean[\"combined_name\"].value_counts() % 11  #---------------- This seems to get rid of too much\n",
    "bad_qid = s.index[s.ne(0)].tolist()  \n",
    "\n",
    "combined_clean = combined_clean[~combined_clean[\"combined_name\"].isin(bad_qid)]\n",
    "\n",
    "\n",
    "# this gets the qid and qset where coerce is false\n",
    "bad_qid_qset = combined_df[combined_df[\"Coerce\"] == False][[\"Question ID\", \"Question Set\"]]\n",
    "\n",
    "#bad_qid_qset = bad_qid_qset[[\"Question ID\", \"Question Set\"]]\n",
    "\n",
    "bad = set(bad_qid_qset[\"Question ID\"] +  \"_\" + bad_qid_qset[\"Question Set\"])\n",
    "\n",
    "mask = ~combined_clean[\"combined_name\"].isin(bad)   # True = keep\n",
    "\n",
    "combined_clean = combined_clean[mask]\n",
    "combined_clean.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71da9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Score</th>\n",
       "      <th>...</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "      <th>combined_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barq's Root Beer is not a Pepsi product. It is...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Barq's Root Beer is not a...</td>\n",
       "      <td>5</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5_BoolQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reasoning Answer  \\\n",
       "0  The question is asking whether the production ...  False   \n",
       "2  This question is asking about phantom limb pai...   True   \n",
       "3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "5  Barq's Root Beer is not a Pepsi product. It is...  False   \n",
       "\n",
       "  Stated Confidence Answer  Coerce  \\\n",
       "0                      0.6    True   \n",
       "2                     0.95    True   \n",
       "3                     0.98    True   \n",
       "4                     0.98    True   \n",
       "5                     0.98    True   \n",
       "\n",
       "                                             Content Question ID  \\\n",
       "0  {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "2  {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "3  {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "4  {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "5  {\\n    \"Reasoning\": \"Barq's Root Beer is not a...           5   \n",
       "\n",
       "                        Model Model Type Question Set  Score  ...  \\\n",
       "0  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "2  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "3  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "4  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "5  claude-3-7-sonnet-20250219     Claude        BoolQ    1.0  ...   \n",
       "\n",
       "   Stated Confidence E  Token Probability True  Token Probability False  \\\n",
       "0                  NaN                     NaN                      NaN   \n",
       "2                  NaN                     NaN                      NaN   \n",
       "3                  NaN                     NaN                      NaN   \n",
       "4                  NaN                     NaN                      NaN   \n",
       "5                  NaN                     NaN                      NaN   \n",
       "\n",
       "   Token Probability Answer  Token Probability A  Token Probability B  \\\n",
       "0                       NaN                  NaN                  NaN   \n",
       "2                       NaN                  NaN                  NaN   \n",
       "3                       NaN                  NaN                  NaN   \n",
       "4                       NaN                  NaN                  NaN   \n",
       "5                       NaN                  NaN                  NaN   \n",
       "\n",
       "   Token Probability C  Token Probability D  Token Probability E  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "5                  NaN                  NaN                  NaN   \n",
       "\n",
       "   combined_name  \n",
       "0        0_BoolQ  \n",
       "2        2_BoolQ  \n",
       "3        3_BoolQ  \n",
       "4        4_BoolQ  \n",
       "5        5_BoolQ  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(69278, 24)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clean up MCQ Question Sets\n",
    "cc = combined_clean[combined_clean[\"Question Set\"].isin(mcq_qsets)]\n",
    "cc_letters = cc[[\"Stated Confidence A\", \"Stated Confidence B\", \"Stated Confidence C\",\"Stated Confidence D\",\"Stated Confidence E\",]].copy()\n",
    "sum_confidence = cc_letters.sum(axis = 1)\n",
    "\n",
    "con_mask = cc[sum_confidence == 0.0][\"combined_name\"]\n",
    "\n",
    "# Indixes that aren't in con_mask\n",
    "combined_clean = combined_clean.loc[~combined_clean[\"combined_name\"].isin(con_mask)]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "## Special mask for LifeEval:\n",
    "\n",
    "le_df = combined_clean[combined_clean[\"Question Set\"] == \"LifeEval\"]\n",
    "\n",
    "con_isnum = pd.to_numeric( le_df['Stated Confidence Answer'], errors='coerce').notna()\n",
    "le_bad_qid= le_df[~con_isnum][\"combined_name\"]\n",
    "\n",
    "\n",
    "combined_clean = combined_clean[(~combined_clean[\"combined_name\"].isin(le_bad_qid)) ]\n",
    "\n",
    "\n",
    "display(combined_clean.head(5))\n",
    "combined_clean.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae325426",
   "metadata": {},
   "source": [
    "# Score all Question Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcc209",
   "metadata": {},
   "source": [
    "---\n",
    "### Functions for Computing Scores on LifeEval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6a61724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "mcq_qsets = ['LSAT-AR', 'SAT-EN', 'SciQ']\n",
    "\n",
    "\n",
    "def get_age(qid: int) -> int:\n",
    "    if qid < 404:\n",
    "        return m.floor(\n",
    "            abs(\n",
    "                (qid) / 4\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        return m.floor(\n",
    "            abs(\n",
    "                (qid - 404) / 4\n",
    "                )\n",
    "            )\n",
    "\n",
    "def compute_prob(point_estimate: float,\n",
    "                 R: float,\n",
    "                 gender: str,          # 'male' or 'female' (case-insensitive)\n",
    "                 min_age: int,         # condition \"already lived at least min_age\"\n",
    "                 df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Using a life table with columns:\n",
    "      - 'Age'\n",
    "      - 'Death probability (MALE)'\n",
    "      - 'Death probability (FEMALE)'\n",
    "    compute P(death occurs within [point_estimate - R, point_estimate + R] | survived to min_age).\n",
    "    \"\"\"\n",
    "\n",
    "    if \"Age\" not in df.columns:\n",
    "        raise ValueError(\"Expected an 'Age' column in the life table.\")\n",
    "\n",
    "    g = gender.strip().lower()\n",
    "    if g not in (\"male\", \"female\"):\n",
    "        raise ValueError(\"gender must be 'male' or 'female'\")\n",
    "\n",
    "    qx_col_map = {\n",
    "        \"male\":   \"Death probability (MALE)\",\n",
    "        \"female\": \"Death probability (FEMALE)\",\n",
    "    }\n",
    "    qx_col = qx_col_map[g]\n",
    "    if qx_col not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{qx_col}' in the life table.\")\n",
    "\n",
    "\n",
    "    # Work on a clean copy sorted by Age\n",
    "    global tab\n",
    "    tab = df[[\"Age\", qx_col]].copy().sort_values(\"Age\").reset_index(drop=True).dropna()\n",
    "    tab.rename(columns={qx_col: \"q\"}, inplace=True)\n",
    "    tab[\"Age\"] = tab[\"Age\"].astype(int)\n",
    "    tab[\"q\"] = tab[\"q\"].astype(float)\n",
    "\n",
    "    # Integer-age window [lo, hi), clamped to table bounds and min_age\n",
    "    table_min = int(tab[\"Age\"].min())\n",
    "    table_max = int(tab[\"Age\"].max())  # last age with q_x for [x, x+1)\n",
    "    lo = max(int(np.floor(point_estimate - R)), int(min_age), table_min)\n",
    "    hi = min(int(np.ceil(point_estimate + R)), table_max + 1)  # exclusive upper bound\n",
    "\n",
    "    if hi <= lo:\n",
    "        return 0.0\n",
    "\n",
    "    # Align to contiguous ages and extract q_x as numpy\n",
    "    ages = np.arange(table_min, table_max + 1, dtype=int)\n",
    "    sub = tab.set_index(\"Age\").reindex(ages)\n",
    "    if sub[\"q\"].isna().any():\n",
    "        # restrict to contiguous valid block if necessary\n",
    "        valid = sub[\"q\"].notna()\n",
    "        first = int(ages[valid.argmax()])\n",
    "        last = int(ages[::-1][valid.iloc[::-1].argmax()])\n",
    "        lo = max(lo, first)\n",
    "        hi = min(hi, last + 1)\n",
    "        if hi <= lo:\n",
    "            return 0.0\n",
    "        sub = sub.loc[first:last]\n",
    "        ages = sub.index.values\n",
    "\n",
    "    q = sub[\"q\"].to_numpy()\n",
    "    offset = int(ages[0])\n",
    "\n",
    "    # Survival from min_age (m)\n",
    "    m = max(int(min_age), int(ages[0]))\n",
    "    if m >= hi:\n",
    "        return 0.0\n",
    "\n",
    "    one_minus_q = 1.0 - q\n",
    "    start = m - offset\n",
    "    end_excl = hi - offset\n",
    "\n",
    "    # Relative survival S_rel(x) = Π_{k=m}^{x-1} (1-q_k), with S_rel(m)=1\n",
    "    S_rel = np.ones(end_excl - start + 1, dtype=float)\n",
    "    if end_excl - start > 0:\n",
    "        S_rel[1:] = np.cumprod(one_minus_q[start:end_excl])\n",
    "\n",
    "    # Sum P(T in [x, x+1) | T >= m) = S_rel(x) * q_x for x = max(lo,m) .. hi-1\n",
    "    x0 = max(lo, m)\n",
    "    xs = np.arange(x0, hi, dtype=int)\n",
    "    if xs.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    idx = xs - offset\n",
    "    S_rel_x = S_rel[(xs - m)]\n",
    "    q_x = q[idx]\n",
    "    prob = float(np.sum(S_rel_x * q_x))\n",
    "\n",
    "    # Clamp for numerical safety\n",
    "    return max(0.0, min(1.0, prob))\n",
    "\n",
    "def score_life_eval(df, act_table):\n",
    "    answers = df['Answer']\n",
    "    \n",
    "    #confidence = df['Stated Confidence'].astype(float)\n",
    "    qid = df['Question ID']\n",
    "\n",
    "    # Get Radius\n",
    "    radius_list = [1, 5, 10, 20]\n",
    "    # Get the modulus of QID then use that as an index for radius_list such that 0-> 1, 1-> 5, 2-> 10, 3-> 20\n",
    "    mod_qid = df['Question ID'].astype('int').apply(lambda x: x % 4)\n",
    "\n",
    "\n",
    "    rads = qid_to_rads(qid)\n",
    "    #df['radius'] = rads\n",
    "\n",
    "    #Gold Answer:\n",
    "\n",
    "    all_data = pd.DataFrame({\n",
    "        'Question ID': qid,\n",
    "        'Answer': answers,\n",
    "        #'Confidence': confidence.astype(float),\n",
    "        'Radius': rads,\n",
    "    })\n",
    "\n",
    "    data = all_data[all_data['Answer'].notna()].copy()\n",
    "    data['Gender'] = ['female' if i >= 404 else 'male' for i in df.index]\n",
    "    data['Age'] = [get_age(qid) for qid in df['Question ID']]\n",
    "\n",
    "\n",
    "    data['Score'] = data.apply(lambda row: compute_prob(\n",
    "        point_estimate= row['Answer'],\n",
    "        min_age= row['Age'],\n",
    "        gender= row['Gender'],\n",
    "        R= row['Radius'],\n",
    "        df = act_table\n",
    "        ),\n",
    "        axis = 1\n",
    "        )\n",
    "\n",
    "\n",
    "    #data['Overconfidence'] = (data['Confidence'] - data['Score'])\n",
    "    return data[\"Score\"]\n",
    "\n",
    "def qid_to_rads(qid: pd.Series)-> pd.Series:\n",
    "    radius_list = [1, 5, 10, 20]\n",
    "    mod_qid = qid.astype('int').apply(lambda x: x % 4)\n",
    "    rads = mod_qid.apply(lambda i: radius_list[i])\n",
    "    return rads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094c1f6",
   "metadata": {},
   "source": [
    "---\n",
    "### Grading Function\n",
    "\n",
    "This function compares the data to a gold standard dataset which provides the correct answers for each question. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_df(source_df, gold_df, qset_name):\n",
    "    df = source_df.copy()\n",
    "    mask = df.index\n",
    "    \n",
    "    if qset_name in mcq_qsets:\n",
    "\n",
    "        correct_answer_letters = gold_df.loc[mask]['Correct Answer Letter']\n",
    "        df[\"Score\"] = (df[\"Answer\"].str.lower().str.strip() == correct_answer_letters.str.lower().str.strip()).astype(float)\n",
    "\n",
    "\n",
    "        # assumes stated confidences live in columns [\"A\",\"B\",\"C\",\"D\"]\n",
    "        if qset_name == \"LSAT-AR\":\n",
    "            opt_cols = [\"Stated Confidence A\",\"Stated Confidence B\",\"Stated Confidence C\",\"Stated Confidence D\", \"Stated Confidence E\"]\n",
    "        else:\n",
    "            opt_cols = [\"Stated Confidence A\",\"Stated Confidence B\",\"Stated Confidence C\",\"Stated Confidence D\"]\n",
    "\n",
    "\n",
    "        # normalize answer letters\n",
    "        ans = df[\"Answer\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "        # map letters to column indices\n",
    "        idx = ans.map({\"A\":0, \"B\":1, \"C\":2, \"D\":3}).to_numpy()\n",
    "\n",
    "        vals = df[opt_cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy()\n",
    "        mask = ~np.isnan(idx)\n",
    "\n",
    "        chosen = np.full(len(df), np.nan, dtype=float)\n",
    "        chosen[mask] = vals[mask, idx[mask].astype(int)]\n",
    "        df[\"Stated Confidence Answer (MCQ)\"] = chosen\n",
    "\n",
    "    elif qset_name == \"BoolQ\":\n",
    "        correct_answers = gold_df.loc[mask]['Correct Answer']\n",
    "        df[\"Score\"] = (df[\"Answer\"].astype(str).str.lower().str.strip() == correct_answers.astype(str).str.lower().str.strip()).astype(float)\n",
    "    elif qset_name == \"LifeEval\":\n",
    "        df[\"Question ID\"]=df[\"Question ID\"].astype(int)\n",
    "        df[\"Score\"] = score_life_eval(df, gold_df)\n",
    "        df[\"Question ID\"]=df[\"Question ID\"].astype(str)\n",
    "    elif qset_name == \"HaluEval\":\n",
    "        df[\"Score\"] = df[\"Question ID\"].str.contains(\"_r\").astype(float)\n",
    "    else:\n",
    "        df[\"Score\"] = \"UNRECOGINIZED QUESTION SET\"\n",
    "    \"\"\"\n",
    "    Don wants: one column that indicates stated confidence in the correct answer, with remaining \n",
    "    in correct answers in different columns.\n",
    "    (It would of course follow that token prob should be reorganized the same way.) \n",
    "\n",
    "    Currently we evaluate the whether the answer was correct and then the confidence it was assigned. Is this different?\n",
    "    Don's Approach: Grade the question based off of the correct answer.\n",
    "    My Approach: Grade the question based off of the chosen answer.\n",
    "    \"\"\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b18f84",
   "metadata": {},
   "source": [
    "---\n",
    "### Score DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20123456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoolQ\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n",
      "HaluEval\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n",
      "LifeEval\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n",
      "LSAT-AR\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n",
      "SAT-EN\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n",
      "SciQ\n",
      "    claude-3-7-sonnet-20250219\n",
      "    claude-3-haiku-20240307\n",
      "    claude-sonnet-4-20250514\n",
      "    deepseek-r1\n",
      "    deepseek-v3\n",
      "    gemini-2.5-flash\n",
      "    gemini-2.5-pro\n",
      "    gpt-4o\n",
      "    o3-2025-04-16\n",
      "    Meta-Llama-3.1-70B-Instruct\n",
      "    Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>...</th>\n",
       "      <th>Stated Confidence Answer (MCQ)</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "      <th>combined_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Barq's Root Beer is not a Pepsi product. It is...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Barq's Root Beer is not a...</td>\n",
       "      <td>5</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69284</th>\n",
       "      <td>82636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n\"Reasoning\": \"Predation is a biological pro...</td>\n",
       "      <td>995</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>SciQ</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>9.645942e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>995_SciQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69285</th>\n",
       "      <td>82637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n\"Reasoning\": \"Short period comets are thoug...</td>\n",
       "      <td>996</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>SciQ</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.966547e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>996_SciQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69286</th>\n",
       "      <td>82638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n\"Reasoning\": \"The question is asking about ...</td>\n",
       "      <td>997</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>SciQ</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>9.997405e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>997_SciQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69287</th>\n",
       "      <td>82639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n\"Reasoning\": \"Atoms with unstable nuclei ar...</td>\n",
       "      <td>998</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>SciQ</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>9.999572e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998_SciQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69288</th>\n",
       "      <td>82640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n\"Reasoning\": \"Atmospheric sulfur is primari...</td>\n",
       "      <td>999</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>SciQ</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>8.449691e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999_SciQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69289 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                          Reasoning Answer  \\\n",
       "0          0  The question is asking whether the production ...  False   \n",
       "1          2  This question is asking about phantom limb pai...   True   \n",
       "2          3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "3          4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "4          5  Barq's Root Beer is not a Pepsi product. It is...  False   \n",
       "...      ...                                                ...    ...   \n",
       "69284  82636                                                NaN      A   \n",
       "69285  82637                                                NaN      A   \n",
       "69286  82638                                                NaN      D   \n",
       "69287  82639                                                NaN      D   \n",
       "69288  82640                                                NaN      C   \n",
       "\n",
       "      Stated Confidence Answer  Coerce  \\\n",
       "0                          0.6    True   \n",
       "1                         0.95    True   \n",
       "2                         0.98    True   \n",
       "3                         0.98    True   \n",
       "4                         0.98    True   \n",
       "...                        ...     ...   \n",
       "69284                      NaN    True   \n",
       "69285                      NaN    True   \n",
       "69286                      NaN    True   \n",
       "69287                      NaN    True   \n",
       "69288                      NaN    True   \n",
       "\n",
       "                                                 Content Question ID  \\\n",
       "0      {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "1      {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "2      {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "3      {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "4      {\\n    \"Reasoning\": \"Barq's Root Beer is not a...           5   \n",
       "...                                                  ...         ...   \n",
       "69284  {\\n\"Reasoning\": \"Predation is a biological pro...         995   \n",
       "69285  {\\n\"Reasoning\": \"Short period comets are thoug...         996   \n",
       "69286  {\\n\"Reasoning\": \"The question is asking about ...         997   \n",
       "69287  {\\n\"Reasoning\": \"Atoms with unstable nuclei ar...         998   \n",
       "69288  {\\n\"Reasoning\": \"Atmospheric sulfur is primari...         999   \n",
       "\n",
       "                            Model Model Type Question Set  ...  \\\n",
       "0      claude-3-7-sonnet-20250219     Claude        BoolQ  ...   \n",
       "1      claude-3-7-sonnet-20250219     Claude        BoolQ  ...   \n",
       "2      claude-3-7-sonnet-20250219     Claude        BoolQ  ...   \n",
       "3      claude-3-7-sonnet-20250219     Claude        BoolQ  ...   \n",
       "4      claude-3-7-sonnet-20250219     Claude        BoolQ  ...   \n",
       "...                           ...        ...          ...  ...   \n",
       "69284  Meta-Llama-3.1-8B-Instruct      Llama         SciQ  ...   \n",
       "69285  Meta-Llama-3.1-8B-Instruct      Llama         SciQ  ...   \n",
       "69286  Meta-Llama-3.1-8B-Instruct      Llama         SciQ  ...   \n",
       "69287  Meta-Llama-3.1-8B-Instruct      Llama         SciQ  ...   \n",
       "69288  Meta-Llama-3.1-8B-Instruct      Llama         SciQ  ...   \n",
       "\n",
       "       Stated Confidence Answer (MCQ)  Token Probability True  \\\n",
       "0                                 NaN                     NaN   \n",
       "1                                 NaN                     NaN   \n",
       "2                                 NaN                     NaN   \n",
       "3                                 NaN                     NaN   \n",
       "4                                 NaN                     NaN   \n",
       "...                               ...                     ...   \n",
       "69284                            0.95                     NaN   \n",
       "69285                            0.95                     NaN   \n",
       "69286                            1.00                     NaN   \n",
       "69287                            1.00                     NaN   \n",
       "69288                            0.80                     NaN   \n",
       "\n",
       "       Token Probability False  Token Probability Answer  Token Probability A  \\\n",
       "0                          NaN                       NaN                  NaN   \n",
       "1                          NaN                       NaN                  NaN   \n",
       "2                          NaN                       NaN                  NaN   \n",
       "3                          NaN                       NaN                  NaN   \n",
       "4                          NaN                       NaN                  NaN   \n",
       "...                        ...                       ...                  ...   \n",
       "69284                      NaN                       NaN             0.999986   \n",
       "69285                      NaN                       NaN             0.999989   \n",
       "69286                      NaN                       NaN             0.000175   \n",
       "69287                      NaN                       NaN             0.000015   \n",
       "69288                      NaN                       NaN             0.000011   \n",
       "\n",
       "       Token Probability B  Token Probability C  Token Probability D  \\\n",
       "0                      NaN                  NaN                  NaN   \n",
       "1                      NaN                  NaN                  NaN   \n",
       "2                      NaN                  NaN                  NaN   \n",
       "3                      NaN                  NaN                  NaN   \n",
       "4                      NaN                  NaN                  NaN   \n",
       "...                    ...                  ...                  ...   \n",
       "69284             0.000003             0.000001         9.645942e-06   \n",
       "69285             0.000004             0.000004         2.966547e-06   \n",
       "69286             0.000042             0.000043         9.997405e-01   \n",
       "69287             0.000018             0.000010         9.999572e-01   \n",
       "69288             0.000008             0.999980         8.449691e-07   \n",
       "\n",
       "       Token Probability E  combined_name  \n",
       "0                      NaN        0_BoolQ  \n",
       "1                      NaN        2_BoolQ  \n",
       "2                      NaN        3_BoolQ  \n",
       "3                      NaN        4_BoolQ  \n",
       "4                      NaN        5_BoolQ  \n",
       "...                    ...            ...  \n",
       "69284                  NaN       995_SciQ  \n",
       "69285                  NaN       996_SciQ  \n",
       "69286                  NaN       997_SciQ  \n",
       "69287                  NaN       998_SciQ  \n",
       "69288                  NaN       999_SciQ  \n",
       "\n",
       "[69289 rows x 26 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "all_qset_names = list(combined_clean['Question Set'].unique())\n",
    "all_model_names = list(combined_clean['Model'].unique())\n",
    "\n",
    "\n",
    "gold_paths = {\n",
    "    \"BoolQ\": r\"Formatted Benchmarks\\boolq_valid_formatted.csv\",\n",
    "    \"HaluEval\": r\"Formatted Benchmarks\\halu_eval_qa_formatted.csv\",\n",
    "    \"LifeEval\":    r\"Formatted Benchmarks\\PeriodLifeTable_2022_RawData.csv\",\n",
    "    \"LSAT-AR\": r\"Formatted Benchmarks\\lsat_ar_test_formatted.csv\", \n",
    "    \"SAT-EN\": r\"Formatted Benchmarks\\sat_en_formatted.csv\",\n",
    "    \"SciQ\": r\"Formatted Benchmarks\\sciq_test_formatted.csv\"    \n",
    "}\n",
    "\n",
    "\n",
    "combined_graded = pd.DataFrame()\n",
    "\n",
    "\n",
    "for qset_name in all_qset_names:\n",
    "    print(qset_name)\n",
    "    for model_name in all_model_names:\n",
    "        print(f\"    {model_name}\")\n",
    "        only_qset = combined_clean[(combined_clean['Question Set'] == qset_name) & (combined_clean['Model'] == model_name)].reset_index()\n",
    "\n",
    "        gold_df_path = gold_paths[qset_name]\n",
    "        gold_df = pd.read_csv(gold_df_path)\n",
    "    \n",
    "        scored_qset = grade_df(only_qset, gold_df = gold_df, qset_name= qset_name)\n",
    "\n",
    "\n",
    "        #display(scored_qset[\"Score\"])\n",
    "        combined_graded = pd.concat([combined_graded, scored_qset], ignore_index=True)\n",
    "\n",
    "\n",
    "combined_graded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf90169d",
   "metadata": {},
   "source": [
    "# Summary Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd746d8",
   "metadata": {},
   "source": [
    "Here we see the Raw versus Filtered counts for all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9efcb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw</th>\n",
       "      <th>Filtered</th>\n",
       "      <th>Prop. Kept</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Question Set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>3270.000000</td>\n",
       "      <td>2503.0</td>\n",
       "      <td>0.765443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaluEval</th>\n",
       "      <td>1998.909091</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>0.895488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSAT-AR</th>\n",
       "      <td>229.909091</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.374061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LifeEval</th>\n",
       "      <td>808.000000</td>\n",
       "      <td>752.0</td>\n",
       "      <td>0.930693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT-EN</th>\n",
       "      <td>206.000000</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.839806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SciQ</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>995.0</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Raw  Filtered  Prop. Kept\n",
       "Question Set                                   \n",
       "BoolQ         3270.000000    2503.0    0.765443\n",
       "HaluEval      1998.909091    1790.0    0.895488\n",
       "LSAT-AR        229.909091      86.0    0.374061\n",
       "LifeEval       808.000000     752.0    0.930693\n",
       "SAT-EN         206.000000     173.0    0.839806\n",
       "SciQ          1000.000000     995.0    0.995000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.DataFrame({\n",
    "    \"Raw\": combined_df[\"Question Set\"].value_counts() / 11, # This is slightly below 2000 for Llama because we had to drop 3 duplicates\n",
    "    \"Filtered\": combined_clean[\"Question Set\"].value_counts() / 11\n",
    "})\n",
    "\n",
    "counts[~counts.index.isin(mcq_qsets)]\n",
    "\n",
    "counts[\"Prop. Kept\"] = counts[\"Filtered\"] / counts[\"Raw\"]\n",
    "counts\n",
    "# Need to look at: LSAT-AR (-1)???,  Sciq(-1) Im getting rid of too much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ecd9798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question Set</th>\n",
       "      <th>BoolQ</th>\n",
       "      <th>HaluEval</th>\n",
       "      <th>LSAT-AR</th>\n",
       "      <th>LifeEval</th>\n",
       "      <th>SAT-EN</th>\n",
       "      <th>SciQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>3241</td>\n",
       "      <td>1960</td>\n",
       "      <td>208</td>\n",
       "      <td>807</td>\n",
       "      <td>202</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>3200</td>\n",
       "      <td>1997</td>\n",
       "      <td>190</td>\n",
       "      <td>800</td>\n",
       "      <td>202</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-7-sonnet-20250219</th>\n",
       "      <td>3267</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-haiku-20240307</th>\n",
       "      <td>3036</td>\n",
       "      <td>1855</td>\n",
       "      <td>230</td>\n",
       "      <td>808</td>\n",
       "      <td>181</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-sonnet-4-20250514</th>\n",
       "      <td>3258</td>\n",
       "      <td>2000</td>\n",
       "      <td>188</td>\n",
       "      <td>808</td>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-r1</th>\n",
       "      <td>3214</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-v3</th>\n",
       "      <td>2898</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>204</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash</th>\n",
       "      <td>3261</td>\n",
       "      <td>2000</td>\n",
       "      <td>177</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-pro</th>\n",
       "      <td>3190</td>\n",
       "      <td>1984</td>\n",
       "      <td>188</td>\n",
       "      <td>808</td>\n",
       "      <td>204</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>3247</td>\n",
       "      <td>2000</td>\n",
       "      <td>230</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o3-2025-04-16</th>\n",
       "      <td>3070</td>\n",
       "      <td>1991</td>\n",
       "      <td>193</td>\n",
       "      <td>761</td>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Question Set                 BoolQ  HaluEval  LSAT-AR  LifeEval  SAT-EN  SciQ\n",
       "Model                                                                        \n",
       "Meta-Llama-3.1-70B-Instruct   3241      1960      208       807     202  1000\n",
       "Meta-Llama-3.1-8B-Instruct    3200      1997      190       800     202   997\n",
       "claude-3-7-sonnet-20250219    3267      2000      228       808     206   999\n",
       "claude-3-haiku-20240307       3036      1855      230       808     181   999\n",
       "claude-sonnet-4-20250514      3258      2000      188       808     205  1000\n",
       "deepseek-r1                   3214      2000      228       808     206  1000\n",
       "deepseek-v3                   2898      2000      228       808     204  1000\n",
       "gemini-2.5-flash              3261      2000      177       808     206  1000\n",
       "gemini-2.5-pro                3190      1984      188       808     204  1000\n",
       "gpt-4o                        3247      2000      230       808     206  1000\n",
       "o3-2025-04-16                 3070      1991      193       761     205  1000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df[\"Coerce\"] == True].pivot_table(index=\"Model\", columns=\"Question Set\",\n",
    "                        aggfunc=\"size\", fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675f8d2",
   "metadata": {},
   "source": [
    "---\n",
    "We had some notable differences from Jacob's results in **LSAT-AR** and **SciQ**. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e58eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question ID</th>\n",
       "      <th>false_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question ID  false_count\n",
       "0          13            1\n",
       "1         295            1\n",
       "2          40            1\n",
       "3         699            1\n",
       "4         705            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_filtered = combined_clean[combined_clean[\"Question Set\"] == \"SciQ\"]\n",
    "sciq_raw = combined_df[combined_df[\"Question Set\"] == \"SciQ\"]\n",
    "\n",
    "\n",
    "sciq_filtered_ids = sciq_filtered.index\n",
    "\n",
    "deleted = sciq_raw[~sciq_raw.index.isin(sciq_filtered_ids)]\n",
    "deleted[\"Question ID\"].unique()\n",
    "\n",
    "# Deleted QID in SciQ are ['13', '40', '295', '699', '705'] after visual inspection they all seem to have False coerce values in one of 11 rows\n",
    "\n",
    "\n",
    "false_counts = (deleted['Coerce'].eq(False)\n",
    "                .groupby(deleted['Question ID'])\n",
    "                .sum()\n",
    "                .rename('false_count'))\n",
    "\n",
    "# as a DataFrame\n",
    "false_counts = false_counts.reset_index()\n",
    "false_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff4a25",
   "metadata": {},
   "source": [
    "Jacob's cleaned version of SciQ has 996 QIDs while mine has 995. When investigating each deleted Question ID in my version we find that they all have 1 instance where the answer could not be coerced. Does Jacob have an extra question that shouldn't be there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa9a0c",
   "metadata": {},
   "source": [
    "---\n",
    "Looking at LSAT-AR we see that Jacob had 87 QIDs while mine had 86. Let's investigate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7316c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Question IDs: ['64']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Stated Confidence A</th>\n",
       "      <th>Stated Confidence B</th>\n",
       "      <th>Stated Confidence C</th>\n",
       "      <th>Stated Confidence D</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>64</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>64</td>\n",
       "      <td>claude-3-haiku-20240307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21170</th>\n",
       "      <td>64</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28693</th>\n",
       "      <td>64</td>\n",
       "      <td>deepseek-r1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36198</th>\n",
       "      <td>64</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43712</th>\n",
       "      <td>64</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51226</th>\n",
       "      <td>64</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58740</th>\n",
       "      <td>64</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73758</th>\n",
       "      <td>64</td>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81269</th>\n",
       "      <td>64</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.900</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Question ID                        Model  Stated Confidence A  \\\n",
       "6142           64   claude-3-7-sonnet-20250219                0.050   \n",
       "13656          64      claude-3-haiku-20240307                0.000   \n",
       "21170          64     claude-sonnet-4-20250514                0.100   \n",
       "28693          64                  deepseek-r1                0.000   \n",
       "36198          64                  deepseek-v3                0.000   \n",
       "43712          64             gemini-2.5-flash                0.050   \n",
       "51226          64               gemini-2.5-pro                0.005   \n",
       "58740          64                       gpt-4o                0.100   \n",
       "73758          64  Meta-Llama-3.1-70B-Instruct                0.000   \n",
       "81269          64   Meta-Llama-3.1-8B-Instruct                0.800   \n",
       "\n",
       "       Stated Confidence B  Stated Confidence C  Stated Confidence D  \\\n",
       "6142                 0.050                0.050                 0.05   \n",
       "13656                0.000                0.500                 0.50   \n",
       "21170                0.100                0.100                 0.10   \n",
       "28693                0.000                0.000                 1.00   \n",
       "36198                1.000                0.000                 0.00   \n",
       "43712                0.050                0.050                 0.80   \n",
       "51226                0.005                0.005                 0.98   \n",
       "58740                0.700                0.050                 0.10   \n",
       "73758                0.000                1.000                 0.00   \n",
       "81269                0.900                0.800                 0.90   \n",
       "\n",
       "       Stated Confidence E  Sum  \n",
       "6142                 0.800  1.0  \n",
       "13656                0.000  1.0  \n",
       "21170                0.600  1.0  \n",
       "28693                0.000  1.0  \n",
       "36198                0.000  1.0  \n",
       "43712                0.050  1.0  \n",
       "51226                0.005  1.0  \n",
       "58740                0.050  1.0  \n",
       "73758                0.000  1.0  \n",
       "81269                0.900  4.3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lsat_filtered = combined_clean[combined_clean[\"Question Set\"] == \"LSAT-AR\"]\n",
    "lsat_raw = combined_df[combined_df[\"Question Set\"] == \"LSAT-AR\"]\n",
    "\n",
    "\n",
    "lsat_filtered_ids = lsat_filtered.index\n",
    "\n",
    "deleted = lsat_raw[~lsat_raw.index.isin(lsat_filtered_ids)]\n",
    "deleted[\"Question ID\"].unique()\n",
    "\n",
    "\n",
    "\n",
    "false_counts = (deleted['Coerce'].eq(False)\n",
    "                .groupby(deleted['Question ID'])\n",
    "                .sum()\n",
    "                .rename('false_count'))\n",
    "\n",
    "# as a DataFrame\n",
    "false_counts = false_counts.reset_index()\n",
    "\n",
    "# There is one QID that was removed while having all coerce values as True. Lets investigate:\n",
    "special_qid = list(false_counts[false_counts['false_count'] == 0]['Question ID'])\n",
    "print(f\"Special Question IDs: {special_qid}\")\n",
    "\n",
    "# At some point QID 64\n",
    "bad_qid_confidence = lsat_raw[lsat_raw['Question ID'].isin(special_qid)][[\"Question ID\", \"Model\", \"Stated Confidence A\", \"Stated Confidence B\", \"Stated Confidence C\",\"Stated Confidence D\",\"Stated Confidence E\",]]#.sum(axis =1)\n",
    "\n",
    "bad_qid_confidence['Sum'] = bad_qid_confidence.sum(axis=1, numeric_only = True)\n",
    "bad_qid_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32dcfde",
   "metadata": {},
   "source": [
    "Question ID 64 in LSAT-AR only had 10 answered instances. Notably, GPT-o3 failed to respond. Because of this, we dropped it from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Noam Michael\\AppData\\Local\\Temp\\ipykernel_25892\\717121390.py:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  jb_df = pd.read_csv(\"Combined Results\\llm-confidence-correct.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm</th>\n",
       "      <th>qset</th>\n",
       "      <th>question_id</th>\n",
       "      <th>stated_confidence</th>\n",
       "      <th>chosen_token_confidence</th>\n",
       "      <th>max_token_confidence</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>boolq_valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.997624</td>\n",
       "      <td>0.997624</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>boolq_valid</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.995222</td>\n",
       "      <td>0.995222</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>boolq_valid</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>boolq_valid</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>boolq_valid</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.882367</td>\n",
       "      <td>0.882367</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72180</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>halu_eval_qa</td>\n",
       "      <td>9975_r</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72181</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>halu_eval_qa</td>\n",
       "      <td>9989_h</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72182</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>halu_eval_qa</td>\n",
       "      <td>9989_r</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72183</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>halu_eval_qa</td>\n",
       "      <td>9993_h</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72184</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>halu_eval_qa</td>\n",
       "      <td>9993_r</td>\n",
       "      <td>0.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72185 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               llm          qset question_id  \\\n",
       "0      Meta-Llama-3.1-70B-Instruct   boolq_valid           0   \n",
       "1      Meta-Llama-3.1-70B-Instruct   boolq_valid           1   \n",
       "2      Meta-Llama-3.1-70B-Instruct   boolq_valid           2   \n",
       "3      Meta-Llama-3.1-70B-Instruct   boolq_valid           3   \n",
       "4      Meta-Llama-3.1-70B-Instruct   boolq_valid           4   \n",
       "...                            ...           ...         ...   \n",
       "72180                o3-2025-04-16  halu_eval_qa      9975_r   \n",
       "72181                o3-2025-04-16  halu_eval_qa      9989_h   \n",
       "72182                o3-2025-04-16  halu_eval_qa      9989_r   \n",
       "72183                o3-2025-04-16  halu_eval_qa      9993_h   \n",
       "72184                o3-2025-04-16  halu_eval_qa      9993_r   \n",
       "\n",
       "       stated_confidence  chosen_token_confidence  max_token_confidence  \\\n",
       "0                   0.80                 0.997624              0.997624   \n",
       "1                   0.90                 0.995222              0.995222   \n",
       "2                   0.90                 0.999983              0.999983   \n",
       "3                   0.90                 0.999939              0.999939   \n",
       "4                   0.90                 0.882367              0.882367   \n",
       "...                  ...                      ...                   ...   \n",
       "72180               0.80                      NaN                   NaN   \n",
       "72181               0.05                      NaN                   NaN   \n",
       "72182               0.90                      NaN                   NaN   \n",
       "72183               0.05                      NaN                   NaN   \n",
       "72184               0.93                      NaN                   NaN   \n",
       "\n",
       "       correct  \n",
       "0        False  \n",
       "1         True  \n",
       "2         True  \n",
       "3        False  \n",
       "4         True  \n",
       "...        ...  \n",
       "72180     True  \n",
       "72181    False  \n",
       "72182     True  \n",
       "72183    False  \n",
       "72184     True  \n",
       "\n",
       "[72185 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jacob's file:\n",
    "\n",
    "jb_df = pd.read_csv(\"Combined Results\\llm-confidence-correct.csv\")\n",
    "jb_df.shape, combined_clean.shape\n",
    "\n",
    "## SciQ\n",
    "jb_qid = jb_df[jb_df[\"qset\"] == \"sciq_test\"][\"question_id\"].unique()\n",
    "my_qid = combined_clean[combined_clean[\"Question Set\"] == \"SciQ\"][\"Question ID\"].unique()\n",
    "\n",
    "# elements not in the intersection (i.e., in exactly one array)\n",
    "not_in_intersection = np.setxor1d(jb_qid, my_qid)\n",
    "\n",
    "# if you also want side-specific lists:\n",
    "only_in_jb = np.setdiff1d(jb_qid, my_qid)\n",
    "only_in_me = np.setdiff1d(my_qid, jb_qid)\n",
    "\n",
    "only_in_jb\n",
    "\n",
    "\n",
    "## LSAT-AR\n",
    "jb_qid = jb_df[jb_df[\"qset\"] == \"lsat_ar_test\"][\"question_id\"].unique()\n",
    "my_qid = combined_clean[combined_clean[\"Question Set\"] == \"LSAT-AR\"][\"Question ID\"].unique()\n",
    "\n",
    "# elements not in the intersection (i.e., in exactly one array)\n",
    "not_in_intersection = np.setxor1d(jb_qid, my_qid)\n",
    "\n",
    "# if you also want side-specific lists:\n",
    "only_in_jb = np.setdiff1d(jb_qid, my_qid)\n",
    "only_in_me = np.setdiff1d(my_qid, jb_qid)\n",
    "\n",
    "only_in_jb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d86feb",
   "metadata": {},
   "source": [
    "## Save all DataFrames to CSVs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8469f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Shapes:\n",
      "------------------------------------------------------------\n",
      "Raw: (82641, 22)\n",
      "Cleaned: (69289, 25)\n",
      "Graded: (69289, 26)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_path   = Path(\"Combined Results/combined_raw.csv\")\n",
    "clean_path = Path(\"Combined Results/combined_clean.csv\")\n",
    "graded_path = Path(\"Combined Results/combined_graded.csv\")\n",
    "\n",
    "\n",
    "# write\n",
    "combined_df.to_csv(raw_path, index=False, encoding=\"utf-8\")\n",
    "combined_clean.to_csv(clean_path, index=False, encoding=\"utf-8\")\n",
    "combined_graded.to_csv(graded_path, index = False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "print(f\"\"\"CSV Shapes:\\n{'-'*60}\n",
    "Raw: {combined_df.shape}\n",
    "Cleaned: {combined_clean.shape}\n",
    "Graded: {combined_graded.shape}\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412eeb9",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f802a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
