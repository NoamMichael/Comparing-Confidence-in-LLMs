{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2436248",
   "metadata": {},
   "source": [
    "# Comparing Calibration Data Across All Models / Question Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9704e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f7b78",
   "metadata": {},
   "source": [
    "## Combine All Results Into Composite CSV\n",
    "\n",
    "First we will read from the Parsed Results folder and make a dictionary that abstracts the structure of the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "009288cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Claude', 'Deepseek', 'Gemini', 'GPT', 'Llama'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the folder and create a dictionary to model the structure of the files\n",
    "\n",
    "def folder_tree_dict(root, *, include_files=True, follow_symlinks=False, ignore_hidden=True):\n",
    "    root = Path(root)\n",
    "\n",
    "    def build(p: Path):\n",
    "        out = {}\n",
    "        for entry in sorted(p.iterdir(), key=lambda x: (x.is_file(), x.name.lower())):\n",
    "            if ignore_hidden and entry.name.startswith(\".\"):\n",
    "                continue\n",
    "            try:\n",
    "                if entry.is_dir() and (follow_symlinks or not entry.is_symlink()):\n",
    "                    out[entry.name] = build(entry)\n",
    "                else:\n",
    "                    if include_files:\n",
    "                        out[entry.name] = None  # or {\"size\": entry.stat().st_size}\n",
    "            except PermissionError:\n",
    "                out[entry.name] = \"<permission-denied>\"\n",
    "        return out\n",
    "\n",
    "    return {root.name: build(root)}\n",
    "\n",
    "\n",
    "folder_path = r\"Parsed Results\"\n",
    "\n",
    "folder_abstraction_dict = folder_tree_dict(folder_path)[folder_path]\n",
    "\n",
    "folder_abstraction_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f850c",
   "metadata": {},
   "source": [
    "### Combined CSV\n",
    "We want to make a well-formed CSV for future analysis. This CSV will have the following fields for columns:\n",
    "\n",
    "```text\n",
    "Question Set (str) ---------------- Required: The display name of the question set\n",
    "Question ID (str) ----------------- Required: The Question ID\n",
    "Model (str) ----------------------- Required: The model that provided the response (e.g. Llama-3.1-8B-Instruct)\n",
    "Model Type (str) ------------------ Required: The family of models which  the model (e.g. Llama)\n",
    "Coerce (Bool) --------------------- Required: Whether the parser was able to understand the response\n",
    "\n",
    "Question (str) -------------------- Required: The question posed to the model\n",
    "Correct Answer (str) -------------- Optional: Depends on Question Set (LifeEval is different than others)\n",
    "Content (str) --------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Reasoning (str) ------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Answer (str) ---------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "Score (float) --------------------- Optional: Depends on Coerce value (NA if Coerce == False)\n",
    "\n",
    "Stated Confidence Answer (float) -- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence A (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence B (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence C (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence D (float) ------- Optional: Depends on Question Set (NA if not available)\n",
    "Stated Confidence E (float)-------- Optional: Depends on Question Set (NA if not available)\n",
    "\n",
    "Token Probability Answer (float) -- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability A (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability B (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability C (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability D (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "Token Probability E (float) ------- Optional: Depends on Model Type (NA if not available)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "1ac22891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Results/Claude\n",
      "    Parsed Results/Claude/claude-3-7-sonnet-20250219\n",
      "        boolq_valid:    Parsed Results/Claude/claude-3-7-sonnet-20250219/boolq_valid_claude-3-7-sonnet-20250219.csv\n",
      "        halu_eval_qa:    Parsed Results/Claude/claude-3-7-sonnet-20250219/halu_eval_qa_claude-3-7-sonnet-20250219.csv\n",
      "        life_eval:    Parsed Results/Claude/claude-3-7-sonnet-20250219/life_eval_claude-3-7-sonnet-20250219.csv\n",
      "        lsat_ar_test:    Parsed Results/Claude/claude-3-7-sonnet-20250219/lsat_ar_test_claude-3-7-sonnet-20250219.csv\n",
      "        sat_en:    Parsed Results/Claude/claude-3-7-sonnet-20250219/sat_en_claude-3-7-sonnet-20250219.csv\n",
      "        sciq_test:    Parsed Results/Claude/claude-3-7-sonnet-20250219/sciq_test_claude-3-7-sonnet-20250219.csv\n",
      "    Parsed Results/Claude/claude-3-haiku-20240307\n",
      "        boolq_valid:    Parsed Results/Claude/claude-3-haiku-20240307/boolq_valid_claude-3-haiku-20240307.csv\n",
      "        halu_eval_qa:    Parsed Results/Claude/claude-3-haiku-20240307/halu_eval_qa_claude-3-haiku-20240307.csv\n",
      "        life_eval:    Parsed Results/Claude/claude-3-haiku-20240307/life_eval_claude-3-haiku-20240307.csv\n",
      "        lsat_ar_test:    Parsed Results/Claude/claude-3-haiku-20240307/lsat_ar_test_claude-3-haiku-20240307.csv\n",
      "        sat_en:    Parsed Results/Claude/claude-3-haiku-20240307/sat_en_claude-3-haiku-20240307.csv\n",
      "        sciq_test:    Parsed Results/Claude/claude-3-haiku-20240307/sciq_test_claude-3-haiku-20240307.csv\n",
      "    Parsed Results/Claude/claude-sonnet-4-20250514\n",
      "        boolq_valid:    Parsed Results/Claude/claude-sonnet-4-20250514/boolq_valid_claude-sonnet-4-20250514.csv\n",
      "        halu_eval_qa:    Parsed Results/Claude/claude-sonnet-4-20250514/halu_eval_qa_claude-sonnet-4-20250514.csv\n",
      "        life_eval:    Parsed Results/Claude/claude-sonnet-4-20250514/life_eval_claude-sonnet-4-20250514.csv\n",
      "        lsat_ar_test:    Parsed Results/Claude/claude-sonnet-4-20250514/lsat_ar_test_claude-sonnet-4-20250514.csv\n",
      "        sat_en:    Parsed Results/Claude/claude-sonnet-4-20250514/sat_en_claude-sonnet-4-20250514.csv\n",
      "        sciq_test:    Parsed Results/Claude/claude-sonnet-4-20250514/sciq_test_claude-sonnet-4-20250514.csv\n",
      "Parsed Results/Deepseek\n",
      "    Parsed Results/Deepseek/deepseek-r1\n",
      "        boolq_valid:    Parsed Results/Deepseek/deepseek-r1/boolq_valid_deepseek-r1.csv\n",
      "        halu_eval_qa:    Parsed Results/Deepseek/deepseek-r1/halu_eval_qa_deepseek-r1.csv\n",
      "        life_eval:    Parsed Results/Deepseek/deepseek-r1/life_eval_deepseek-r1.csv\n",
      "        lsat_ar_test:    Parsed Results/Deepseek/deepseek-r1/lsat_ar_test_deepseek-r1.csv\n",
      "        sat_en:    Parsed Results/Deepseek/deepseek-r1/sat_en_deepseek-r1.csv\n",
      "        sciq_test:    Parsed Results/Deepseek/deepseek-r1/sciq_test_deepseek-r1.csv\n",
      "    Parsed Results/Deepseek/deepseek-v3\n",
      "        boolq_valid:    Parsed Results/Deepseek/deepseek-v3/boolq_valid_deepseek-v3.csv\n",
      "        halu_eval_qa:    Parsed Results/Deepseek/deepseek-v3/halu_eval_qa_deepseek-v3.csv\n",
      "        life_eval:    Parsed Results/Deepseek/deepseek-v3/life_eval_deepseek-v3.csv\n",
      "        lsat_ar_test:    Parsed Results/Deepseek/deepseek-v3/lsat_ar_test_deepseek-v3.csv\n",
      "        sat_en:    Parsed Results/Deepseek/deepseek-v3/sat_en_deepseek-v3.csv\n",
      "        sciq_test:    Parsed Results/Deepseek/deepseek-v3/sciq_test_deepseek-v3.csv\n",
      "Parsed Results/Gemini\n",
      "    Parsed Results/Gemini/gemini-2.5-flash\n",
      "        boolq_valid:    Parsed Results/Gemini/gemini-2.5-flash/boolq_valid_gemini-2.5-flash.csv\n",
      "        halu_eval_qa:    Parsed Results/Gemini/gemini-2.5-flash/halu_eval_qa_gemini-2.5-flash.csv\n",
      "        life_eval:    Parsed Results/Gemini/gemini-2.5-flash/life_eval_gemini-2.5-flash.csv\n",
      "        lsat_ar_test:    Parsed Results/Gemini/gemini-2.5-flash/lsat_ar_test_gemini-2.5-flash.csv\n",
      "        sat_en:    Parsed Results/Gemini/gemini-2.5-flash/sat_en_gemini-2.5-flash.csv\n",
      "        sciq_test:    Parsed Results/Gemini/gemini-2.5-flash/sciq_test_gemini-2.5-flash.csv\n",
      "    Parsed Results/Gemini/gemini-2.5-pro\n",
      "        boolq_valid:    Parsed Results/Gemini/gemini-2.5-pro/boolq_valid_gemini-2.5-pro.csv\n",
      "        halu_eval_qa:    Parsed Results/Gemini/gemini-2.5-pro/halu_eval_qa_gemini-2.5-pro.csv\n",
      "        life_eval:    Parsed Results/Gemini/gemini-2.5-pro/life_eval_gemini-2.5-pro.csv\n",
      "        lsat_ar_test:    Parsed Results/Gemini/gemini-2.5-pro/lsat_ar_test_gemini-2.5-pro.csv\n",
      "        sat_en:    Parsed Results/Gemini/gemini-2.5-pro/sat_en_gemini-2.5-pro.csv\n",
      "        sciq_test:    Parsed Results/Gemini/gemini-2.5-pro/sciq_test_gemini-2.5-pro.csv\n",
      "Parsed Results/GPT\n",
      "    Parsed Results/GPT/gpt-4o\n",
      "        boolq_valid:    Parsed Results/GPT/gpt-4o/boolq_valid_gpt-4o.csv\n",
      "        halu_eval_qa:    Parsed Results/GPT/gpt-4o/halu_eval_qa_gpt-4o.csv\n",
      "        life_eval:    Parsed Results/GPT/gpt-4o/life_eval_gpt-4o.csv\n",
      "        lsat_ar_test:    Parsed Results/GPT/gpt-4o/lsat_ar_test_gpt-4o.csv\n",
      "        sat_en:    Parsed Results/GPT/gpt-4o/sat_en_gpt-4o.csv\n",
      "        sciq_test:    Parsed Results/GPT/gpt-4o/sciq_test_gpt-4o.csv\n",
      "    Parsed Results/GPT/o3-2025-04-16\n",
      "        boolq_valid:    Parsed Results/GPT/o3-2025-04-16/boolq_valid_o3-2025-04-16.csv\n",
      "        halu_eval_qa:    Parsed Results/GPT/o3-2025-04-16/halu_eval_qa_o3-2025-04-16.csv\n",
      "        life_eval:    Parsed Results/GPT/o3-2025-04-16/life_eval_o3-2025-04-16.csv\n",
      "        lsat_ar_test:    Parsed Results/GPT/o3-2025-04-16/lsat_ar_test_o3-2025-04-16.csv\n",
      "        sat_en:    Parsed Results/GPT/o3-2025-04-16/sat_en_o3-2025-04-16.csv\n",
      "        sciq_test:    Parsed Results/GPT/o3-2025-04-16/sciq_test_o3-2025-04-16.csv\n",
      "Parsed Results/Llama\n",
      "    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct\n",
      "        boolq_valid:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/boolq_valid_Meta-Llama-3.1-70B-Instruct.csv\n",
      "        halu_eval_qa:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/halu_eval_qa_Meta-Llama-3.1-70B-Instruct.csv\n",
      "        life_eval:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/life_eval_Meta-Llama-3.1-70B-Instruct.csv\n",
      "        lsat_ar_test:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/lsat_ar_test_Meta-Llama-3.1-70B-Instruct.csv\n",
      "        sat_en:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/sat_en_Meta-Llama-3.1-70B-Instruct.csv\n",
      "        sciq_test:    Parsed Results/Llama/Meta-Llama-3.1-70B-Instruct/sciq_test_Meta-Llama-3.1-70B-Instruct.csv\n",
      "    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct\n",
      "        boolq_valid:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/boolq_valid_Meta-Llama-3.1-8B-Instruct.csv\n",
      "        halu_eval_qa:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/halu_eval_qa_Meta-Llama-3.1-8B-Instruct.csv\n",
      "        life_eval:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/life_eval_Meta-Llama-3.1-8B-Instruct.csv\n",
      "        lsat_ar_test:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/lsat_ar_test_Meta-Llama-3.1-8B-Instruct.csv\n",
      "        sat_en:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/sat_en_Meta-Llama-3.1-8B-Instruct.csv\n",
      "        sciq_test:    Parsed Results/Llama/Meta-Llama-3.1-8B-Instruct/sciq_test_Meta-Llama-3.1-8B-Instruct.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Stated Confidence A</th>\n",
       "      <th>Stated Confidence B</th>\n",
       "      <th>Stated Confidence C</th>\n",
       "      <th>Stated Confidence D</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House tax and property tax are often used inte...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"House tax and property ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reasoning Answer  \\\n",
       "0  The question is asking whether the production ...  False   \n",
       "1  House tax and property tax are often used inte...   True   \n",
       "2  This question is asking about phantom limb pai...   True   \n",
       "3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "\n",
       "  Stated Confidence Answer  Coerce  \\\n",
       "0                      0.6    True   \n",
       "1                     0.85    True   \n",
       "2                     0.95    True   \n",
       "3                     0.98    True   \n",
       "4                     0.98    True   \n",
       "\n",
       "                                             Content Question ID  \\\n",
       "0  {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "1  {\\n    \"Reasoning\": \"House tax and property ta...           1   \n",
       "2  {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "3  {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "4  {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "\n",
       "                        Model Model Type Question Set  Stated Confidence A  \\\n",
       "0  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "1  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "2  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "3  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "4  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "\n",
       "   Stated Confidence B  Stated Confidence C  Stated Confidence D  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   Stated Confidence E  Token Probability True  Token Probability False  \\\n",
       "0                  NaN                     NaN                      NaN   \n",
       "1                  NaN                     NaN                      NaN   \n",
       "2                  NaN                     NaN                      NaN   \n",
       "3                  NaN                     NaN                      NaN   \n",
       "4                  NaN                     NaN                      NaN   \n",
       "\n",
       "   Token Probability Answer  Token Probability A  Token Probability B  \\\n",
       "0                       NaN                  NaN                  NaN   \n",
       "1                       NaN                  NaN                  NaN   \n",
       "2                       NaN                  NaN                  NaN   \n",
       "3                       NaN                  NaN                  NaN   \n",
       "4                       NaN                  NaN                  NaN   \n",
       "\n",
       "   Token Probability C  Token Probability D  Token Probability E  \n",
       "0                  NaN                  NaN                  NaN  \n",
       "1                  NaN                  NaN                  NaN  \n",
       "2                  NaN                  NaN                  NaN  \n",
       "3                  NaN                  NaN                  NaN  \n",
       "4                  NaN                  NaN                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "clean_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "mcq_qsets = ['LSAT-AR', 'SAT-EN', 'SciQ']\n",
    "\n",
    "\n",
    "\n",
    "for model_type, models in folder_abstraction_dict.items():\n",
    "    model_type_path = folder_path + f\"/{model_type}\"\n",
    "    print(model_type_path)\n",
    "    for model_name, qsets in models.items():\n",
    "        model_path = model_type_path + f\"/{model_name}\"\n",
    "        print(f\"    {model_path}\")\n",
    "        for qset_file_name in qsets:\n",
    "            splitter = f\"_{model_name}\"\n",
    "            qset_name = qset_file_name.split(splitter)[0]\n",
    "            qset_path = model_path + f\"/{qset_file_name}\"\n",
    "            print(f\"        {qset_name}:    {qset_path}\")\n",
    "            #--------- Write a function to spit out a dataframe w/ model_name, qset_name, true_answer and concat it\n",
    "\n",
    "            source_df = pd.read_csv(qset_path)\n",
    "            source_df[\"Model\"] = model_name\n",
    "            source_df[\"Model Type\"] = model_type\n",
    "            source_df[\"Question Set\"] = qset_name\n",
    "            source_df[\"Question ID\"] = source_df[\"Question ID\"].astype(str)\n",
    "            combined_df = pd.concat([combined_df, source_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df.drop([\"Unnamed: 0\", \"Question ID.1\"], axis = 1, inplace = True)\n",
    "\n",
    "## Still need to add correct answer and score\n",
    "\n",
    "col_rename_map ={\n",
    "# Metadata\n",
    "'Question Set': \"Question Set\",\n",
    "'Question ID': \"Question ID\",\n",
    "'Model': \"Model\",\n",
    "'Model Type': \"Model Type\",\n",
    "'coerce': \"Coerce\",\n",
    "\n",
    "# Model Response\n",
    "\n",
    "'content': \"Content\",\n",
    "'Reasoning': \"Reasoning\",\n",
    "'Answer': \"Answer\",\n",
    "\n",
    "# Stated Confidence\n",
    "'Confidence': \"Stated Confidence Answer\",\n",
    "\"A\": \"Stated Confidence A\",\n",
    "\"B\": \"Stated Confidence B\",\n",
    "'C': \"Stated Confidence C\",\n",
    "'D': \"Stated Confidence D\",\n",
    "'E': \"Stated Confidence E\",\n",
    "\n",
    "# Token Probability\n",
    "'True_prob': \"Token Probability True\",\n",
    "'False_prob': \"Token Probability False\",\n",
    "'Answer_prob': \"Token Probability Answer\",\n",
    "'A_prob': \"Token Probability A\",\n",
    "'B_prob': \"Token Probability B\",\n",
    "'C_prob': \"Token Probability C\",\n",
    "'D_prob': \"Token Probability D\",\n",
    "'E_prob': \"Token Probability E\"\n",
    "}\n",
    "\n",
    "combined_df = combined_df.rename(columns = col_rename_map)\n",
    "\n",
    "qset_rename = {\n",
    " 'boolq_valid': \"BoolQ\",\n",
    " 'halu_eval_qa': \"HaluEval\",\n",
    " 'life_eval': \"LifeEval\",\n",
    " 'lsat_ar_test': \"LSAT-AR\",\n",
    " 'sat_en': \"SAT-EN\",\n",
    " 'sciq_test':\"SciQ\"\n",
    "}\n",
    "\n",
    "combined_df[\"Question Set\"] = combined_df[\"Question Set\"].map(qset_rename)\n",
    "\n",
    "with pd.option_context('display.max_columns', None,\n",
    "                       #'display.width', None,\n",
    "                       #'display.max_colwidth', None\n",
    "                       ):\n",
    "    display(combined_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "666f9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_clean = combined_df.copy()\n",
    "\n",
    "# this gets the qid and qset where coerce is false\n",
    "bad_qid_qset = combined_df[combined_df[\"Coerce\"] == False][[\"Question ID\", \"Question Set\"]]\n",
    "\n",
    "#bad_qid_qset = bad_qid_qset[[\"Question ID\", \"Question Set\"]]\n",
    "\n",
    "bad = set(bad_qid_qset[\"Question ID\"] +  \"_\" + bad_qid_qset[\"Question Set\"])\n",
    "\n",
    "\n",
    "combined_clean['combined_name'] = combined_clean['Question ID'] + \"_\" + combined_clean[\"Question Set\"]\n",
    "\n",
    "\n",
    "\n",
    "mask = ~combined_clean[\"combined_name\"].isin(bad)   # True = keep\n",
    "\n",
    "combined_clean = combined_clean[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d71da9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Stated Confidence A</th>\n",
       "      <th>...</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "      <th>combined_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The question is asking whether the production ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"The question is asking wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question is asking about phantom limb pai...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"This question is asking a...</td>\n",
       "      <td>2</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Harry Potter and the Esca...</td>\n",
       "      <td>3</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydroxyzine HCl (hydrochloride) and hydroxyzin...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...</td>\n",
       "      <td>4</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barq's Root Beer is not a Pepsi product. It is...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.98</td>\n",
       "      <td>True</td>\n",
       "      <td>{\\n    \"Reasoning\": \"Barq's Root Beer is not a...</td>\n",
       "      <td>5</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>Claude</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5_BoolQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reasoning Answer  \\\n",
       "0  The question is asking whether the production ...  False   \n",
       "2  This question is asking about phantom limb pai...   True   \n",
       "3  Harry Potter and the Escape from Gringotts is ...   True   \n",
       "4  Hydroxyzine HCl (hydrochloride) and hydroxyzin...   True   \n",
       "5  Barq's Root Beer is not a Pepsi product. It is...  False   \n",
       "\n",
       "  Stated Confidence Answer  Coerce  \\\n",
       "0                      0.6    True   \n",
       "2                     0.95    True   \n",
       "3                     0.98    True   \n",
       "4                     0.98    True   \n",
       "5                     0.98    True   \n",
       "\n",
       "                                             Content Question ID  \\\n",
       "0  {\\n    \"Reasoning\": \"The question is asking wh...           0   \n",
       "2  {\\n    \"Reasoning\": \"This question is asking a...           2   \n",
       "3  {\\n    \"Reasoning\": \"Harry Potter and the Esca...           3   \n",
       "4  {\\n    \"Reasoning\": \"Hydroxyzine HCl (hydrochl...           4   \n",
       "5  {\\n    \"Reasoning\": \"Barq's Root Beer is not a...           5   \n",
       "\n",
       "                        Model Model Type Question Set  Stated Confidence A  \\\n",
       "0  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "2  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "3  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "4  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "5  claude-3-7-sonnet-20250219     Claude        BoolQ                  NaN   \n",
       "\n",
       "   ...  Stated Confidence E  Token Probability True  Token Probability False  \\\n",
       "0  ...                  NaN                     NaN                      NaN   \n",
       "2  ...                  NaN                     NaN                      NaN   \n",
       "3  ...                  NaN                     NaN                      NaN   \n",
       "4  ...                  NaN                     NaN                      NaN   \n",
       "5  ...                  NaN                     NaN                      NaN   \n",
       "\n",
       "   Token Probability Answer  Token Probability A  Token Probability B  \\\n",
       "0                       NaN                  NaN                  NaN   \n",
       "2                       NaN                  NaN                  NaN   \n",
       "3                       NaN                  NaN                  NaN   \n",
       "4                       NaN                  NaN                  NaN   \n",
       "5                       NaN                  NaN                  NaN   \n",
       "\n",
       "   Token Probability C  Token Probability D  Token Probability E  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "5                  NaN                  NaN                  NaN   \n",
       "\n",
       "   combined_name  \n",
       "0        0_BoolQ  \n",
       "2        2_BoolQ  \n",
       "3        3_BoolQ  \n",
       "4        4_BoolQ  \n",
       "5        5_BoolQ  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(69278, 23)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clean up MCQ Question Sets\n",
    "cc = combined_clean[combined_clean[\"Question Set\"].isin(mcq_qsets)]\n",
    "cc_letters = cc[[\"Stated Confidence A\", \"Stated Confidence B\", \"Stated Confidence C\",\"Stated Confidence D\",\"Stated Confidence E\",]].copy()\n",
    "sum_confidence = cc_letters.sum(axis = 1)\n",
    "\n",
    "con_mask = cc[sum_confidence == 0.0][\"combined_name\"]\n",
    "\n",
    "# Indixes that aren't in con_mask\n",
    "combined_clean = combined_clean.loc[~combined_clean[\"combined_name\"].isin(con_mask)]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "## Make sure we also elimanate QID that are missing\n",
    "he = combined_clean[combined_clean[\"Question Set\"] == \"HaluEval\"]\n",
    "\n",
    "s = combined_clean[\"combined_name\"].value_counts() % 11  #---------------- This seems to get rid of too much\n",
    "bad_qid = s.index[s.ne(0)].tolist()  \n",
    "\n",
    "\n",
    "\n",
    "combined_clean = combined_clean[~combined_clean[\"combined_name\"].isin(bad_qid)]\n",
    "\n",
    "\n",
    "## Special mask for LifeEval:\n",
    "\n",
    "le_df = combined_clean[combined_clean[\"Question Set\"] == \"LifeEval\"]\n",
    "\n",
    "con_isnum = pd.to_numeric( le_df['Stated Confidence Answer'], errors='coerce').notna()\n",
    "le_bad_qid= le_df[~con_isnum][\"combined_name\"]\n",
    "\n",
    "\n",
    "combined_clean = combined_clean[(~combined_clean[\"combined_name\"].isin(le_bad_qid)) ]\n",
    "\n",
    "\n",
    "display(combined_clean.head(5))\n",
    "combined_clean.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879e949",
   "metadata": {},
   "source": [
    "## Summary Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd746d8",
   "metadata": {},
   "source": [
    "Here we see the Raw versus Filtered counts for all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efcb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw</th>\n",
       "      <th>Filtered</th>\n",
       "      <th>Prop. Kept</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Question Set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>3270.0</td>\n",
       "      <td>2503.0</td>\n",
       "      <td>0.765443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaluEval</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>0.895448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSAT-AR</th>\n",
       "      <td>230.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.373913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LifeEval</th>\n",
       "      <td>808.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>0.929455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT-EN</th>\n",
       "      <td>206.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.839806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SciQ</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Raw  Filtered  Prop. Kept\n",
       "Question Set                              \n",
       "BoolQ         3270.0    2503.0    0.765443\n",
       "HaluEval      1999.0    1790.0    0.895448\n",
       "LSAT-AR        230.0      86.0    0.373913\n",
       "LifeEval       808.0     751.0    0.929455\n",
       "SAT-EN         206.0     173.0    0.839806\n",
       "SciQ          1000.0     995.0    0.995000"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.DataFrame({\n",
    "    \"Raw\": round(combined_df[\"Question Set\"].value_counts() / 11), # This is slightly below 2000 for Llama because we had to drop 3 duplicates\n",
    "    \"Filtered\": combined_clean[\"Question Set\"].value_counts() / 11\n",
    "})\n",
    "\n",
    "counts[~counts.index.isin(mcq_qsets)]\n",
    "\n",
    "counts[\"Prop. Kept\"] = counts[\"Filtered\"] / counts[\"Raw\"]\n",
    "counts\n",
    "# Need to look at: LSAT-AR (-1)???,  Sciq(-1) Im getting rid of too much?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff4a25",
   "metadata": {},
   "source": [
    "Jacob's cleaned version of SciQ has 996 rows while mine has 995. When investigating each deleted Question ID in my version we find that they all have 1 instance where the answer could not be coerced. Does Jacob have an extra question that shouldn't be there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "26e58eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question ID</th>\n",
       "      <th>false_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question ID  false_count\n",
       "0          13            1\n",
       "1         295            1\n",
       "2          40            1\n",
       "3         699            1\n",
       "4         705            1"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sciq_filtered = combined_clean[combined_clean[\"Question Set\"] == \"SciQ\"]\n",
    "sciq_raw = combined_df[combined_df[\"Question Set\"] == \"SciQ\"]\n",
    "\n",
    "\n",
    "sciq_filtered_ids = sciq_filtered.index\n",
    "\n",
    "deleted = sciq_raw[~sciq_raw.index.isin(sciq_filtered_ids)]\n",
    "deleted[\"Question ID\"].unique()\n",
    "\n",
    "# Deleted QID in SciQ are ['13', '40', '295', '699', '705'] after visual inspection they all seem to have False\n",
    "\n",
    "\n",
    "false_counts = (deleted['Coerce'].eq(False)\n",
    "                .groupby(deleted['Question ID'])\n",
    "                .sum()\n",
    "                .rename('false_count'))\n",
    "\n",
    "# as a DataFrame\n",
    "false_counts = false_counts.reset_index()\n",
    "false_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1ecd9798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question Set</th>\n",
       "      <th>BoolQ</th>\n",
       "      <th>HaluEval</th>\n",
       "      <th>LSAT-AR</th>\n",
       "      <th>LifeEval</th>\n",
       "      <th>SAT-EN</th>\n",
       "      <th>SciQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>3241</td>\n",
       "      <td>1960</td>\n",
       "      <td>208</td>\n",
       "      <td>807</td>\n",
       "      <td>202</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>3200</td>\n",
       "      <td>1997</td>\n",
       "      <td>190</td>\n",
       "      <td>800</td>\n",
       "      <td>202</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-7-sonnet-20250219</th>\n",
       "      <td>3267</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-haiku-20240307</th>\n",
       "      <td>3036</td>\n",
       "      <td>1855</td>\n",
       "      <td>230</td>\n",
       "      <td>808</td>\n",
       "      <td>181</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-sonnet-4-20250514</th>\n",
       "      <td>3258</td>\n",
       "      <td>2000</td>\n",
       "      <td>188</td>\n",
       "      <td>808</td>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-r1</th>\n",
       "      <td>3214</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-v3</th>\n",
       "      <td>2898</td>\n",
       "      <td>2000</td>\n",
       "      <td>228</td>\n",
       "      <td>808</td>\n",
       "      <td>204</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash</th>\n",
       "      <td>3261</td>\n",
       "      <td>2000</td>\n",
       "      <td>177</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-pro</th>\n",
       "      <td>3190</td>\n",
       "      <td>1984</td>\n",
       "      <td>188</td>\n",
       "      <td>808</td>\n",
       "      <td>204</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>3247</td>\n",
       "      <td>2000</td>\n",
       "      <td>230</td>\n",
       "      <td>808</td>\n",
       "      <td>206</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o3-2025-04-16</th>\n",
       "      <td>3070</td>\n",
       "      <td>1991</td>\n",
       "      <td>193</td>\n",
       "      <td>761</td>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Question Set                 BoolQ  HaluEval  LSAT-AR  LifeEval  SAT-EN  SciQ\n",
       "Model                                                                        \n",
       "Meta-Llama-3.1-70B-Instruct   3241      1960      208       807     202  1000\n",
       "Meta-Llama-3.1-8B-Instruct    3200      1997      190       800     202   997\n",
       "claude-3-7-sonnet-20250219    3267      2000      228       808     206   999\n",
       "claude-3-haiku-20240307       3036      1855      230       808     181   999\n",
       "claude-sonnet-4-20250514      3258      2000      188       808     205  1000\n",
       "deepseek-r1                   3214      2000      228       808     206  1000\n",
       "deepseek-v3                   2898      2000      228       808     204  1000\n",
       "gemini-2.5-flash              3261      2000      177       808     206  1000\n",
       "gemini-2.5-pro                3190      1984      188       808     204  1000\n",
       "gpt-4o                        3247      2000      230       808     206  1000\n",
       "o3-2025-04-16                 3070      1991      193       761     205  1000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df[\"Coerce\"] == True].pivot_table(index=\"Model\", columns=\"Question Set\",\n",
    "                        aggfunc=\"size\", fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca270a",
   "metadata": {},
   "source": [
    "### Additional EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112eca0e",
   "metadata": {},
   "source": [
    "Don was curious about some missing fields. For BoolQ we can see that all models have 2503 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4854d335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "claude-3-7-sonnet-20250219     2503\n",
       "claude-3-haiku-20240307        2503\n",
       "claude-sonnet-4-20250514       2503\n",
       "deepseek-r1                    2503\n",
       "deepseek-v3                    2503\n",
       "gemini-2.5-flash               2503\n",
       "gemini-2.5-pro                 2503\n",
       "gpt-4o                         2503\n",
       "o3-2025-04-16                  2503\n",
       "Meta-Llama-3.1-70B-Instruct    2503\n",
       "Meta-Llama-3.1-8B-Instruct     2503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_clean[(combined_clean[\"Question Set\"] == \"BoolQ\")][\"Model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e79a241c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Stated Confidence Answer</th>\n",
       "      <th>Coerce</th>\n",
       "      <th>Content</th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Question Set</th>\n",
       "      <th>Stated Confidence A</th>\n",
       "      <th>...</th>\n",
       "      <th>Stated Confidence E</th>\n",
       "      <th>Token Probability True</th>\n",
       "      <th>Token Probability False</th>\n",
       "      <th>Token Probability Answer</th>\n",
       "      <th>Token Probability A</th>\n",
       "      <th>Token Probability B</th>\n",
       "      <th>Token Probability C</th>\n",
       "      <th>Token Probability D</th>\n",
       "      <th>Token Probability E</th>\n",
       "      <th>combined_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37570</th>\n",
       "      <td>The television series 'The Tudors' aired on Sh...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>```json\\n{\\n    \"Reasoning\": \"The television s...</td>\n",
       "      <td>3000</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3000_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37571</th>\n",
       "      <td>The question asks if the character Marley dies...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>```json\\n{\\n    \"Reasoning\": \"The question ask...</td>\n",
       "      <td>3001</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3001_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37572</th>\n",
       "      <td>Canada is a prominent country with a well-deve...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>```json\\n{\\n    \"Reasoning\": \"Canada is a prom...</td>\n",
       "      <td>3002</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3002_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37574</th>\n",
       "      <td>The term 'Pit Bull' commonly refers to a type ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>```json\\n{\\n    \"Reasoning\": \"The term 'Pit Bu...</td>\n",
       "      <td>3004</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3004_BoolQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37575</th>\n",
       "      <td>A 'postal code' is a general term used interna...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Response: {\\n    \"Reasoning\": \"A 'postal code'...</td>\n",
       "      <td>3005</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>BoolQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3005_BoolQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Reasoning Answer  \\\n",
       "37570  The television series 'The Tudors' aired on Sh...  False   \n",
       "37571  The question asks if the character Marley dies...   True   \n",
       "37572  Canada is a prominent country with a well-deve...   True   \n",
       "37574  The term 'Pit Bull' commonly refers to a type ...  False   \n",
       "37575  A 'postal code' is a general term used interna...  False   \n",
       "\n",
       "      Stated Confidence Answer  Coerce  \\\n",
       "37570                      1.0    True   \n",
       "37571                      1.0    True   \n",
       "37572                      1.0    True   \n",
       "37574                      1.0    True   \n",
       "37575                      1.0    True   \n",
       "\n",
       "                                                 Content Question ID  \\\n",
       "37570  ```json\\n{\\n    \"Reasoning\": \"The television s...        3000   \n",
       "37571  ```json\\n{\\n    \"Reasoning\": \"The question ask...        3001   \n",
       "37572  ```json\\n{\\n    \"Reasoning\": \"Canada is a prom...        3002   \n",
       "37574  ```json\\n{\\n    \"Reasoning\": \"The term 'Pit Bu...        3004   \n",
       "37575  Response: {\\n    \"Reasoning\": \"A 'postal code'...        3005   \n",
       "\n",
       "                  Model Model Type Question Set  Stated Confidence A  ...  \\\n",
       "37570  gemini-2.5-flash     Gemini        BoolQ                  NaN  ...   \n",
       "37571  gemini-2.5-flash     Gemini        BoolQ                  NaN  ...   \n",
       "37572  gemini-2.5-flash     Gemini        BoolQ                  NaN  ...   \n",
       "37574  gemini-2.5-flash     Gemini        BoolQ                  NaN  ...   \n",
       "37575  gemini-2.5-flash     Gemini        BoolQ                  NaN  ...   \n",
       "\n",
       "       Stated Confidence E  Token Probability True  Token Probability False  \\\n",
       "37570                  NaN                     NaN                      NaN   \n",
       "37571                  NaN                     NaN                      NaN   \n",
       "37572                  NaN                     NaN                      NaN   \n",
       "37574                  NaN                     NaN                      NaN   \n",
       "37575                  NaN                     NaN                      NaN   \n",
       "\n",
       "       Token Probability Answer  Token Probability A  Token Probability B  \\\n",
       "37570                       NaN                  NaN                  NaN   \n",
       "37571                       NaN                  NaN                  NaN   \n",
       "37572                       NaN                  NaN                  NaN   \n",
       "37574                       NaN                  NaN                  NaN   \n",
       "37575                       NaN                  NaN                  NaN   \n",
       "\n",
       "       Token Probability C  Token Probability D  Token Probability E  \\\n",
       "37570                  NaN                  NaN                  NaN   \n",
       "37571                  NaN                  NaN                  NaN   \n",
       "37572                  NaN                  NaN                  NaN   \n",
       "37574                  NaN                  NaN                  NaN   \n",
       "37575                  NaN                  NaN                  NaN   \n",
       "\n",
       "       combined_name  \n",
       "37570     3000_BoolQ  \n",
       "37571     3001_BoolQ  \n",
       "37572     3002_BoolQ  \n",
       "37574     3004_BoolQ  \n",
       "37575     3005_BoolQ  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_clean[(combined_clean[\"Question Set\"] == \"BoolQ\") & (combined_clean[\"Model Type\"] == \"Gemini\")].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d86feb",
   "metadata": {},
   "source": [
    "## Save both CSVs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "8469f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_path   = Path(\"Parsed Results/combined_raw.csv\")\n",
    "clean_path = Path(\"Parsed Results/combined_clean.csv\")\n",
    "\n",
    "\n",
    "# write\n",
    "combined_df.to_csv(raw_path, index=False, encoding=\"utf-8\")\n",
    "combined_clean.to_csv(clean_path, index=False, encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
