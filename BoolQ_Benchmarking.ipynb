{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6gksvy3yywZM/EWbFlqWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be37759997e949e7a0ede0f7fbe260f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81e38d7935d74989bf3a3f6f7e9a7e8c",
              "IPY_MODEL_e28d0892f118430fa42fab40e20d3901",
              "IPY_MODEL_7f743cf19e6e40d683b227bf89c4ad11"
            ],
            "layout": "IPY_MODEL_8f2c3ba5104548ff8c3e56091ab1e11e"
          }
        },
        "81e38d7935d74989bf3a3f6f7e9a7e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80dab920f25e4e74a5c94bcba5945224",
            "placeholder": "​",
            "style": "IPY_MODEL_1afec6f38c664181a03a56b30faeb6b0",
            "value": "Processing Questions:   0%"
          }
        },
        "e28d0892f118430fa42fab40e20d3901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb8c51b8b28345e19ff57b188b0b812e",
            "max": 3270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca50d0953efb4d5c95e94128a4f85a0b",
            "value": 3
          }
        },
        "7f743cf19e6e40d683b227bf89c4ad11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a797b89f6644cb1958146c5677623bb",
            "placeholder": "​",
            "style": "IPY_MODEL_99a7e8fb3985485f881f1d0b88be32c7",
            "value": " 3/3270 [01:16&lt;22:36:50, 24.92s/it]"
          }
        },
        "8f2c3ba5104548ff8c3e56091ab1e11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80dab920f25e4e74a5c94bcba5945224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afec6f38c664181a03a56b30faeb6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb8c51b8b28345e19ff57b188b0b812e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca50d0953efb4d5c95e94128a4f85a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a797b89f6644cb1958146c5677623bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a7e8fb3985485f881f1d0b88be32c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoamMichael/Comparing-Confidence-in-LLMs/blob/main/BoolQ_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CAvlxo4yzo2"
      },
      "outputs": [],
      "source": [
        "## Notebook for benchmarking BoolQ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install anthropic\n",
        "%pip install openai\n",
        "%pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_18iWpMy9Lt",
        "outputId": "a90a198b-44e5-4993-e5cf-ddd9395467b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.54.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.54.0-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.54.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (AutoTokenizer,\n",
        "                        AutoModelForCausalLM,\n",
        "                        BitsAndBytesConfig,\n",
        "                        pipeline)\n",
        "import warnings\n",
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "from abc import ABC, abstractmethod\n",
        "from tqdm.notebook import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata\n",
        "\n",
        "max_tokens = 150\n",
        "\n",
        "class OpenModel: ## This class is built around Hugging Face methods\n",
        "  def __init__(self, name, key, MaxTokens = 150):\n",
        "    self.name = name\n",
        "    self.key = key\n",
        "    self.MaxTokens = MaxTokens\n",
        "    print(f\"Downloading Tokenizer for {self.name}\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name,token = self.key) ## Import Tokenizer\n",
        "    print(f\"Downloading Model Weights for {self.name}\")\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.name, token = self.key, device_map=\"auto\") ## Import Model\n",
        "\n",
        "    ## Make text generation pipeline\n",
        "    self.pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = self.model,\n",
        "    tokenizer = self.tokenizer,\n",
        "    do_sample = False,\n",
        "    max_new_tokens = self.MaxTokens,\n",
        "    eos_token_id = self.tokenizer.eos_token_id,\n",
        "    pad_token_id = self.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  def generate(self, prompt):\n",
        "    return self.pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "  def GetTokens(self, prompt: str):\n",
        "    ## Get Answer:\n",
        "    batch = self.tokenizer(prompt, return_tensors= \"pt\").to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = self.model(**batch)\n",
        "    ## Get Token Probabilites\n",
        "    logits = outputs.logits\n",
        "\n",
        "    ## Apply softmax to the logits to get probabilities\n",
        "    probs = torch.softmax(logits[0, -1], dim=0)\n",
        "\n",
        "    ##Get the top k token indices and their probabilities\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, 100, sorted =True)\n",
        "\n",
        "    ## Convert token indices to tokens\n",
        "    top_k_tokens = [self.tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "    ## Convert probabilities to list of floats\n",
        "    top_k_probs = top_k_probs.tolist()                  #list of probabilities\n",
        "\n",
        "    ## Create a Pandas Series with tokens as index and probabilities as values\n",
        "    logit_series = pd.Series(top_k_probs, index=top_k_tokens)\n",
        "\n",
        "    ## Sort the series by values in descending order\n",
        "    logit_series = logit_series.sort_values(ascending=False)\n",
        "    logit_series.index.name = \"Token\"\n",
        "    logit_series.name = \"Probability\"\n",
        "    return logit_series\n",
        "\n",
        "class ClosedModel(ABC):\n",
        "  @abstractmethod\n",
        "  def generate(self, prompt: str, system:str = \"\")-> str:\n",
        "        \"\"\"\n",
        "        Abstract method to generate a response from the language model.\n",
        "        \"\"\"\n",
        "        pass\n",
        "  @abstractmethod\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Reasoning', 'Error'])\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def client(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def GetRAC(self, prompt: str, system1:str = \"\", system2: str = \"\")-> str: ## Get Reasoning Answer Confidence\n",
        "    pass\n",
        "\n",
        "class GPTmodel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Error'])\n",
        "  def client(self):\n",
        "    # Initialize the OpenAI client with the API key\n",
        "    self.client = openai.OpenAI(api_key=self.key)\n",
        "\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Use the new client-based API call\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens= max_tokens\n",
        "    )\n",
        "    # Access the content from the new response object structure\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "  def GetRAC(self, prompt: str)-> str: ## Get Reasoning Answer Confidence\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1\n",
        "    ## Get the reasoning\n",
        "    rac = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    return rac\n",
        "\n",
        "class AnthropicModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Error'])\n",
        "  def client(self):\n",
        "    # Initialize the Anthropic client with the API key\n",
        "    self.client = anthropic.Anthropic(api_key=self.key)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # The messages list should only contain user and assistant roles\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Use the Anthropic client to create a message\n",
        "    # Pass the system message as a top-level 'system' parameter\n",
        "    message = self.client.messages.create(\n",
        "        model=self.name,\n",
        "        max_tokens= max_tokens, # You can adjust this or make it an instance variable\n",
        "        messages=messages,\n",
        "        system=system if system else None # Pass system as a separate parameter, or None if empty\n",
        "    )\n",
        "    # Access the content from the response object\n",
        "    return message.content[0].text\n",
        "\n",
        "  def GetRAC(self, prompt: str)-> str: ## Get Reasoning Answer Confidence\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1\n",
        "    ## Get the reasoning\n",
        "    rac = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    return rac\n",
        "\n",
        "\n",
        "class GeminiModel(ClosedModel):\n",
        "  def __init__(self, name, api_key):\n",
        "    self.name = name\n",
        "    self.key = api_key\n",
        "    self.results = pd.DataFrame(columns = ['Question ID','Question', 'Answer', 'Error'])\n",
        "\n",
        "  def client(self):\n",
        "    # Initialize the google.generativeai client with the API key\n",
        "\n",
        "    genai.configure(api_key=self.key)\n",
        "    self.model = genai.GenerativeModel(model_name=self.name)\n",
        "\n",
        "  def generate(self, prompt: str, system: str = \"\") -> str:\n",
        "    # Build the content list, including the system message if provided\n",
        "    contents = [{\"role\": \"user\", \"parts\": [prompt]}]\n",
        "    if system:\n",
        "        contents = [{\"role\": \"user\", \"parts\": [system]}] + contents\n",
        "\n",
        "    # Use the Gemini model to generate content\n",
        "    response = self.model.generate_content(contents)\n",
        "\n",
        "    # Access the content from the response object\n",
        "    return response.text\n",
        "\n",
        "  def GetRAC(self, prompt: str)-> str: ## Get Reasoning Answer Confidence\n",
        "    # Access global system prompts\n",
        "    global sys_prompt1\n",
        "    ## Get the reasoning\n",
        "    rac = self.generate(prompt, sys_prompt1)\n",
        "    ## Get the answer and confidence\n",
        "    return rac"
      ],
      "metadata": {
        "id": "100GdP6Dy-LU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Functions:\n",
        "def init_models(models_dict,\n",
        "                test_prompt = \"Zdzisław Beksiński was\",\n",
        "                test_system = \"You are a helpful assistant.\"):\n",
        "  print('Initializing Closed Models:')\n",
        "  closed_models = []\n",
        "  for model_type in my_closed_models:\n",
        "      print(f'{model_type}:')\n",
        "      api_key_name = my_closed_models[model_type]['api_key_name']\n",
        "      api_key = userdata.get(api_key_name)\n",
        "      print(f'  API Key Name: {my_closed_models[model_type][\"api_key_name\"]}')\n",
        "      for model_name in my_closed_models[model_type]['models']:\n",
        "        # Instantiate the correct subclass based on model_type\n",
        "        if model_type == 'GPT':\n",
        "            my_model = GPTmodel(name = model_name, api_key = api_key)\n",
        "        elif model_type == 'Claude':\n",
        "            my_model = AnthropicModel(name = model_name, api_key = api_key)\n",
        "        elif model_type == 'Gemini':\n",
        "            my_model = GeminiModel(name = model_name, api_key = api_key)\n",
        "        else:\n",
        "            # Handle unexpected model types if necessary\n",
        "            print(f\"Warning: Unknown model type {model_type}. Skipping.\")\n",
        "            continue # Skip to the next model name if type is unknow\n",
        "        my_model.client()\n",
        "        closed_models.append(my_model)\n",
        "        print(f'    {model_name}')\n",
        "\n",
        "\n",
        "  print(f'Models Initialized: {len(closed_models)}')\n",
        "  print(f'Model locations:\\n{closed_models}')\n",
        "\n",
        "  print('-'*42)\n",
        "  print('Testing all closed models:')\n",
        "  print(f'Test prompt: {test_prompt}')\n",
        "  print(f'Test system: {test_system}')\n",
        "\n",
        "  for model in closed_models:\n",
        "    print(f'\\nTesting model: {model.name}')\n",
        "    print(model.generate(test_prompt, test_system))\n",
        "  return closed_models\n",
        "\n",
        "def format_df(df):\n",
        "\n",
        "  ## %%%%%%%%%%%%%%\n",
        "  ## I need to fix how formating is done for some Q's. As daniel pointed out some\n",
        "  ## questions only have 4 options, not 5.\n",
        "  ## %%%%%%%%%%%%%%\n",
        "\n",
        "  ## Takes in a dataframe in the form:\n",
        "  ## | Question Number | Question | Option A | Option B | ... | Correct Answer Letter |\n",
        "  ## |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "  ##\n",
        "  ## Returns a dataframe in the form:\n",
        "  ## | Question Number | Full Prompt 1 | Full Prompt 2 |\n",
        "  ## |     (Int)       |    (Str)      |    (Str)      |\n",
        "\n",
        "  columns = df.columns\n",
        "  num_options = columns.str.contains('Option').astype(int).sum()\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Check if DF is formatted properly\n",
        "  error_text = f'''Make sure dataframe is in following format:\n",
        "  | Question Number | Question | Option A | Option B | ... | Correct Answer Letter |\n",
        "  |     (Int)       |     (Str)     |  (Str)   |  (Str)   |     |       (Char)          |\n",
        "\n",
        "  The current format of Dataframe is: {columns}\n",
        "  '''\n",
        "  ['Question Number', 'Question', 'Correct Answer Letter']\n",
        "  if num_options < 2:\n",
        "    raise Exception(error_text)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  ## Initialize Output dataframe:\n",
        "  header = ['Question Num', 'Full Prompt 1', 'Full Prompt 2']\n",
        "  output_df = pd.DataFrame(columns = header)\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "\n",
        "  ## Format questions for benchmark\n",
        "  letters = ['A', 'B', 'C', 'D', 'E']\n",
        "  options = ['Option A', 'Option B', 'Option C', 'Option D', 'Option E']\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    question = df['Question'][i]\n",
        "\n",
        "    sys_prompt_temp1 = sys_prompt1\n",
        "    sys_prompt_temp2 = sys_prompt2\n",
        "    ## Reformat system prompt in order to fit number of options in benchmark\n",
        "    if type(df['Option E'][i]) == float: ## ABCD\n",
        "      sys_prompt_temp1 = (sys_prompt1\n",
        "                    .replace('(A, B, C, D, or E)', '(A, B, C, or D)')\n",
        "                    .replace('E) ${Option E}', '')\n",
        "          )\n",
        "      sys_prompt_temp2 = (sys_prompt2\n",
        "                    .replace('(A, B, C, D, or E)', '(A, B, C, or D)')\n",
        "                    .replace('E) ${Option E}', '')\n",
        "          )\n",
        "      if type(df['Option D'][i]) == float: ## ABC\n",
        "        sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                      .replace('(A, B, C, or D)', '(A, B, or C)')\n",
        "                      .replace('D) ${Option D}', '')\n",
        "            )\n",
        "        sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                    .replace('(A, B, C, or D)', '(A, B, or C)')\n",
        "                    .replace('D) ${Option D}', '')\n",
        "          )\n",
        "\n",
        "        if type(df['Option C'][i]) == float: ## AB\n",
        "          sys_prompt_temp1 = (sys_prompt_temp1\n",
        "                        .replace('(A, B, or C)', '(A or B)')\n",
        "                        .replace('C) ${Option C}', '')\n",
        "              )\n",
        "          sys_prompt_temp2 = (sys_prompt_temp2\n",
        "                      .replace('(A, B, or C)', '(A or B)')\n",
        "                      .replace('C) ${Option C}', '')\n",
        "            )\n",
        "\n",
        "    option_text = df[options[:num_options]].iloc[i].to_list()\n",
        "    ## Prompt for specific question\n",
        "    new_prompt = sys_prompt_temp1.replace('${Question}', question)\n",
        "    for j in range(num_options): ## This for loop allows for dynamic question amounts\n",
        "        new_prompt = new_prompt.replace(f'${{Option {letters[j]}}}', str(option_text[j]))\n",
        "\n",
        "\n",
        "    ## Add formatted prompts.\n",
        "    ## Note that this is formatted to llama so changes may be needed down the line.\n",
        "    prompts1 = (new_prompt.split('<Your concise reasoning here. Max 100 words>')[0]) ## Specific prompt for question\n",
        "\n",
        "    prompts2 = (sys_prompt_temp2) ## Generic prompt for question confidence\n",
        "    output_df.loc[i] = [df['Question Number'].iloc[i], prompts1, prompts2]\n",
        "\n",
        "  return output_df\n",
        "\n",
        "def test_models_sequential_by_question(df, models, debug=False, start = 0):\n",
        "    \"\"\"\n",
        "    Tests a list of models on a given dataset sequentially,\n",
        "    iterating through questions and then models for each question.\n",
        "    Includes a debug mode to process only the first 10 questions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset containing questions and prompts.\n",
        "        models (list): A list of initialized model objects.\n",
        "        debug (bool): If True, only process the first 10 questions.\n",
        "    \"\"\"\n",
        "    print(\"Clearing previous results for each model...\")\n",
        "    for model in models:\n",
        "        model.results = pd.DataFrame(columns=['Question ID', 'Question', 'Answer', 'Error'])\n",
        "        print(f\"  Cleared results for {model.name}\")\n",
        "    print(\"Starting sequential testing (by question)...\")\n",
        "\n",
        "    # Determine the number of questions to process\n",
        "    num_questions_to_process = 10 if debug else len(df)\n",
        "\n",
        "    # Iterate over questions first\n",
        "    for index, row in tqdm(df.iloc[start: num_questions_to_process].iterrows(), total=num_questions_to_process, desc=\"Processing Questions\"):\n",
        "        question_num = row['Question Number']\n",
        "        prompt = row['Question'] + '\\nResponse:\\n'\n",
        "        '''\n",
        "\n",
        "        I dont love this implementation But honestly,\n",
        "        unless my logic is flawed with some edge case, I think this should work\n",
        "        and I dont want to rewrite the functions for each subclass to take in the\n",
        "        dataframe in order to work.\n",
        "        '''\n",
        "\n",
        "\n",
        "        print(f\"\\nProcessing Question {question_num}\")\n",
        "\n",
        "        # Iterate over models for the current question\n",
        "        for model in models:\n",
        "\n",
        "            try:\n",
        "                print(f\"  Testing with model: {model.name}\")\n",
        "                # Call GetRAC and add the result to the model's self.results\n",
        "                answer = model.GetRAC(prompt=prompt)\n",
        "\n",
        "                # Add the results to the model's self.results DataFrame\n",
        "                new_row = pd.DataFrame([{\n",
        "                    'Question ID': question_num,\n",
        "                    'Question': prompt,\n",
        "                    'Answer': answer,\n",
        "                    'Error': False\n",
        "                }])\n",
        "                model.results = model.results._append(new_row, ignore_index=True)\n",
        "                filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "                model.results.to_csv(filename, index=False)\n",
        "            except Exception as e:\n",
        "                print(f\"  Error testing {model.name} on Question {question_num}: {e}\")\n",
        "                # Optionally add an error entry to the results\n",
        "                error_row = pd.DataFrame([{\n",
        "                    'Question ID': question_num,\n",
        "                    'Question': prompt,\n",
        "                    'Answer': f\"Error: {e}\",\n",
        "\n",
        "                    'Error': True\n",
        "                }])\n",
        "                model.results = model.results._append(error_row, ignore_index=True)\n",
        "                filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "                model.results.to_csv(filename, index=False)\n",
        "    print(\"\\nSequential testing complete.\")\n",
        "\n",
        "    # After processing all questions, save the results for each model\n",
        "    for model in models:\n",
        "        filename = f\"{model.name.replace('/', '_').replace('-', '_')}_test_results.csv\"\n",
        "        model.results.to_csv(filename, index=False)\n",
        "        print(f\"Results for {model.name} saved to '{filename}'\")"
      ],
      "metadata": {
        "id": "Dku9njUrzBaq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import BoolQ\n",
        "print('-' *42)\n",
        "file_path = '/content/boolq_valid_formatted.csv'\n",
        "print(f'Importing Dataset: {file_path}')\n",
        "dataset = pd.read_csv(file_path)\n",
        "dataset.head()\n",
        "\n",
        "dataset['Question'].iloc[0] = 'does ethanol take more energy than it produces'\n",
        "dataset['Question Number'] = dataset.index\n",
        "display(dataset.head())\n",
        "\n",
        "\n",
        "sys_prompt1 = '''\n",
        "Analyze the following question and determine whether it is true or false. Your reasoning should not exceed 100 words. After your explanation, clearly state your answer.\n",
        "\n",
        "Question: ${Question}\n",
        "\n",
        "Resopnse: {\n",
        "  'Reasoning': <Your concise reasoning here. Max 100 words>\n",
        "  'Answer': <True or False>\n",
        "  'Confidence': <Probability that your answer is correct. As a float from 0.0 to 1.0>\n",
        "}\n",
        "\n",
        "All options have a non-zero probability of being correct. No option should have a probability of 0 or 1.\n",
        "Be modest about your certainty.  Do not provide any additional reasoning.\n",
        "'''\n",
        "\n",
        "## Initialize Models\n",
        "print('-' *42)\n",
        "print('Initializing Models:')\n",
        "my_closed_models = {\n",
        "    'GPT': {\n",
        "        'api_key_name': 'gpt_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gpt-4',\n",
        "            'gpt-3.5-turbo'\n",
        "        ]\n",
        "    },\n",
        "    'Claude': {\n",
        "        'api_key_name': 'claude_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'claude-3-7-sonnet-20250219',\n",
        "            'claude-3-haiku-20240307'\n",
        "        ]\n",
        "    },\n",
        "    'Gemini': {\n",
        "        'api_key_name': 'gemini_api_key', # Name of the key to retrieve from userdata\n",
        "        'models': [\n",
        "            'gemini-1.5-flash',\n",
        "            'gemini-2.5-pro-preview-06-05'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "closed_models = init_models(my_closed_models)\n",
        "print(' Successfully Initialied Models')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cqWGhdY4Bw-Q",
        "outputId": "20eab699-ee81-47fd-9a7c-3c2f5fd2d2be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "Importing Dataset: /content/boolq_valid_formatted.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            Question  Correct Answer  \\\n",
              "0     does ethanol take more energy than it produces           False   \n",
              "1             is house tax and property tax are same            True   \n",
              "2  is pain experienced in a missing body part or ...            True   \n",
              "3  is harry potter and the escape from gringotts ...            True   \n",
              "4  is there a difference between hydroxyzine hcl ...            True   \n",
              "\n",
              "   Question Number  \n",
              "0                0  \n",
              "1                1  \n",
              "2                2  \n",
              "3                3  \n",
              "4                4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-750741d7-27c1-45a3-bfe9-9a0e21f9ca46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Correct Answer</th>\n",
              "      <th>Question Number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>does ethanol take more energy than it produces</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is house tax and property tax are same</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is pain experienced in a missing body part or ...</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is harry potter and the escape from gringotts ...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is there a difference between hydroxyzine hcl ...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-750741d7-27c1-45a3-bfe9-9a0e21f9ca46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-750741d7-27c1-45a3-bfe9-9a0e21f9ca46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-750741d7-27c1-45a3-bfe9-9a0e21f9ca46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bb266d88-4d27-425a-bb5e-ffc9cf54777b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb266d88-4d27-425a-bb5e-ffc9cf54777b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bb266d88-4d27-425a-bb5e-ffc9cf54777b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(' Successfully Initialied Models')\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"is house tax and property tax are same\",\n          \"is there a difference between hydroxyzine hcl and hydroxyzine pam\",\n          \"is pain experienced in a missing body part or paralyzed area\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct Answer\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "Initializing Models:\n",
            "Initializing Closed Models:\n",
            "GPT:\n",
            "  API Key Name: gpt_api_key\n",
            "    gpt-4\n",
            "    gpt-3.5-turbo\n",
            "Claude:\n",
            "  API Key Name: claude_api_key\n",
            "    claude-3-7-sonnet-20250219\n",
            "    claude-3-haiku-20240307\n",
            "Gemini:\n",
            "  API Key Name: gemini_api_key\n",
            "    gemini-1.5-flash\n",
            "    gemini-2.5-pro-preview-06-05\n",
            "Models Initialized: 6\n",
            "Model locations:\n",
            "[<__main__.GPTmodel object at 0x79c94d33c150>, <__main__.GPTmodel object at 0x79c94d529450>, <__main__.AnthropicModel object at 0x79c94f454350>, <__main__.AnthropicModel object at 0x79c94df55810>, <__main__.GeminiModel object at 0x79c94dfb3a50>, <__main__.GeminiModel object at 0x79c950e3a510>]\n",
            "------------------------------------------\n",
            "Testing all closed models:\n",
            "Test prompt: Zdzisław Beksiński was\n",
            "Test system: You are a helpful assistant.\n",
            "\n",
            "Testing model: gpt-4\n",
            "a renowned Polish painter, photographer, and sculptor. He is best known for his large, detailed images of a surreal, post-apocalyptic environment. Beksiński's works are characterized by their haunting, dystopian feel, often featuring desolate landscapes and grotesque, distorted figures. Despite the grim themes, he insisted his work was not to be interpreted too literally, and that he was more interested in the form and color than the specific details. Beksiński was born on February 24, 1929, and tragically murdered in 2005.\n",
            "\n",
            "Testing model: gpt-3.5-turbo\n",
            "Zdzisław Beksiński was a renowned Polish artist known for his surreal and dystopian paintings. He was primarily known for his dark and haunting artworks that often depicted nightmarish and otherworldly scenes. Beksiński's work has gained international recognition and has left a lasting impact on the art world.\n",
            "\n",
            "Testing model: claude-3-7-sonnet-20250219\n",
            "Zdzisław Beksiński (1929-2005) was a renowned Polish painter, photographer, and sculptor known for his distinctive and haunting surrealist and dystopian artwork. His paintings often featured disturbing, nightmarish landscapes filled with distorted figures, decaying structures, and an atmosphere of dread and melancholy.\n",
            "\n",
            "Beksiński had no formal artistic training and worked as an industrial designer before devoting himself to art. His work is typically divided into periods, with his most famous being his \"fantastic period\" from the 1960s to the 1980s, characterized by detailed, dreamlike images with apocalyptic and death-related themes.\n",
            "\n",
            "Despite the disturbing\n",
            "\n",
            "Testing model: claude-3-haiku-20240307\n",
            "Zdzisław Beksiński was a Polish painter, photographer, and sculptor known for his distinctive style of dark, surreal, and dystopian art. Here are some key facts about him:\n",
            "\n",
            "- Born in 1929 in Sanok, Poland, he is considered one of the most prominent and influential Polish artists of the 20th century.\n",
            "\n",
            "- His paintings, which he began creating in the 1960s, are characterized by their nightmarish, post-apocalyptic, and fantastical themes. They often depict strange, biomechanical creatures and landscapes.\n",
            "\n",
            "- Beksiński's work has been described as \"photorealistic surrealism\" due to its\n",
            "\n",
            "Testing model: gemini-1.5-flash\n",
            "Zdzisław Beksiński was a Polish painter, sculptor, and photographer. He is best known for his dystopian and surrealist paintings, characterized by their bleak, desolate landscapes and disturbing imagery.  His work often features decaying figures, skeletal remains, and apocalyptic scenes.\n",
            "\n",
            "\n",
            "Testing model: gemini-2.5-pro-preview-06-05\n",
            "**Zdzisław Beksiński** (1929-2005) was a renowned Polish painter, photographer, and sculptor, best known for his unique and deeply unsettling style of **dystopian surrealism**.\n",
            "\n",
            "He is one of the most internationally recognized Polish artists of the 20th century, famous for creating haunting, nightmarish images that explore themes of death, decay, anxiety, and human fragility.\n",
            "\n",
            "Here's a breakdown of his life and work:\n",
            "\n",
            "### His Art\n",
            "\n",
            "Beksiński's artistic career is often divided into distinct periods:\n",
            "\n",
            "**1. The \"Fantastic Period\" (c. 1960s - 1980s)**\n",
            "This is his most famous and celebrated period. His oil paintings from this time are characterized by:\n",
            "*   **Post-apocalyptic landscapes:** Vast, deserted plains under ominous skies, filled with decaying ruins and skeletal structures.\n",
            "*   **Macabre Figures:** Distorted, skeletal, or mummified figures, often in poses of suffering or despair.\n",
            "*   **Surreal Architecture:** Impossible, gothic, and baroque-inspired structures that seem to be organically growing or crumbling.\n",
            "*   **Meticulous Detail:** Despite the fantastical subjects, his paintings have an incredible level of detail and a realistic texture, making them feel disturbingly plausible. He famously said, **\"I wish to paint in such a manner as if I were photographing dreams.\"**\n",
            "\n",
            "**2. Early Photography and Sculpture (1950s - 1960s)**\n",
            "Before he dedicated himself to painting, Beksiński was an accomplished avant-garde photographer. His photos often featured distorted human bodies, rough textures, and surreal compositions that foreshadowed the themes in his later paintings.\n",
            "\n",
            "**3. Later Period (1990s - 2005)**\n",
            "In his later years, his work became more abstract and simplified. He focused more on form, color, and monumental heads or figures against stark backgrounds. During this time, he also heavily embraced digital art and photo manipulation.\n",
            "\n",
            "### His Philosophy and Personality\n",
            "\n",
            "*   **No Titles, No Meaning:** Beksiński almost never titled his works, believing that titles would impose a single, limiting interpretation on the viewer. He insisted that he did not know the meaning of his own paintings and was often annoyed when people tried to analyze them.\n",
            "*   **The Man vs. The Art:** Contrary to the dark and morbid nature of his art, Beksiński was known to be a pleasant, soft-spoken man with a good sense of humor. He lived a quiet life, disliked traveling, and enjoyed listening to classical music while he painted for hours on end.\n",
            "\n",
            "### A Life Marked by Tragedy\n",
            "\n",
            "Beksiński's personal life was filled with immense loss, which many believe is reflected in the profound sadness of his work:\n",
            "*   His wife, Zofia, died of a serious illness in 1998.\n",
            "*   A year later, on Christmas Eve 1999, his son Tomasz—a popular music journalist and translator—died by suicide. Beksiński was the one who discovered his body.\n",
            "*   **His own death was shocking and violent.** In 2005, at the age of 75, he was stabbed to death in his Warsaw apartment by the 19-year-old son of his longtime caretaker after he refused to lend the teenager a small amount of money (around $100).\n",
            "\n",
            "### Legacy\n",
            "\n",
            "Zdzisław Beksiński left behind a vast and unforgettable body of work. His influence can be seen in various forms of dark art, heavy metal album covers, and the atmospheric design of video games and films (notably, the imagery in the game *The Medium* is heavily inspired by his art).\n",
            "\n",
            "In essence, **Beksiński was an artist who delved into the darkest corners of the subconscious, creating a universe that was uniquely his own—terrible, beautiful, and utterly unforgettable.**\n",
            " Successfully Initialied Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_gpt = closed_models[0]\n",
        "\n",
        "test_prompt = dataset['Question'].iloc[56]\n",
        "\n",
        "print(my_gpt.GetRAC(test_prompt))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-W3pLY3JgXQ",
        "outputId": "c8748389-3e64-41be-c3bf-4408ff649192"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: {\n",
            "  'Reasoning': Yes, there is a player in the NFL who is missing a hand. Shaquem Griffin, who was drafted by the Seattle Seahawks in 2018, was born with amniotic band syndrome which affected his left hand, leading to its amputation when he was four years old.\n",
            "  'Answer': True\n",
            "  'Confidence': 1.0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_models_sequential_by_question(dataset, closed_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750,
          "referenced_widgets": [
            "be37759997e949e7a0ede0f7fbe260f6",
            "81e38d7935d74989bf3a3f6f7e9a7e8c",
            "e28d0892f118430fa42fab40e20d3901",
            "7f743cf19e6e40d683b227bf89c4ad11",
            "8f2c3ba5104548ff8c3e56091ab1e11e",
            "80dab920f25e4e74a5c94bcba5945224",
            "1afec6f38c664181a03a56b30faeb6b0",
            "fb8c51b8b28345e19ff57b188b0b812e",
            "ca50d0953efb4d5c95e94128a4f85a0b",
            "1a797b89f6644cb1958146c5677623bb",
            "99a7e8fb3985485f881f1d0b88be32c7"
          ]
        },
        "id": "SkIm7YpKK6wg",
        "outputId": "e2cd50c2-5020-4b14-d398-9beeba7f888d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing previous results for each model...\n",
            "  Cleared results for gpt-4\n",
            "  Cleared results for gpt-3.5-turbo\n",
            "  Cleared results for claude-3-7-sonnet-20250219\n",
            "  Cleared results for claude-3-haiku-20240307\n",
            "  Cleared results for gemini-1.5-flash\n",
            "  Cleared results for gemini-2.5-pro-preview-06-05\n",
            "Starting sequential testing (by question)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Questions:   0%|          | 0/3270 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be37759997e949e7a0ede0f7fbe260f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Question 0\n",
            "  Testing with model: gpt-4\n",
            "  Testing with model: gpt-3.5-turbo\n",
            "  Testing with model: claude-3-7-sonnet-20250219\n",
            "  Testing with model: claude-3-haiku-20240307\n",
            "  Testing with model: gemini-1.5-flash\n",
            "  Testing with model: gemini-2.5-pro-preview-06-05\n",
            "\n",
            "Processing Question 1\n",
            "  Testing with model: gpt-4\n",
            "  Testing with model: gpt-3.5-turbo\n",
            "  Testing with model: claude-3-7-sonnet-20250219\n",
            "  Testing with model: claude-3-haiku-20240307\n",
            "  Testing with model: gemini-1.5-flash\n",
            "  Testing with model: gemini-2.5-pro-preview-06-05\n",
            "\n",
            "Processing Question 2\n",
            "  Testing with model: gpt-4\n",
            "  Testing with model: gpt-3.5-turbo\n",
            "  Testing with model: claude-3-7-sonnet-20250219\n",
            "  Testing with model: claude-3-haiku-20240307\n",
            "  Testing with model: gemini-1.5-flash\n",
            "  Testing with model: gemini-2.5-pro-preview-06-05\n",
            "\n",
            "Processing Question 3\n",
            "  Testing with model: gpt-4\n",
            "  Testing with model: gpt-3.5-turbo\n",
            "  Testing with model: claude-3-7-sonnet-20250219\n",
            "  Testing with model: claude-3-haiku-20240307\n",
            "  Testing with model: gemini-1.5-flash\n"
          ]
        }
      ]
    }
  ]
}